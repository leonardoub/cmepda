{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BRATS_nested_CV.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPErsR1sR3S6f9Brg3pM/Af",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/BRATS_nested_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BthL8ZcBZ44G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "4b500b99-8e7d-4a0c-ae0a-652b682d0ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_without_HISTO_without_SPATIAL_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "30186edd-fea6-4aa7-ca74-6fe1661e2d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>INTENSITY_Mean_NET_FLAIR</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>227.7510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>209.7901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>165.1014</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>207.5531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>139.3234</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>195.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>176.3511</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>152.2227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>161.5295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>163.4821</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 578 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               ID        Date  VOLUME_ET  ...  TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0    TCGA-02-0006  1996.08.23       1662  ...     0.052741    2.00000        GBM\n",
              "1    TCGA-02-0009  1997.06.14       4362  ...     0.094336   91.47360        GBM\n",
              "2    TCGA-02-0011  1998.02.01      33404  ...     0.096035  272.42900        GBM\n",
              "3    TCGA-02-0027  1999.03.28      12114  ...     0.096470  128.46800        GBM\n",
              "4    TCGA-02-0033  1997.05.26      34538  ...     0.096894  240.77800        GBM\n",
              "..            ...         ...        ...  ...          ...        ...        ...\n",
              "141  TCGA-HT-7694  1995.04.04       1036  ...     0.090456  719.23800        LGG\n",
              "142  TCGA-HT-8018  1997.04.11       2093  ...     0.054307    2.00000        LGG\n",
              "143  TCGA-HT-8111  1998.03.30       1929  ...     0.126712    7.06744        LGG\n",
              "144  TCGA-HT-8114  1998.10.30       8755  ...     0.107433   15.52240        LGG\n",
              "145  TCGA-HT-8563  1998.12.09      11757  ...     0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 578 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "75a60b27-bd41-4c0d-a32e-358f4947c1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Date', 'VOLUME_ET', 'VOLUME_NET', 'VOLUME_ED', 'VOLUME_TC',\n",
              "       'VOLUME_WT', 'VOLUME_BRAIN', 'VOLUME_ET_OVER_NET', 'VOLUME_ET_OVER_ED',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=578)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b_lnjdLcWEj",
        "colab_type": "text"
      },
      "source": [
        "#Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0hG99bXOxf7",
        "colab_type": "code",
        "outputId": "5a41c2bc-6a78-4a2d-b868-df9dc11ede3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV,KFold,cross_val_predict,cross_val_score,StratifiedKFold\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report,accuracy_score"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYvW7PB3Rqly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vettorizzare i label\n",
        "encoder = LabelEncoder()\n",
        "labels_encoded = encoder.fit_transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmG6Fx2eR-3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scalers\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "scalers_to_test = [StandardScaler(), RobustScaler(), MinMaxScaler(), None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcDzZhRSSCLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Designate distributions to sample hyperparameters from \n",
        "n_tree = [10, 50, 100, 150]\n",
        "n_features_to_test = [0.9]\n",
        "depth = [10, None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JigjLrG-Vp0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose cross-validation techniques for the inner and outer loops,\n",
        "# independently of the dataset.\n",
        "# E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
        "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "outer_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wTioNOISOm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RandomForestClassifier\n",
        "steps = [('scaler', StandardScaler()), ('clf', RandomForestClassifier(random_state=503))]\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "parameteres = [{'scaler':[StandardScaler()], \n",
        "                'clf__n_estimators':list(n_tree), 'clf__criterion':['gini', 'entropy'], \n",
        "                'clf__max_depth':depth, 'clf__min_samples_split':[2, 5], \n",
        "                'clf__min_samples_leaf':[2, 4], 'clf__class_weight':[None, 'balanced']}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnj6__VFVbc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=inner_cv, n_jobs=-1, scoring='roc_auc', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOy1QLkWZNMv",
        "colab_type": "code",
        "outputId": "b5628418-eab0-43b6-9ef2-73ac03a55b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "# Nested CV with parameter optimization\n",
        "nested_score = cross_val_score(grid, X=data, y=labels_encoded, cv=outer_cv)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-e8e2f08bb3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnested_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mouter_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKW1rFsNaR2n",
        "colab_type": "text"
      },
      "source": [
        "In questo modo non riesco a sapere quali sono i best HP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbZgiLfkaRe_",
        "colab_type": "code",
        "outputId": "735720fb-95e1-4289-fb72-e700f7a3f756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nested_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81176471, 0.90784314, 0.88865546])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z5lfT0qXkIh",
        "colab_type": "text"
      },
      "source": [
        "questi dovrebbero essere i risultati del ciclo esterno di CV, ma con quali hp???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oT1Ld2cXZjv",
        "colab_type": "text"
      },
      "source": [
        "#ANOTHER WAY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3CYwBpIclrU",
        "colab_type": "code",
        "outputId": "701832a3-da66-4392-f3da-67a2d806fbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "train_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a3cd8a57426b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiZqSx_Yat3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMV4uK0Akehf",
        "colab_type": "code",
        "outputId": "f30a5878-3688-408b-856f-51adae3742a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "dfObj.iloc[[2 ,0 ] , : ]\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9f406580d3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dfObj' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSnfwj50liqq",
        "colab_type": "code",
        "outputId": "ec9ec81c-4452-4538-9508-ce910dda3373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "train_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a3cd8a57426b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F44Ru3-xlytR",
        "colab_type": "code",
        "outputId": "4bea8980-1916-4311-8a67-cd5eafc3a9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "labels_encoded[train_index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-17e3bd7c074f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVfX7QlJXrtT",
        "colab_type": "code",
        "outputId": "0180b2d9-f42a-4a08-b3cc-d6b8fce74602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "outer_loop_roc_auc_scores = []\n",
        "outer_loop_accuracy_scores = []\n",
        "inner_loop_won_params = []\n",
        "inner_loop_roc_auc_scores = []\n",
        "inner_loop_accuracy_scores = []\n",
        "best_est_dict = {}\n",
        "best_feat_dict = {}\n",
        "i = 0\n",
        "\n",
        "# Looping through the outer loop, feeding each training set into a GSCV as the inner loop\n",
        "for train_index, test_index in outer_cv.split(data, labels_encoded):\n",
        "    \n",
        "    i+=1\n",
        "\n",
        "    GSCV = GridSearchCV(pipeline, param_grid=parameteres, cv=inner_cv, n_jobs=-1, scoring=['roc_auc', 'accuracy'], refit='roc_auc', verbose=1)\n",
        "    \n",
        "    # GSCV is looping through the training data to find the best parameters. This is the inner loop\n",
        "    GSCV.fit(data.iloc[train_index, :], labels_encoded[train_index])\n",
        "    \n",
        "    # The best hyper parameters from GSCV is now being tested on the unseen outer loop test data.\n",
        "    pred = GSCV.predict(data.iloc[test_index, :])\n",
        "    \n",
        "\n",
        "    #per far uscire i best_estimators in qualche modo\n",
        "    best_est_dict.update({f'best_est_{i}' : GSCV.best_estimator_})\n",
        "\n",
        "    #The most important 10 features for the best estimator\n",
        "    #features_list =  sorted(zip(rank_features, data.columns), reverse=True)[:10]\n",
        "    #best_feat_dict.update({f'best_set_HP_{i}' : })\n",
        "\n",
        "\n",
        "    # Appending the \"winning\" hyper parameters and their associated accuracy score\n",
        "    inner_loop_won_params.append(GSCV.best_params_)\n",
        "    \n",
        "    outer_loop_roc_auc_scores.append(roc_auc_score(labels_encoded[test_index], pred))\n",
        "    outer_loop_accuracy_scores.append(accuracy_score(labels_encoded[test_index], pred))\n",
        "\n",
        "    inner_loop_roc_auc_scores.append(GSCV.best_score_)\n",
        "    #inner_loop_accuracy_scores.append(GSCV.best_score_)\n",
        "\n",
        "for i in zip(inner_loop_won_params, outer_loop_roc_auc_scores, inner_loop_roc_auc_scores):\n",
        "    print(i)\n",
        "\n",
        "#print('Mean of outer loop accuracy score:' np.mean(outer_loop_roc_auc_scores))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   24.0s\n",
            "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:   46.7s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    9.7s\n",
            "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:   45.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:   45.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "({'clf__class_weight': None, 'clf__criterion': 'entropy', 'clf__max_depth': 10, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 5, 'clf__n_estimators': 50, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}, 0.8, 0.9494510320597277)\n",
            "({'clf__class_weight': None, 'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__min_samples_leaf': 4, 'clf__min_samples_split': 2, 'clf__n_estimators': 150, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}, 0.9, 0.9006660811008637)\n",
            "({'clf__class_weight': None, 'clf__criterion': 'entropy', 'clf__max_depth': 10, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 5, 'clf__n_estimators': 150, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}, 0.8550420168067226, 0.9573122529644268)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2bFP40snboT",
        "colab_type": "code",
        "outputId": "785a563d-77e6-48af-d154-340b29b4be65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inner_loop_roc_auc_scores"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9494510320597277, 0.9006660811008637, 0.9573122529644268]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46GIyheq0VhC",
        "colab_type": "code",
        "outputId": "739bd556-a90c-41f8-dfda-44d2e38c97f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outer_loop_accuracy_scores"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8775510204081632, 0.9387755102040817, 0.8541666666666666]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCtgiIUpneJQ",
        "colab_type": "code",
        "outputId": "8b79db8f-fee7-4ccf-adee-991187ee86ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outer_loop_roc_auc_scores"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8, 0.9, 0.8550420168067226]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzH5o8atLSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rank_features = GSCV.best_estimator_.named_steps[\"clf\"].feature_importances_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPCzpd9Kx_SS",
        "colab_type": "code",
        "outputId": "1f9d0bec-7b24-4a83-a1d9-338125440a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rank_features"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01414181, 0.00442697, 0.00161773, 0.00085371, 0.00091926,\n",
              "       0.0020287 , 0.00905819, 0.00284512, 0.00414331, 0.01156362,\n",
              "       0.02088045, 0.        , 0.01830897, 0.00258127, 0.0009319 ,\n",
              "       0.        , 0.0091932 , 0.00111161, 0.00065254, 0.00069908,\n",
              "       0.00045894, 0.00073061, 0.00191462, 0.00036889, 0.00132477,\n",
              "       0.00052045, 0.00116913, 0.00147153, 0.00319024, 0.00260624,\n",
              "       0.00312825, 0.        , 0.00033873, 0.00090147, 0.        ,\n",
              "       0.        , 0.00268342, 0.00238228, 0.00030076, 0.        ,\n",
              "       0.00501625, 0.        , 0.00038916, 0.0009644 , 0.0007241 ,\n",
              "       0.00048085, 0.00190937, 0.        , 0.00086071, 0.00060549,\n",
              "       0.00179221, 0.        , 0.00081886, 0.00266559, 0.00125548,\n",
              "       0.00140828, 0.        , 0.        , 0.        , 0.00610427,\n",
              "       0.00022161, 0.00980102, 0.00263838, 0.00214781, 0.0011957 ,\n",
              "       0.        , 0.        , 0.00427285, 0.00072029, 0.        ,\n",
              "       0.        , 0.        , 0.00014516, 0.00105318, 0.        ,\n",
              "       0.00039788, 0.0005725 , 0.        , 0.        , 0.        ,\n",
              "       0.00068678, 0.00052223, 0.00073971, 0.        , 0.00105379,\n",
              "       0.00256781, 0.        , 0.00045604, 0.00052149, 0.        ,\n",
              "       0.        , 0.00016574, 0.00400876, 0.00325373, 0.        ,\n",
              "       0.        , 0.00045115, 0.        , 0.00104477, 0.        ,\n",
              "       0.00020532, 0.        , 0.00231039, 0.00113737, 0.00025786,\n",
              "       0.00080258, 0.00027758, 0.00060773, 0.        , 0.00176181,\n",
              "       0.        , 0.00039053, 0.0077741 , 0.0083984 , 0.00065332,\n",
              "       0.01562909, 0.0029913 , 0.00060259, 0.00121844, 0.        ,\n",
              "       0.00320773, 0.00686905, 0.00592351, 0.        , 0.00445781,\n",
              "       0.00081343, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.00435686, 0.00122073, 0.        , 0.00342143, 0.00072174,\n",
              "       0.        , 0.00056209, 0.00083454, 0.        , 0.00057279,\n",
              "       0.00170089, 0.00042009, 0.        , 0.00102495, 0.00094136,\n",
              "       0.00123732, 0.00020802, 0.0005672 , 0.0036119 , 0.00124153,\n",
              "       0.        , 0.00333653, 0.        , 0.        , 0.        ,\n",
              "       0.00075645, 0.00094863, 0.00092791, 0.        , 0.00089882,\n",
              "       0.00086882, 0.        , 0.0002024 , 0.        , 0.00075293,\n",
              "       0.00044156, 0.01228522, 0.        , 0.00025489, 0.00955674,\n",
              "       0.00134755, 0.00047829, 0.00209315, 0.00087989, 0.00151001,\n",
              "       0.0017282 , 0.        , 0.        , 0.        , 0.0015809 ,\n",
              "       0.        , 0.00112334, 0.        , 0.00056161, 0.        ,\n",
              "       0.00107242, 0.0002838 , 0.        , 0.        , 0.00079589,\n",
              "       0.        , 0.00159446, 0.00055157, 0.        , 0.        ,\n",
              "       0.00176213, 0.00121448, 0.00565581, 0.00258562, 0.00075979,\n",
              "       0.00256796, 0.        , 0.        , 0.00073783, 0.        ,\n",
              "       0.        , 0.        , 0.00013485, 0.00312815, 0.01322057,\n",
              "       0.00084674, 0.00085997, 0.        , 0.        , 0.00266868,\n",
              "       0.00404528, 0.00081047, 0.00469888, 0.        , 0.00015026,\n",
              "       0.        , 0.00387927, 0.00778699, 0.00023859, 0.00195162,\n",
              "       0.00654767, 0.        , 0.        , 0.        , 0.00566551,\n",
              "       0.00263117, 0.00581495, 0.00396764, 0.01949738, 0.        ,\n",
              "       0.01293831, 0.        , 0.        , 0.00112316, 0.        ,\n",
              "       0.        , 0.0006313 , 0.00407153, 0.00172679, 0.01106499,\n",
              "       0.        , 0.00526061, 0.00805128, 0.02034514, 0.00051198,\n",
              "       0.        , 0.00087793, 0.00031844, 0.        , 0.00067434,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.00107551, 0.00086248, 0.        , 0.00143508, 0.000772  ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.00020992,\n",
              "       0.        , 0.        , 0.        , 0.00034463, 0.        ,\n",
              "       0.        , 0.00033984, 0.00114078, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.00062198, 0.00054335, 0.00217208,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.00162074, 0.00088149, 0.        , 0.00119086, 0.        ,\n",
              "       0.00112528, 0.00027274, 0.00203074, 0.        , 0.00272085,\n",
              "       0.        , 0.        , 0.00205781, 0.        , 0.00331341,\n",
              "       0.00090102, 0.        , 0.00113553, 0.00125101, 0.        ,\n",
              "       0.00021525, 0.00085705, 0.        , 0.00112797, 0.00087527,\n",
              "       0.00045348, 0.        , 0.        , 0.        , 0.00222035,\n",
              "       0.        , 0.00031561, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.00295323, 0.        , 0.00286129, 0.        ,\n",
              "       0.00107253, 0.00023941, 0.00094288, 0.        , 0.00039383,\n",
              "       0.        , 0.00127129, 0.00218123, 0.        , 0.00116942,\n",
              "       0.        , 0.        , 0.        , 0.00073914, 0.00054153,\n",
              "       0.00073717, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.0014015 , 0.        , 0.00036911, 0.00310355,\n",
              "       0.00070576, 0.00139233, 0.0052569 , 0.        , 0.00032676,\n",
              "       0.00050591, 0.00093856, 0.00330277, 0.0105096 , 0.00327307,\n",
              "       0.01356565, 0.00050056, 0.00735792, 0.00105875, 0.        ,\n",
              "       0.00053818, 0.0009828 , 0.00101581, 0.00397515, 0.00043253,\n",
              "       0.00170535, 0.00024791, 0.0072063 , 0.00598523, 0.00126343,\n",
              "       0.00674605, 0.00348286, 0.00064572, 0.        , 0.01026226,\n",
              "       0.00222674, 0.00148865, 0.00448971, 0.01258479, 0.00026904,\n",
              "       0.00697557, 0.01935835, 0.        , 0.        , 0.00301157,\n",
              "       0.        , 0.        , 0.0094112 , 0.00293831, 0.        ,\n",
              "       0.0011326 , 0.00112046, 0.00181606, 0.01668796, 0.01099164,\n",
              "       0.        , 0.        , 0.        , 0.0010376 , 0.        ,\n",
              "       0.00029302, 0.00059018, 0.        , 0.0004488 , 0.00144126,\n",
              "       0.        , 0.00068252, 0.        , 0.        , 0.00144446,\n",
              "       0.00099026, 0.        , 0.        , 0.0003925 , 0.        ,\n",
              "       0.        , 0.00066975, 0.00046251, 0.        , 0.00190511,\n",
              "       0.00074068, 0.00264358, 0.        , 0.00046337, 0.        ,\n",
              "       0.        , 0.        , 0.0019703 , 0.00098846, 0.00057248,\n",
              "       0.        , 0.        , 0.        , 0.00047344, 0.00217255,\n",
              "       0.00145412, 0.        , 0.        , 0.00072757, 0.        ,\n",
              "       0.00490243, 0.00132389, 0.00298138, 0.0013895 , 0.        ,\n",
              "       0.        , 0.        , 0.00050001, 0.        , 0.00094793,\n",
              "       0.00071911, 0.00233558, 0.00077902, 0.        , 0.00168501,\n",
              "       0.0010865 , 0.00139202, 0.00097157, 0.00105766, 0.        ,\n",
              "       0.        , 0.        , 0.00169811, 0.        , 0.00093801,\n",
              "       0.        , 0.        , 0.00186154, 0.00061091, 0.0012681 ,\n",
              "       0.00228856, 0.        , 0.0005912 , 0.        , 0.        ,\n",
              "       0.        , 0.00059325, 0.00068971, 0.0005166 , 0.        ,\n",
              "       0.00221704, 0.00148325, 0.00161972, 0.        , 0.00111741,\n",
              "       0.00028453, 0.        , 0.        , 0.00041798, 0.00185343,\n",
              "       0.        , 0.00058347, 0.        , 0.        , 0.00040974,\n",
              "       0.        , 0.        , 0.00144315, 0.00067178, 0.00801947,\n",
              "       0.00048126, 0.00612174, 0.00051588, 0.01346922, 0.00874898,\n",
              "       0.        , 0.00522128, 0.        , 0.00492673, 0.01955113,\n",
              "       0.00119476, 0.0079732 , 0.00089242, 0.01648705, 0.00912322,\n",
              "       0.00034783, 0.01794858, 0.00279331, 0.00020458, 0.        ,\n",
              "       0.        , 0.00163432, 0.00255738, 0.001185  , 0.00025714,\n",
              "       0.00117479, 0.00189458, 0.00039806, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0009842 , 0.00059489,\n",
              "       0.        , 0.        , 0.00202348, 0.00070907, 0.00328877,\n",
              "       0.00097824, 0.00141298, 0.        , 0.        , 0.00783602,\n",
              "       0.00193876, 0.        , 0.        , 0.00260472, 0.0048896 ,\n",
              "       0.        , 0.00736058, 0.        , 0.00054559, 0.01122704,\n",
              "       0.00150145, 0.0008903 , 0.00090311, 0.00143295, 0.0010355 ,\n",
              "       0.        , 0.        , 0.0007708 , 0.00030171, 0.00383618])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6aqtSxMvhKC",
        "colab_type": "code",
        "outputId": "af09152e-4810-4271-958d-2149fb6d5f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        " sorted(zip(rank_features, data.columns), reverse=True)[:10]\n",
        " #ATTENZIONE HA SENSO SOLO SE NON SI FA PCA"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.020880451373372744, 'VOLUME_NET_over_TC'),\n",
              " (0.020345144047118215, 'TEXTURE_GLRLM_ET_FLAIR_RLV'),\n",
              " (0.019551125599257924, 'TEXTURE_NGTDM_ET_T2_Coarseness'),\n",
              " (0.019497380013951862, 'TEXTURE_GLRLM_ET_T2_LRHGE'),\n",
              " (0.019358349466383415, 'TEXTURE_GLSZM_ET_T2_ZSV'),\n",
              " (0.01830897495301974, 'VOLUME_ET_OVER_WT'),\n",
              " (0.01794858066316487, 'TEXTURE_NGTDM_ET_FLAIR_Busyness'),\n",
              " (0.016687958316660163, 'TEXTURE_GLSZM_ET_FLAIR_GLV'),\n",
              " (0.01648704612607801, 'TEXTURE_NGTDM_ET_T2_Strength'),\n",
              " (0.015629087119733754, 'TEXTURE_GLCM_ET_T2_AutoCorrelation')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7KBcqHaxUD7",
        "colab_type": "code",
        "outputId": "c96aefbf-6e4b-42fc-ac4b-b8cf1f5ffd70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "best_est_dict"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_est_1': Pipeline(memory=None,\n",
              "          steps=[('scaler',\n",
              "                  StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                 ('clf',\n",
              "                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                         class_weight=None, criterion='entropy',\n",
              "                                         max_depth=10, max_features='auto',\n",
              "                                         max_leaf_nodes=None, max_samples=None,\n",
              "                                         min_impurity_decrease=0.0,\n",
              "                                         min_impurity_split=None,\n",
              "                                         min_samples_leaf=2, min_samples_split=5,\n",
              "                                         min_weight_fraction_leaf=0.0,\n",
              "                                         n_estimators=50, n_jobs=None,\n",
              "                                         oob_score=False, random_state=503,\n",
              "                                         verbose=0, warm_start=False))],\n",
              "          verbose=False), 'best_est_2': Pipeline(memory=None,\n",
              "          steps=[('scaler',\n",
              "                  StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                 ('clf',\n",
              "                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                         class_weight=None, criterion='gini',\n",
              "                                         max_depth=10, max_features='auto',\n",
              "                                         max_leaf_nodes=None, max_samples=None,\n",
              "                                         min_impurity_decrease=0.0,\n",
              "                                         min_impurity_split=None,\n",
              "                                         min_samples_leaf=4, min_samples_split=2,\n",
              "                                         min_weight_fraction_leaf=0.0,\n",
              "                                         n_estimators=150, n_jobs=None,\n",
              "                                         oob_score=False, random_state=503,\n",
              "                                         verbose=0, warm_start=False))],\n",
              "          verbose=False), 'best_est_3': Pipeline(memory=None,\n",
              "          steps=[('scaler',\n",
              "                  StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                 ('clf',\n",
              "                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                         class_weight=None, criterion='entropy',\n",
              "                                         max_depth=10, max_features='auto',\n",
              "                                         max_leaf_nodes=None, max_samples=None,\n",
              "                                         min_impurity_decrease=0.0,\n",
              "                                         min_impurity_split=None,\n",
              "                                         min_samples_leaf=2, min_samples_split=5,\n",
              "                                         min_weight_fraction_leaf=0.0,\n",
              "                                         n_estimators=150, n_jobs=None,\n",
              "                                         oob_score=False, random_state=503,\n",
              "                                         verbose=0, warm_start=False))],\n",
              "          verbose=False)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbgrsCxZpo54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_est_1 = best_est_dict['best_est_1']\n",
        "best_est_2 = best_est_dict['best_est_2']\n",
        "best_est_3 = best_est_dict['best_est_3']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5PqrmQup0Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rank_feat_1 = best_est_1.named_steps[\"clf\"].feature_importances_\n",
        "rank_feat_2 = best_est_2.named_steps[\"clf\"].feature_importances_\n",
        "rank_feat_3 = best_est_3.named_steps[\"clf\"].feature_importances_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMThq-IDqBCZ",
        "colab_type": "code",
        "outputId": "298d371b-6759-4723-9e94-b11849084404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        " sorted(zip(rank_feat_1, data.columns), reverse=True)[:10]\n",
        " #ATTENZIONE HA SENSO SOLO SE NON SI FA PCA"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.05545908449902637, 'TEXTURE_GLSZM_ET_T1Gd_GLV'),\n",
              " (0.0259030738745922, 'TEXTURE_GLSZM_ET_T1Gd_ZP'),\n",
              " (0.024523086680276, 'VOLUME_ET'),\n",
              " (0.02435452291787448, 'VOLUME_ET_over_TC'),\n",
              " (0.024288759231805707, 'TEXTURE_GLRLM_ET_FLAIR_LRLGE'),\n",
              " (0.023537224467970262, 'VOLUME_NET_over_TC'),\n",
              " (0.02225654915689084, 'TEXTURE_GLSZM_ET_T1_GLV'),\n",
              " (0.021450935370269555, 'TEXTURE_GLCM_ET_T2_AutoCorrelation'),\n",
              " (0.021334939161023275, 'TEXTURE_GLRLM_ET_T2_SRHGE'),\n",
              " (0.021179159278858925, 'TEXTURE_NGTDM_ET_T2_Coarseness')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3tneOros_hR",
        "colab_type": "code",
        "outputId": "d5dca0a8-7d7a-4a30-db31-87bfa9a75138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        " sorted(zip(rank_feat_2, data.columns), reverse=True)[:10]\n",
        " #ATTENZIONE HA SENSO SOLO SE NON SI FA PCA"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.03146400992470652, 'TEXTURE_NGTDM_ET_T2_Coarseness'),\n",
              " (0.0294446301800531, 'TEXTURE_GLRLM_ET_T1Gd_GLV'),\n",
              " (0.02431474644344582, 'TEXTURE_GLSZM_ET_T1Gd_GLV'),\n",
              " (0.02252327207594658, 'TEXTURE_NGTDM_NET_FLAIR_Coarseness'),\n",
              " (0.022224936935591674, 'TEXTURE_GLSZM_ET_T1Gd_ZP'),\n",
              " (0.021354980073060206, 'TEXTURE_GLSZM_ET_T2_ZSV'),\n",
              " (0.020064728397499055, 'TEXTURE_GLRLM_ET_FLAIR_GLV'),\n",
              " (0.019459898461311336, 'TEXTURE_GLSZM_ET_T1Gd_LZLGE'),\n",
              " (0.018935412432419967, 'VOLUME_NET_over_TC'),\n",
              " (0.01877666629286694, 'TEXTURE_NGTDM_ET_FLAIR_Busyness')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pul74TketATX",
        "colab_type": "code",
        "outputId": "de8b4217-d6b2-47a8-afa3-c103f3f5795c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        " sorted(zip(rank_feat_3, data.columns), reverse=True)[:10]\n",
        " #ATTENZIONE HA SENSO SOLO SE NON SI FA PCA"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.020880451373372744, 'VOLUME_NET_over_TC'),\n",
              " (0.020345144047118215, 'TEXTURE_GLRLM_ET_FLAIR_RLV'),\n",
              " (0.019551125599257924, 'TEXTURE_NGTDM_ET_T2_Coarseness'),\n",
              " (0.019497380013951862, 'TEXTURE_GLRLM_ET_T2_LRHGE'),\n",
              " (0.019358349466383415, 'TEXTURE_GLSZM_ET_T2_ZSV'),\n",
              " (0.01830897495301974, 'VOLUME_ET_OVER_WT'),\n",
              " (0.01794858066316487, 'TEXTURE_NGTDM_ET_FLAIR_Busyness'),\n",
              " (0.016687958316660163, 'TEXTURE_GLSZM_ET_FLAIR_GLV'),\n",
              " (0.01648704612607801, 'TEXTURE_NGTDM_ET_T2_Strength'),\n",
              " (0.015629087119733754, 'TEXTURE_GLCM_ET_T2_AutoCorrelation')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkErgXj3VRa",
        "colab_type": "text"
      },
      "source": [
        "#SICCOME OTTENGO 3 SET DI BEST HP COME POSSO OTTENERE UN SOLO CLASSIFICATORE, PROVA MAJOR VOTING RULE TRA I 3 CLF, PER POI FARE CV E DARE STIMA PRESTAZIONI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_5hJb3d8jGX",
        "colab_type": "text"
      },
      "source": [
        "##E' UNA CAZZATA PERCHE COMUNQUE DOVREI VALUTARE I DIVERSI CLF SUGLI STESSI PATTERN, INTRODUCO UN BIAS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fD93iGI3mQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h7Et6Qv30Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = best_est_1\n",
        "clf2 = best_est_2\n",
        "clf3 = best_est_3\n",
        "\n",
        "eclf1 = VotingClassifier(estimators=[\n",
        "        ('clf1', clf1), ('clf2', clf2), ('clf3', clf3)], voting='soft')\n",
        "eclf1 = eclf1.fit(data, labels_encoded)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UUqVdIy41km",
        "colab_type": "code",
        "outputId": "a625f9f0-e878-4692-edd1-f2f852ccaa83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cross_val_score(eclf1, data, labels_encoded, scoring='accuracy', cv=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8       , 0.86206897, 0.93103448, 0.89655172, 0.89655172])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwvHrkgB4h5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
        "               eclf1.named_estimators_['lr'].predict(X))\n",
        "\n",
        "eclf2 = VotingClassifier(estimators=[\n",
        "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
        "        voting='soft')\n",
        "eclf2 = eclf2.fit(X, y)\n",
        "print(eclf2.predict(X))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ptiJolN-RP6",
        "colab_type": "text"
      },
      "source": [
        "#Prova ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7466H-Q-x_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.iloc[train_index, :], labels_encoded[train_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToV8YRWE-Tx8",
        "colab_type": "code",
        "outputId": "af6a2b42-7881-4552-f573-5acabb3fa3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = best_est_3.predict_proba(data.iloc[test_index, :])\n",
        "preds = probs[:,1]\n",
        "fpr, tpr, threshold = metrics.roc_curve(labels_encoded[test_index], preds)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "# method I: plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6J3anCRcVlg",
        "colab_type": "code",
        "outputId": "7ca2534f-b77a-4749-8822-7000d241efbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1, 6):\n",
        "\n",
        "    #Train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(public_data, public_labels, test_size=0.3, \n",
        "    stratify=public_labels, random_state=i*500)\n",
        "\n",
        "    #Vettorizzare i label\n",
        "    train_labels_encoded = encoder.fit_transform(y_train)\n",
        "    test_labels_encoded = encoder.transform(y_test)\n",
        "\n",
        "    #RandomForestClassifier\n",
        "    steps = [('scaler', StandardScaler()), ('red_dim', PCA()), ('clf', RandomForestClassifier(random_state=i*503))]\n",
        "\n",
        "    pipeline = Pipeline(steps)\n",
        "\n",
        "    parameteres = [{'scaler':scalers_to_test, 'red_dim':[PCA(random_state=42)], 'red_dim__n_components':list(n_features_to_test),\n",
        "                    'red_dim__whiten':[False, True], \n",
        "                    'clf__n_estimators':list(n_tree), 'clf__criterion':['gini', 'entropy'], \n",
        "                    'clf__max_depth':depth, 'clf__min_samples_split':[2, 5, 10], \n",
        "                    'clf__min_samples_leaf':[1, 2, 4], 'clf__class_weight':[None, 'balanced']}]\n",
        "\n",
        "    grid = GridSearchCV(pipeline, param_grid=parameteres, cv=3, n_jobs=-1, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "     grid.fit(X_train, y_train)\n",
        "\n",
        "    score_train = grid.score(X_train, y_train)\n",
        "    score_test = grid.score(X_test, y_test)\n",
        "    best_p = grid.best_params_\n",
        "\n",
        "    bp = pd.DataFrame(best_p, index=[i])\n",
        "    bp['accuracy_train'] = score_train\n",
        "    bp['accuracy_test'] = score_test\n",
        "    bp['random_state'] = i*500\n",
        "    bp['random_state_pca'] = i*42\n",
        "    bp['random_state_clf'] = i*503\n",
        "\n",
        "    df = df.append(bp, ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-936043f88c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Train test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     X_train, X_test, y_train, y_test = train_test_split(public_data, public_labels, test_size=0.3, \n\u001b[0m\u001b[1;32m     11\u001b[0m     stratify=public_labels, random_state=i*500)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'public_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYtNggBRaf2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7ROe5ga1fue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D={i:0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dDNMU1z1jJP",
        "colab_type": "code",
        "outputId": "6e9493a8-4eb9-440f-974c-641e89a9c1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "D"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvrS2Iqw1j__",
        "colab_type": "code",
        "outputId": "c33ddd64-38e0-4687-e51e-89fb2c697e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "GSCV.cv_results_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.04292591, 0.15979902, 0.28904231, 0.42704121, 0.03603745,\n",
              "        0.14704879, 0.29131174, 0.42921432, 0.03875256, 0.15052414,\n",
              "        0.28611469, 0.4364055 , 0.03680285, 0.15737081, 0.28341047,\n",
              "        0.41951609, 0.04110026, 0.14520152, 0.2877512 , 0.43643975,\n",
              "        0.03826706, 0.1525561 , 0.28027264, 0.42455276, 0.03693469,\n",
              "        0.14523554, 0.2879293 , 0.42229978, 0.03653971, 0.14446545,\n",
              "        0.28954506, 0.42053429, 0.04056875, 0.16319013, 0.31778296,\n",
              "        0.47783176, 0.04572646, 0.16461666, 0.31940985, 0.48252185,\n",
              "        0.04021899, 0.16200439, 0.31158249, 0.46601915, 0.04007824,\n",
              "        0.15897473, 0.30812319, 0.46444782, 0.04063058, 0.16757449,\n",
              "        0.32255816, 0.48391596, 0.04173533, 0.16349562, 0.32099279,\n",
              "        0.47366333, 0.04209916, 0.16413776, 0.30816483, 0.46665414,\n",
              "        0.03943976, 0.1595428 , 0.33147613, 0.46747883, 0.03749824,\n",
              "        0.14693483, 0.27852821, 0.4263742 , 0.03794654, 0.14843059,\n",
              "        0.28074233, 0.42122142, 0.03908022, 0.14497304, 0.28148254,\n",
              "        0.41844328, 0.0364569 , 0.14536277, 0.27570923, 0.41672508,\n",
              "        0.03934145, 0.15002489, 0.28960601, 0.42366529, 0.03924394,\n",
              "        0.14805698, 0.2901926 , 0.42902708, 0.03728342, 0.14103317,\n",
              "        0.28190947, 0.41161998, 0.03711772, 0.14918224, 0.28000975,\n",
              "        0.42572832, 0.04191828, 0.17080188, 0.32768067, 0.4928751 ,\n",
              "        0.04124141, 0.16917928, 0.33901683, 0.49137894, 0.04441969,\n",
              "        0.16122142, 0.31594992, 0.46889011, 0.04267025, 0.16381113,\n",
              "        0.31519659, 0.47679257, 0.04403321, 0.16847173, 0.33080236,\n",
              "        0.49782054, 0.04121304, 0.16879781, 0.33567882, 0.49179006,\n",
              "        0.04170068, 0.16409206, 0.3241992 , 0.48094765, 0.04098288,\n",
              "        0.16780957, 0.31527289, 0.3799564 ]),\n",
              " 'mean_score_time': array([0.01402855, 0.02848911, 0.04530787, 0.06618921, 0.01223183,\n",
              "        0.02745907, 0.04563904, 0.06898276, 0.01270882, 0.02848363,\n",
              "        0.04644847, 0.06447562, 0.0122691 , 0.02824577, 0.04484232,\n",
              "        0.06659897, 0.01253406, 0.02657731, 0.05142379, 0.06589643,\n",
              "        0.01312685, 0.02798676, 0.04571136, 0.07252224, 0.01259176,\n",
              "        0.02843769, 0.04632926, 0.06515002, 0.01258858, 0.02784308,\n",
              "        0.05240162, 0.06525604, 0.01265923, 0.02669907, 0.04698483,\n",
              "        0.06356017, 0.0131522 , 0.02657851, 0.0489351 , 0.06606452,\n",
              "        0.01263189, 0.02785428, 0.04509226, 0.06764174, 0.01266352,\n",
              "        0.02706234, 0.0452013 , 0.06904157, 0.01242526, 0.02760196,\n",
              "        0.04475602, 0.06348999, 0.01264882, 0.02713474, 0.04449677,\n",
              "        0.06502358, 0.01280324, 0.02622581, 0.0468146 , 0.06612039,\n",
              "        0.01257475, 0.02721834, 0.04971123, 0.06334337, 0.01301718,\n",
              "        0.02660489, 0.04464984, 0.0658466 , 0.0127422 , 0.0271244 ,\n",
              "        0.0446771 , 0.064346  , 0.01263706, 0.02610437, 0.04266842,\n",
              "        0.06560691, 0.01211691, 0.02672243, 0.04400643, 0.06595516,\n",
              "        0.01292888, 0.02679698, 0.04641501, 0.06420151, 0.01284552,\n",
              "        0.03195294, 0.04494635, 0.06377308, 0.01292761, 0.0268019 ,\n",
              "        0.04505952, 0.06506753, 0.01211818, 0.02754021, 0.04462155,\n",
              "        0.06615202, 0.01562651, 0.02716629, 0.04583637, 0.06538669,\n",
              "        0.01207463, 0.02655991, 0.04580053, 0.06358345, 0.01201804,\n",
              "        0.02709023, 0.04402614, 0.06379358, 0.01258198, 0.02650881,\n",
              "        0.0466698 , 0.06119784, 0.0131251 , 0.0275561 , 0.04542184,\n",
              "        0.06522377, 0.01252023, 0.0269084 , 0.0442334 , 0.06245812,\n",
              "        0.01304571, 0.02695775, 0.04826689, 0.06798967, 0.01265502,\n",
              "        0.02751724, 0.0466876 , 0.04267279]),\n",
              " 'mean_test_accuracy': array([0.90782828, 0.86742424, 0.85700758, 0.87752525, 0.90782828,\n",
              "        0.85732323, 0.85700758, 0.88794192, 0.84659091, 0.81628788,\n",
              "        0.86742424, 0.8677399 , 0.84659091, 0.81628788, 0.86742424,\n",
              "        0.8677399 , 0.90782828, 0.86742424, 0.85700758, 0.87752525,\n",
              "        0.90782828, 0.85732323, 0.85700758, 0.88794192, 0.84659091,\n",
              "        0.81628788, 0.86742424, 0.8677399 , 0.84659091, 0.81628788,\n",
              "        0.86742424, 0.8677399 , 0.84722222, 0.86742424, 0.86742424,\n",
              "        0.88794192, 0.85732323, 0.86742424, 0.85700758, 0.88794192,\n",
              "        0.82607323, 0.82638889, 0.85700758, 0.86742424, 0.82607323,\n",
              "        0.82638889, 0.85700758, 0.86742424, 0.84722222, 0.86742424,\n",
              "        0.86742424, 0.88794192, 0.85732323, 0.86742424, 0.85700758,\n",
              "        0.88794192, 0.82607323, 0.82638889, 0.85700758, 0.86742424,\n",
              "        0.82607323, 0.82638889, 0.85700758, 0.86742424, 0.85700758,\n",
              "        0.88825758, 0.87752525, 0.88794192, 0.85700758, 0.89804293,\n",
              "        0.86742424, 0.88794192, 0.88794192, 0.84690657, 0.85700758,\n",
              "        0.85700758, 0.88794192, 0.84690657, 0.85700758, 0.85700758,\n",
              "        0.85700758, 0.88825758, 0.87752525, 0.88794192, 0.85700758,\n",
              "        0.89804293, 0.86742424, 0.88794192, 0.88794192, 0.84690657,\n",
              "        0.85700758, 0.85700758, 0.88794192, 0.84690657, 0.85700758,\n",
              "        0.85700758, 0.8364899 , 0.84690657, 0.87752525, 0.88762626,\n",
              "        0.8364899 , 0.84690657, 0.87752525, 0.88762626, 0.81597222,\n",
              "        0.83680556, 0.85732323, 0.86710859, 0.81597222, 0.83680556,\n",
              "        0.85732323, 0.86710859, 0.8364899 , 0.84690657, 0.87752525,\n",
              "        0.88762626, 0.8364899 , 0.84690657, 0.87752525, 0.88762626,\n",
              "        0.81597222, 0.83680556, 0.85732323, 0.86710859, 0.81597222,\n",
              "        0.83680556, 0.85732323, 0.86710859]),\n",
              " 'mean_test_roc_auc': array([0.93040184, 0.93096179, 0.9513834 , 0.95862978, 0.93267457,\n",
              "        0.93751647, 0.95428195, 0.96014493, 0.92364954, 0.93517787,\n",
              "        0.9527668 , 0.96317523, 0.92364954, 0.93517787, 0.9527668 ,\n",
              "        0.96317523, 0.93040184, 0.93096179, 0.9513834 , 0.95862978,\n",
              "        0.93267457, 0.93751647, 0.95428195, 0.96014493, 0.92364954,\n",
              "        0.93517787, 0.9527668 , 0.96317523, 0.92364954, 0.93517787,\n",
              "        0.9527668 , 0.96317523, 0.90181159, 0.94331357, 0.95359025,\n",
              "        0.95586298, 0.90688406, 0.94104084, 0.95283267, 0.9588274 ,\n",
              "        0.89980237, 0.94242424, 0.95428195, 0.95724638, 0.89980237,\n",
              "        0.94242424, 0.95428195, 0.95724638, 0.90181159, 0.94331357,\n",
              "        0.95359025, 0.95586298, 0.90688406, 0.94104084, 0.95283267,\n",
              "        0.9588274 , 0.89980237, 0.94242424, 0.95428195, 0.95724638,\n",
              "        0.89980237, 0.94242424, 0.95428195, 0.95724638, 0.93083004,\n",
              "        0.95744401, 0.96613966, 0.96337286, 0.94025033, 0.95895916,\n",
              "        0.96324111, 0.96192358, 0.94749671, 0.96034256, 0.95724638,\n",
              "        0.95876153, 0.94749671, 0.96034256, 0.95724638, 0.95876153,\n",
              "        0.93083004, 0.95744401, 0.96613966, 0.96337286, 0.94025033,\n",
              "        0.95895916, 0.96324111, 0.96192358, 0.94749671, 0.96034256,\n",
              "        0.95724638, 0.95876153, 0.94749671, 0.96034256, 0.95724638,\n",
              "        0.95876153, 0.89140316, 0.9384058 , 0.95428195, 0.9513834 ,\n",
              "        0.89720026, 0.94130435, 0.94841897, 0.94993412, 0.90401845,\n",
              "        0.95144928, 0.95876153, 0.95737813, 0.90401845, 0.95144928,\n",
              "        0.95876153, 0.95737813, 0.89140316, 0.9384058 , 0.95428195,\n",
              "        0.9513834 , 0.89720026, 0.94130435, 0.94841897, 0.94993412,\n",
              "        0.90401845, 0.95144928, 0.95876153, 0.95737813, 0.90401845,\n",
              "        0.95144928, 0.95876153, 0.95737813]),\n",
              " 'param_clf__class_weight': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, 'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced',\n",
              "                    'balanced', 'balanced', 'balanced', 'balanced'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_clf__criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
              "                    'gini', 'gini', 'gini', 'gini', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
              "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_clf__max_depth': masked_array(data=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "                    10, 10, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, None, 10, 10,\n",
              "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, 10, 10, 10,\n",
              "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None, 10, 10, 10,\n",
              "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "                    None, None, None, None, None, None, None, None, None,\n",
              "                    None, None, None, None, None, None, None],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_clf__min_samples_leaf': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2,\n",
              "                    2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2,\n",
              "                    2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2,\n",
              "                    2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "                    4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4,\n",
              "                    4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4,\n",
              "                    4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4,\n",
              "                    4, 4],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_clf__min_samples_split': masked_array(data=[2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2,\n",
              "                    2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2,\n",
              "                    5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5,\n",
              "                    5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5,\n",
              "                    2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2,\n",
              "                    2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2,\n",
              "                    5, 5, 5, 5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5, 5,\n",
              "                    5, 5],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_clf__n_estimators': masked_array(data=[10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150, 10, 50, 100, 150,\n",
              "                    10, 50, 100, 150, 10, 50, 100, 150],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_scaler': masked_array(data=[StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              "                    StandardScaler(copy=True, with_mean=True, with_std=True)],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': None,\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'gini',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': 10,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 2,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 2,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 10,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 50,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 100,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)},\n",
              "  {'clf__class_weight': 'balanced',\n",
              "   'clf__criterion': 'entropy',\n",
              "   'clf__max_depth': None,\n",
              "   'clf__min_samples_leaf': 4,\n",
              "   'clf__min_samples_split': 5,\n",
              "   'clf__n_estimators': 150,\n",
              "   'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}],\n",
              " 'rank_test_accuracy': array([  1,  39,  69,  27,   1,  61,  69,   9, 101, 121,  39,  35, 101,\n",
              "        121,  39,  35,   1,  39,  69,  27,   1,  61,  69,   9, 101, 121,\n",
              "         39,  35, 101, 121,  39,  35,  91,  39,  39,   9,  61,  39,  69,\n",
              "          9, 117, 113,  69,  39, 117, 113,  69,  39,  91,  39,  39,   9,\n",
              "         61,  39,  69,   9, 117, 113,  69,  39, 117, 113,  69,  39,  69,\n",
              "          7,  27,   9,  69,   5,  39,   9,   9,  93,  69,  69,   9,  93,\n",
              "         69,  69,  69,   7,  27,   9,  69,   5,  39,   9,   9,  93,  69,\n",
              "         69,   9,  93,  69,  69, 109,  97,  27,  23, 109,  97,  27,  23,\n",
              "        125, 105,  61,  57, 125, 105,  61,  57, 109,  97,  27,  23, 109,\n",
              "         97,  27,  23, 125, 105,  61,  57, 125, 105,  61,  57], dtype=int32),\n",
              " 'rank_test_roc_auc': array([107, 103,  69,  31, 101,  95,  49,  17, 109,  97,  61,   7, 109,\n",
              "         97,  61,   7, 107, 103,  69,  31, 101,  95,  49,  17, 109,  97,\n",
              "         61,   7, 109,  97,  61,   7, 119,  81,  57,  47, 113,  89,  59,\n",
              "         21, 121,  83,  53,  39, 121,  83,  53,  39, 119,  81,  57,  47,\n",
              "        113,  89,  59,  21, 121,  83,  53,  39, 121,  83,  53,  39, 105,\n",
              "         33,   1,   3,  91,  19,   5,  11,  77,  13,  39,  23,  77,  13,\n",
              "         39,  23, 105,  33,   1,   3,  91,  19,   5,  11,  77,  13,  39,\n",
              "         23,  77,  13,  39,  23, 127,  93,  49,  69, 125,  87,  75,  73,\n",
              "        115,  65,  23,  35, 115,  65,  23,  35, 127,  93,  49,  69, 125,\n",
              "         87,  75,  73, 115,  65,  23,  35, 115,  65,  23,  35], dtype=int32),\n",
              " 'split0_test_accuracy': array([0.93939394, 0.84848485, 0.87878788, 0.87878788, 0.93939394,\n",
              "        0.81818182, 0.87878788, 0.87878788, 0.87878788, 0.78787879,\n",
              "        0.87878788, 0.84848485, 0.87878788, 0.78787879, 0.87878788,\n",
              "        0.84848485, 0.93939394, 0.84848485, 0.87878788, 0.87878788,\n",
              "        0.93939394, 0.81818182, 0.87878788, 0.87878788, 0.87878788,\n",
              "        0.78787879, 0.87878788, 0.84848485, 0.87878788, 0.78787879,\n",
              "        0.87878788, 0.84848485, 0.81818182, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.84848485, 0.84848485, 0.87878788, 0.87878788,\n",
              "        0.81818182, 0.78787879, 0.87878788, 0.87878788, 0.81818182,\n",
              "        0.78787879, 0.87878788, 0.87878788, 0.81818182, 0.84848485,\n",
              "        0.87878788, 0.87878788, 0.84848485, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.81818182, 0.78787879, 0.87878788, 0.87878788,\n",
              "        0.81818182, 0.78787879, 0.87878788, 0.87878788, 0.87878788,\n",
              "        0.84848485, 0.90909091, 0.84848485, 0.87878788, 0.87878788,\n",
              "        0.87878788, 0.84848485, 0.87878788, 0.84848485, 0.84848485,\n",
              "        0.84848485, 0.87878788, 0.84848485, 0.84848485, 0.84848485,\n",
              "        0.87878788, 0.84848485, 0.90909091, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.87878788, 0.84848485, 0.87878788, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.87878788, 0.84848485, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.78787879, 0.87878788, 0.87878788,\n",
              "        0.84848485, 0.78787879, 0.87878788, 0.87878788, 0.81818182,\n",
              "        0.81818182, 0.84848485, 0.84848485, 0.81818182, 0.81818182,\n",
              "        0.84848485, 0.84848485, 0.84848485, 0.78787879, 0.87878788,\n",
              "        0.87878788, 0.84848485, 0.78787879, 0.87878788, 0.87878788,\n",
              "        0.81818182, 0.81818182, 0.84848485, 0.84848485, 0.81818182,\n",
              "        0.81818182, 0.84848485, 0.84848485]),\n",
              " 'split0_test_roc_auc': array([0.92826087, 0.9326087 , 0.95217391, 0.96956522, 0.92826087,\n",
              "        0.93478261, 0.95652174, 0.96521739, 0.90869565, 0.93043478,\n",
              "        0.95652174, 0.95652174, 0.90869565, 0.93043478, 0.95652174,\n",
              "        0.95652174, 0.92826087, 0.9326087 , 0.95217391, 0.96956522,\n",
              "        0.92826087, 0.93478261, 0.95652174, 0.96521739, 0.90869565,\n",
              "        0.93043478, 0.95652174, 0.95652174, 0.90869565, 0.93043478,\n",
              "        0.95652174, 0.95652174, 0.88043478, 0.93913043, 0.94782609,\n",
              "        0.95217391, 0.88478261, 0.94347826, 0.94782609, 0.95217391,\n",
              "        0.86086957, 0.92608696, 0.94782609, 0.94782609, 0.86086957,\n",
              "        0.92608696, 0.94782609, 0.94782609, 0.88043478, 0.93913043,\n",
              "        0.94782609, 0.95217391, 0.88478261, 0.94347826, 0.94782609,\n",
              "        0.95217391, 0.86086957, 0.92608696, 0.94782609, 0.94782609,\n",
              "        0.86086957, 0.92608696, 0.94782609, 0.94782609, 0.93913043,\n",
              "        0.94782609, 0.96086957, 0.94782609, 0.95      , 0.94782609,\n",
              "        0.96086957, 0.94782609, 0.95      , 0.95652174, 0.95217391,\n",
              "        0.95217391, 0.95      , 0.95652174, 0.95217391, 0.95217391,\n",
              "        0.93913043, 0.94782609, 0.96086957, 0.94782609, 0.95      ,\n",
              "        0.94782609, 0.96086957, 0.94782609, 0.95      , 0.95652174,\n",
              "        0.95217391, 0.95217391, 0.95      , 0.95652174, 0.95217391,\n",
              "        0.95217391, 0.94130435, 0.93478261, 0.95652174, 0.94782609,\n",
              "        0.94565217, 0.93913043, 0.94782609, 0.94347826, 0.90434783,\n",
              "        0.92608696, 0.95217391, 0.94782609, 0.90434783, 0.92608696,\n",
              "        0.95217391, 0.94782609, 0.94130435, 0.93478261, 0.95652174,\n",
              "        0.94782609, 0.94565217, 0.93913043, 0.94782609, 0.94347826,\n",
              "        0.90434783, 0.92608696, 0.95217391, 0.94782609, 0.90434783,\n",
              "        0.92608696, 0.95217391, 0.94782609]),\n",
              " 'split1_test_accuracy': array([0.90909091, 0.87878788, 0.84848485, 0.87878788, 0.90909091,\n",
              "        0.87878788, 0.84848485, 0.87878788, 0.84848485, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.84848485, 0.84848485, 0.84848485,\n",
              "        0.84848485, 0.90909091, 0.87878788, 0.84848485, 0.87878788,\n",
              "        0.90909091, 0.87878788, 0.84848485, 0.87878788, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.84848485, 0.84848485, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.84848485, 0.87878788, 0.84848485,\n",
              "        0.87878788, 0.84848485, 0.87878788, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.87878788, 0.84848485, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.84848485, 0.84848485, 0.84848485, 0.87878788,\n",
              "        0.84848485, 0.87878788, 0.84848485, 0.87878788, 0.84848485,\n",
              "        0.87878788, 0.87878788, 0.87878788, 0.84848485, 0.84848485,\n",
              "        0.87878788, 0.87878788, 0.84848485, 0.84848485, 0.84848485,\n",
              "        0.87878788, 0.84848485, 0.90909091, 0.84848485, 0.90909091,\n",
              "        0.84848485, 0.90909091, 0.87878788, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.87878788, 0.84848485, 0.87878788, 0.87878788,\n",
              "        0.84848485, 0.87878788, 0.84848485, 0.90909091, 0.84848485,\n",
              "        0.90909091, 0.84848485, 0.90909091, 0.87878788, 0.84848485,\n",
              "        0.87878788, 0.87878788, 0.87878788, 0.84848485, 0.87878788,\n",
              "        0.87878788, 0.84848485, 0.90909091, 0.87878788, 0.90909091,\n",
              "        0.84848485, 0.90909091, 0.87878788, 0.90909091, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.90909091, 0.84848485, 0.84848485,\n",
              "        0.84848485, 0.90909091, 0.84848485, 0.90909091, 0.87878788,\n",
              "        0.90909091, 0.84848485, 0.90909091, 0.87878788, 0.90909091,\n",
              "        0.84848485, 0.84848485, 0.84848485, 0.90909091, 0.84848485,\n",
              "        0.84848485, 0.84848485, 0.90909091]),\n",
              " 'split1_test_roc_auc': array([0.91521739, 0.92391304, 0.95652174, 0.96086957, 0.91521739,\n",
              "        0.93913043, 0.96086957, 0.96521739, 0.93043478, 0.94782609,\n",
              "        0.96086957, 0.97391304, 0.93043478, 0.94782609, 0.96086957,\n",
              "        0.97391304, 0.91521739, 0.92391304, 0.95652174, 0.96086957,\n",
              "        0.91521739, 0.93913043, 0.96086957, 0.96521739, 0.93043478,\n",
              "        0.94782609, 0.96086957, 0.97391304, 0.93043478, 0.94782609,\n",
              "        0.96086957, 0.97391304, 0.9       , 0.95217391, 0.96521739,\n",
              "        0.96086957, 0.91086957, 0.94782609, 0.96521739, 0.96521739,\n",
              "        0.95217391, 0.97391304, 0.96956522, 0.97391304, 0.95217391,\n",
              "        0.97391304, 0.96956522, 0.97391304, 0.9       , 0.95217391,\n",
              "        0.96521739, 0.96086957, 0.91086957, 0.94782609, 0.96521739,\n",
              "        0.96521739, 0.95217391, 0.97391304, 0.96956522, 0.97391304,\n",
              "        0.95217391, 0.97391304, 0.96956522, 0.97391304, 0.92608696,\n",
              "        0.96086957, 0.97391304, 0.96956522, 0.94347826, 0.96086957,\n",
              "        0.96521739, 0.96521739, 0.96521739, 0.96086957, 0.96956522,\n",
              "        0.96956522, 0.96521739, 0.96086957, 0.96956522, 0.96956522,\n",
              "        0.92608696, 0.96086957, 0.97391304, 0.96956522, 0.94347826,\n",
              "        0.96086957, 0.96521739, 0.96521739, 0.96521739, 0.96086957,\n",
              "        0.96956522, 0.96956522, 0.96521739, 0.96086957, 0.96956522,\n",
              "        0.96956522, 0.87608696, 0.93043478, 0.96086957, 0.96086957,\n",
              "        0.88913043, 0.93478261, 0.95652174, 0.96086957, 0.93043478,\n",
              "        0.97826087, 0.96956522, 0.96521739, 0.93043478, 0.97826087,\n",
              "        0.96956522, 0.96521739, 0.87608696, 0.93043478, 0.96086957,\n",
              "        0.96086957, 0.88913043, 0.93478261, 0.95652174, 0.96086957,\n",
              "        0.93043478, 0.97826087, 0.96956522, 0.96521739, 0.93043478,\n",
              "        0.97826087, 0.96956522, 0.96521739]),\n",
              " 'split2_test_accuracy': array([0.875  , 0.875  , 0.84375, 0.875  , 0.875  , 0.875  , 0.84375,\n",
              "        0.90625, 0.8125 , 0.8125 , 0.875  , 0.90625, 0.8125 , 0.8125 ,\n",
              "        0.875  , 0.90625, 0.875  , 0.875  , 0.84375, 0.875  , 0.875  ,\n",
              "        0.875  , 0.84375, 0.90625, 0.8125 , 0.8125 , 0.875  , 0.90625,\n",
              "        0.8125 , 0.8125 , 0.875  , 0.90625, 0.875  , 0.875  , 0.875  ,\n",
              "        0.90625, 0.875  , 0.875  , 0.84375, 0.90625, 0.78125, 0.8125 ,\n",
              "        0.84375, 0.875  , 0.78125, 0.8125 , 0.84375, 0.875  , 0.875  ,\n",
              "        0.875  , 0.875  , 0.90625, 0.875  , 0.875  , 0.84375, 0.90625,\n",
              "        0.78125, 0.8125 , 0.84375, 0.875  , 0.78125, 0.8125 , 0.84375,\n",
              "        0.875  , 0.84375, 0.9375 , 0.875  , 0.90625, 0.84375, 0.90625,\n",
              "        0.875  , 0.90625, 0.90625, 0.84375, 0.84375, 0.84375, 0.90625,\n",
              "        0.84375, 0.84375, 0.84375, 0.84375, 0.9375 , 0.875  , 0.90625,\n",
              "        0.84375, 0.90625, 0.875  , 0.90625, 0.90625, 0.84375, 0.84375,\n",
              "        0.84375, 0.90625, 0.84375, 0.84375, 0.84375, 0.8125 , 0.84375,\n",
              "        0.875  , 0.875  , 0.8125 , 0.84375, 0.875  , 0.875  , 0.78125,\n",
              "        0.84375, 0.875  , 0.84375, 0.78125, 0.84375, 0.875  , 0.84375,\n",
              "        0.8125 , 0.84375, 0.875  , 0.875  , 0.8125 , 0.84375, 0.875  ,\n",
              "        0.875  , 0.78125, 0.84375, 0.875  , 0.84375, 0.78125, 0.84375,\n",
              "        0.875  , 0.84375]),\n",
              " 'split2_test_roc_auc': array([0.94772727, 0.93636364, 0.94545455, 0.94545455, 0.95454545,\n",
              "        0.93863636, 0.94545455, 0.95      , 0.93181818, 0.92727273,\n",
              "        0.94090909, 0.95909091, 0.93181818, 0.92727273, 0.94090909,\n",
              "        0.95909091, 0.94772727, 0.93636364, 0.94545455, 0.94545455,\n",
              "        0.95454545, 0.93863636, 0.94545455, 0.95      , 0.93181818,\n",
              "        0.92727273, 0.94090909, 0.95909091, 0.93181818, 0.92727273,\n",
              "        0.94090909, 0.95909091, 0.925     , 0.93863636, 0.94772727,\n",
              "        0.95454545, 0.925     , 0.93181818, 0.94545455, 0.95909091,\n",
              "        0.88636364, 0.92727273, 0.94545455, 0.95      , 0.88636364,\n",
              "        0.92727273, 0.94545455, 0.95      , 0.925     , 0.93863636,\n",
              "        0.94772727, 0.95454545, 0.925     , 0.93181818, 0.94545455,\n",
              "        0.95909091, 0.88636364, 0.92727273, 0.94545455, 0.95      ,\n",
              "        0.88636364, 0.92727273, 0.94545455, 0.95      , 0.92727273,\n",
              "        0.96363636, 0.96363636, 0.97272727, 0.92727273, 0.96818182,\n",
              "        0.96363636, 0.97272727, 0.92727273, 0.96363636, 0.95      ,\n",
              "        0.95454545, 0.92727273, 0.96363636, 0.95      , 0.95454545,\n",
              "        0.92727273, 0.96363636, 0.96363636, 0.97272727, 0.92727273,\n",
              "        0.96818182, 0.96363636, 0.97272727, 0.92727273, 0.96363636,\n",
              "        0.95      , 0.95454545, 0.92727273, 0.96363636, 0.95      ,\n",
              "        0.95454545, 0.85681818, 0.95      , 0.94545455, 0.94545455,\n",
              "        0.85681818, 0.95      , 0.94090909, 0.94545455, 0.87727273,\n",
              "        0.95      , 0.95454545, 0.95909091, 0.87727273, 0.95      ,\n",
              "        0.95454545, 0.95909091, 0.85681818, 0.95      , 0.94545455,\n",
              "        0.94545455, 0.85681818, 0.95      , 0.94090909, 0.94545455,\n",
              "        0.87727273, 0.95      , 0.95454545, 0.95909091, 0.87727273,\n",
              "        0.95      , 0.95454545, 0.95909091]),\n",
              " 'std_fit_time': array([0.00600516, 0.00533907, 0.00894167, 0.00419145, 0.00038758,\n",
              "        0.00210501, 0.0078749 , 0.00724519, 0.00104781, 0.00537151,\n",
              "        0.0043494 , 0.01354521, 0.00056547, 0.0116985 , 0.00596467,\n",
              "        0.00999507, 0.00548326, 0.00120153, 0.00428955, 0.00556373,\n",
              "        0.00082534, 0.00448438, 0.00078125, 0.0024829 , 0.00080015,\n",
              "        0.0017738 , 0.00725762, 0.00726226, 0.00071757, 0.00109407,\n",
              "        0.00548777, 0.00637396, 0.00021457, 0.00015271, 0.0051678 ,\n",
              "        0.00383335, 0.00608087, 0.00475779, 0.00220188, 0.00784231,\n",
              "        0.00053356, 0.00279919, 0.006175  , 0.00825194, 0.00026466,\n",
              "        0.00038436, 0.00244198, 0.00269751, 0.00095946, 0.00118992,\n",
              "        0.00578048, 0.01506057, 0.00082198, 0.00111965, 0.00282945,\n",
              "        0.00172565, 0.00313888, 0.0069771 , 0.003406  , 0.00812859,\n",
              "        0.00077373, 0.00220296, 0.01105096, 0.00531141, 0.00063738,\n",
              "        0.00272374, 0.00349249, 0.00633494, 0.00050412, 0.00452867,\n",
              "        0.00357568, 0.00418307, 0.00276862, 0.00384592, 0.0069911 ,\n",
              "        0.00951502, 0.00034538, 0.00174076, 0.00234994, 0.00684159,\n",
              "        0.0015262 , 0.0046166 , 0.0102231 , 0.00318145, 0.00066263,\n",
              "        0.00340529, 0.00547223, 0.00813875, 0.00052571, 0.00076336,\n",
              "        0.00543841, 0.00329611, 0.00061715, 0.00439932, 0.00802508,\n",
              "        0.00737674, 0.00074746, 0.00405548, 0.00226716, 0.0093245 ,\n",
              "        0.00045757, 0.00305266, 0.01027475, 0.01240918, 0.00618689,\n",
              "        0.00188383, 0.00375972, 0.00604952, 0.00311872, 0.00383417,\n",
              "        0.00234719, 0.00245862, 0.00189676, 0.00290218, 0.00360198,\n",
              "        0.00933422, 0.00099536, 0.00208489, 0.00830666, 0.00422738,\n",
              "        0.00255989, 0.00173731, 0.00864537, 0.00833873, 0.0006125 ,\n",
              "        0.00538904, 0.00182432, 0.08594617]),\n",
              " 'std_score_time': array([2.92488708e-03, 6.23951853e-04, 1.68207443e-03, 5.79853863e-04,\n",
              "        3.03405792e-04, 9.56209495e-04, 1.20129592e-03, 4.34986774e-03,\n",
              "        4.42183635e-04, 6.10887986e-04, 1.10625075e-03, 2.44918878e-03,\n",
              "        3.41143210e-05, 5.21763044e-04, 5.27086641e-04, 7.50430607e-04,\n",
              "        2.78983178e-04, 6.74820595e-04, 6.23435876e-03, 1.30489972e-03,\n",
              "        3.70576261e-04, 9.72864726e-04, 8.96179747e-04, 1.03291494e-02,\n",
              "        2.43775455e-04, 1.63135618e-04, 7.21326181e-04, 2.37020465e-03,\n",
              "        3.61579695e-04, 9.77597918e-04, 7.53171945e-03, 3.48841060e-03,\n",
              "        6.37537799e-05, 1.05984624e-03, 7.23515406e-04, 2.00521327e-03,\n",
              "        1.60954301e-04, 2.66572111e-04, 6.59239151e-03, 9.69880105e-04,\n",
              "        5.29296978e-04, 7.74523010e-04, 1.20880816e-03, 4.43210575e-03,\n",
              "        2.36404361e-04, 1.13302229e-03, 1.56845308e-03, 2.22948065e-03,\n",
              "        4.44010692e-04, 5.35775017e-04, 1.40244367e-03, 3.03944436e-03,\n",
              "        5.89608616e-04, 1.24313714e-03, 1.81031183e-03, 1.52256946e-03,\n",
              "        1.07153054e-04, 4.78946194e-04, 2.15169316e-03, 4.38800862e-03,\n",
              "        2.71117069e-04, 1.28984047e-03, 4.79644773e-03, 2.59881409e-03,\n",
              "        2.63043304e-04, 1.12721038e-03, 1.67641463e-03, 1.02375443e-03,\n",
              "        3.26731088e-05, 8.62969893e-04, 1.81915855e-03, 2.05194811e-03,\n",
              "        4.31892403e-05, 3.89242474e-04, 6.56193692e-05, 1.92417757e-03,\n",
              "        7.10825978e-05, 8.93926673e-04, 7.61332044e-04, 1.69798861e-03,\n",
              "        2.65309236e-04, 1.48596458e-03, 1.43932455e-03, 4.17192967e-03,\n",
              "        1.89963310e-04, 4.73346242e-03, 1.37877405e-03, 1.03779041e-03,\n",
              "        7.16638112e-05, 3.09435251e-04, 8.98250180e-04, 2.15146068e-03,\n",
              "        6.21525526e-05, 3.78620464e-04, 1.55892858e-03, 1.98217811e-03,\n",
              "        3.85076300e-03, 8.56511046e-04, 1.06541571e-03, 1.48062212e-03,\n",
              "        6.46005346e-05, 5.32143871e-04, 2.05239563e-03, 1.71093487e-03,\n",
              "        1.23982755e-05, 1.13876305e-03, 4.12568775e-04, 2.65873522e-03,\n",
              "        2.35376125e-04, 6.78094211e-04, 5.75646879e-04, 1.36212684e-03,\n",
              "        8.55641140e-05, 1.01038681e-03, 1.03060640e-03, 5.28230601e-04,\n",
              "        3.94831549e-04, 8.64724521e-04, 2.44330034e-03, 1.89404007e-03,\n",
              "        1.41551604e-03, 1.47180010e-03, 2.23486582e-03, 3.35094440e-03,\n",
              "        8.05349116e-05, 1.02260606e-03, 3.46386231e-04, 1.27815993e-02]),\n",
              " 'std_test_accuracy': array([0.02630387, 0.01348116, 0.01552183, 0.00178562, 0.02630387,\n",
              "        0.02772033, 0.01552183, 0.01294577, 0.02709503, 0.02488687,\n",
              "        0.01348116, 0.02723075, 0.02709503, 0.02488687, 0.01348116,\n",
              "        0.02723075, 0.02630387, 0.01348116, 0.01552183, 0.00178562,\n",
              "        0.02630387, 0.02772033, 0.01552183, 0.01294577, 0.02709503,\n",
              "        0.02488687, 0.01348116, 0.02723075, 0.02709503, 0.02488687,\n",
              "        0.01348116, 0.02723075, 0.0232131 , 0.01348116, 0.01348116,\n",
              "        0.01294577, 0.01249936, 0.01348116, 0.01552183, 0.01294577,\n",
              "        0.04020875, 0.0383909 , 0.01552183, 0.01348116, 0.04020875,\n",
              "        0.0383909 , 0.01552183, 0.01348116, 0.0232131 , 0.01348116,\n",
              "        0.01348116, 0.01294577, 0.01249936, 0.01348116, 0.01552183,\n",
              "        0.01294577, 0.04020875, 0.0383909 , 0.01552183, 0.01348116,\n",
              "        0.04020875, 0.0383909 , 0.01552183, 0.01348116, 0.01552183,\n",
              "        0.03695205, 0.02480667, 0.02792446, 0.01552183, 0.01366468,\n",
              "        0.01348116, 0.02792446, 0.01294577, 0.00223203, 0.01552183,\n",
              "        0.01552183, 0.01294577, 0.00223203, 0.01552183, 0.01552183,\n",
              "        0.01552183, 0.03695205, 0.02480667, 0.02792446, 0.01552183,\n",
              "        0.01366468, 0.01348116, 0.02792446, 0.01294577, 0.00223203,\n",
              "        0.01552183, 0.01552183, 0.01294577, 0.00223203, 0.01552183,\n",
              "        0.01552183, 0.01696342, 0.04953495, 0.00178562, 0.01525637,\n",
              "        0.01696342, 0.04953495, 0.00178562, 0.01525637, 0.02749294,\n",
              "        0.01331008, 0.01249936, 0.02974885, 0.02749294, 0.01331008,\n",
              "        0.01249936, 0.02974885, 0.01696342, 0.04953495, 0.00178562,\n",
              "        0.01525637, 0.01696342, 0.04953495, 0.00178562, 0.01525637,\n",
              "        0.02749294, 0.01331008, 0.01249936, 0.02974885, 0.02749294,\n",
              "        0.01331008, 0.01249936, 0.02974885]),\n",
              " 'std_test_roc_auc': array([0.01335817, 0.00521463, 0.00455261, 0.00996974, 0.01635613,\n",
              "        0.00194363, 0.00648939, 0.00717355, 0.01058907, 0.00903633,\n",
              "        0.00857048, 0.00766488, 0.01058907, 0.00903633, 0.00857048,\n",
              "        0.00766488, 0.01335817, 0.00521463, 0.00455261, 0.00996974,\n",
              "        0.01635613, 0.00194363, 0.00648939, 0.00717355, 0.01058907,\n",
              "        0.00903633, 0.00857048, 0.00766488, 0.01058907, 0.00903633,\n",
              "        0.00857048, 0.00766488, 0.01823871, 0.00626845, 0.00822173,\n",
              "        0.00367019, 0.01665879, 0.00675865, 0.00881067, 0.00532824,\n",
              "        0.03846705, 0.02227121, 0.01085018, 0.01181848, 0.03846705,\n",
              "        0.02227121, 0.01085018, 0.01181848, 0.01823871, 0.00626845,\n",
              "        0.00822173, 0.00367019, 0.01665879, 0.00675865, 0.00881067,\n",
              "        0.00532824, 0.03846705, 0.02227121, 0.01085018, 0.01181848,\n",
              "        0.03846705, 0.02227121, 0.01085018, 0.01181848, 0.0058892 ,\n",
              "        0.00689406, 0.00561147, 0.01106876, 0.00955499, 0.00841927,\n",
              "        0.00179686, 0.01042926, 0.01559165, 0.00292834, 0.00875583,\n",
              "        0.00770047, 0.01559165, 0.00292834, 0.00875583, 0.00770047,\n",
              "        0.0058892 , 0.00689406, 0.00561147, 0.01106876, 0.00955499,\n",
              "        0.00841927, 0.00179686, 0.01042926, 0.01559165, 0.00292834,\n",
              "        0.00875583, 0.00770047, 0.01559165, 0.00292834, 0.00875583,\n",
              "        0.00770047, 0.0361517 , 0.00838829, 0.00648939, 0.00677724,\n",
              "        0.0367125 , 0.00639983, 0.00638761, 0.0077745 , 0.02170457,\n",
              "        0.02132455, 0.00770047, 0.00720253, 0.02170457, 0.02132455,\n",
              "        0.00770047, 0.00720253, 0.0361517 , 0.00838829, 0.00648939,\n",
              "        0.00677724, 0.0367125 , 0.00639983, 0.00638761, 0.0077745 ,\n",
              "        0.02170457, 0.02132455, 0.00770047, 0.00720253, 0.02170457,\n",
              "        0.02132455, 0.00770047, 0.00720253])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC0l9m7jzBpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pd.DataFrame.from_dict(GSCV.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8RgFBOQz16S",
        "colab_type": "code",
        "outputId": "3a004323-93c0-4db0-a69e-367accf45c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf__class_weight</th>\n",
              "      <th>param_clf__criterion</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__min_samples_leaf</th>\n",
              "      <th>param_clf__min_samples_split</th>\n",
              "      <th>param_clf__n_estimators</th>\n",
              "      <th>param_scaler</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_roc_auc</th>\n",
              "      <th>split1_test_roc_auc</th>\n",
              "      <th>split2_test_roc_auc</th>\n",
              "      <th>mean_test_roc_auc</th>\n",
              "      <th>std_test_roc_auc</th>\n",
              "      <th>rank_test_roc_auc</th>\n",
              "      <th>split0_test_accuracy</th>\n",
              "      <th>split1_test_accuracy</th>\n",
              "      <th>split2_test_accuracy</th>\n",
              "      <th>mean_test_accuracy</th>\n",
              "      <th>std_test_accuracy</th>\n",
              "      <th>rank_test_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.042926</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.014029</td>\n",
              "      <td>0.002925</td>\n",
              "      <td>None</td>\n",
              "      <td>gini</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
              "      <td>0.928261</td>\n",
              "      <td>0.915217</td>\n",
              "      <td>0.947727</td>\n",
              "      <td>0.930402</td>\n",
              "      <td>0.013358</td>\n",
              "      <td>107</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.87500</td>\n",
              "      <td>0.907828</td>\n",
              "      <td>0.026304</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.159799</td>\n",
              "      <td>0.005339</td>\n",
              "      <td>0.028489</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>None</td>\n",
              "      <td>gini</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
              "      <td>0.932609</td>\n",
              "      <td>0.923913</td>\n",
              "      <td>0.936364</td>\n",
              "      <td>0.930962</td>\n",
              "      <td>0.005215</td>\n",
              "      <td>103</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.87500</td>\n",
              "      <td>0.867424</td>\n",
              "      <td>0.013481</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.289042</td>\n",
              "      <td>0.008942</td>\n",
              "      <td>0.045308</td>\n",
              "      <td>0.001682</td>\n",
              "      <td>None</td>\n",
              "      <td>gini</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
              "      <td>0.952174</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.945455</td>\n",
              "      <td>0.951383</td>\n",
              "      <td>0.004553</td>\n",
              "      <td>69</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.857008</td>\n",
              "      <td>0.015522</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.427041</td>\n",
              "      <td>0.004191</td>\n",
              "      <td>0.066189</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>None</td>\n",
              "      <td>gini</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
              "      <td>0.969565</td>\n",
              "      <td>0.960870</td>\n",
              "      <td>0.945455</td>\n",
              "      <td>0.958630</td>\n",
              "      <td>0.009970</td>\n",
              "      <td>31</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.87500</td>\n",
              "      <td>0.877525</td>\n",
              "      <td>0.001786</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.036037</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>0.012232</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>None</td>\n",
              "      <td>gini</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
              "      <td>0.928261</td>\n",
              "      <td>0.915217</td>\n",
              "      <td>0.954545</td>\n",
              "      <td>0.932675</td>\n",
              "      <td>0.016356</td>\n",
              "      <td>101</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.87500</td>\n",
              "      <td>0.907828</td>\n",
              "      <td>0.026304</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>0.480948</td>\n",
              "      <td>0.008339</td>\n",
              "      <td>0.067990</td>\n",
              "      <td>0.003351</td>\n",
              "      <td>balanced</td>\n",
              "      <td>entropy</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
              "      <td>0.947826</td>\n",
              "      <td>0.965217</td>\n",
              "      <td>0.959091</td>\n",
              "      <td>0.957378</td>\n",
              "      <td>0.007203</td>\n",
              "      <td>35</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.867109</td>\n",
              "      <td>0.029749</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>0.040983</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.012655</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>balanced</td>\n",
              "      <td>entropy</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
              "      <td>0.904348</td>\n",
              "      <td>0.930435</td>\n",
              "      <td>0.877273</td>\n",
              "      <td>0.904018</td>\n",
              "      <td>0.021705</td>\n",
              "      <td>115</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.78125</td>\n",
              "      <td>0.815972</td>\n",
              "      <td>0.027493</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>0.167810</td>\n",
              "      <td>0.005389</td>\n",
              "      <td>0.027517</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>balanced</td>\n",
              "      <td>entropy</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
              "      <td>0.926087</td>\n",
              "      <td>0.978261</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.951449</td>\n",
              "      <td>0.021325</td>\n",
              "      <td>65</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.836806</td>\n",
              "      <td>0.013310</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0.315273</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.046688</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>balanced</td>\n",
              "      <td>entropy</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>100</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
              "      <td>0.952174</td>\n",
              "      <td>0.969565</td>\n",
              "      <td>0.954545</td>\n",
              "      <td>0.958762</td>\n",
              "      <td>0.007700</td>\n",
              "      <td>23</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.87500</td>\n",
              "      <td>0.857323</td>\n",
              "      <td>0.012499</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>0.379956</td>\n",
              "      <td>0.085946</td>\n",
              "      <td>0.042673</td>\n",
              "      <td>0.012782</td>\n",
              "      <td>balanced</td>\n",
              "      <td>entropy</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>150</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
              "      <td>0.947826</td>\n",
              "      <td>0.965217</td>\n",
              "      <td>0.959091</td>\n",
              "      <td>0.957378</td>\n",
              "      <td>0.007203</td>\n",
              "      <td>35</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.867109</td>\n",
              "      <td>0.029749</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean_fit_time  std_fit_time  ...  std_test_accuracy  rank_test_accuracy\n",
              "0         0.042926      0.006005  ...           0.026304                   1\n",
              "1         0.159799      0.005339  ...           0.013481                  39\n",
              "2         0.289042      0.008942  ...           0.015522                  69\n",
              "3         0.427041      0.004191  ...           0.001786                  27\n",
              "4         0.036037      0.000388  ...           0.026304                   1\n",
              "..             ...           ...  ...                ...                 ...\n",
              "123       0.480948      0.008339  ...           0.029749                  57\n",
              "124       0.040983      0.000613  ...           0.027493                 125\n",
              "125       0.167810      0.005389  ...           0.013310                 105\n",
              "126       0.315273      0.001824  ...           0.012499                  61\n",
              "127       0.379956      0.085946  ...           0.029749                  57\n",
              "\n",
              "[128 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne4HVcIfz3Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}