{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BRATS_Kfold_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMtyJ5IuBoAD3ZsC3TGVD7H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/BRATS_Kfold_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5eq9ZaHU43Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "bf5d34c2-4122-440f-92a1-59aaa8e5588c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_without_HISTO_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "b56fd242-df47-47c9-f7e9-8f1765811d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>INTENSITY_Mean_NET_FLAIR</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>227.7510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>209.7901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>165.1014</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>207.5531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>139.3234</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>195.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>176.3511</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>152.2227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>161.5295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>163.4821</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 587 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               ID        Date  VOLUME_ET  ...  TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0    TCGA-02-0006  1996.08.23       1662  ...     0.052741    2.00000        GBM\n",
              "1    TCGA-02-0009  1997.06.14       4362  ...     0.094336   91.47360        GBM\n",
              "2    TCGA-02-0011  1998.02.01      33404  ...     0.096035  272.42900        GBM\n",
              "3    TCGA-02-0027  1999.03.28      12114  ...     0.096470  128.46800        GBM\n",
              "4    TCGA-02-0033  1997.05.26      34538  ...     0.096894  240.77800        GBM\n",
              "..            ...         ...        ...  ...          ...        ...        ...\n",
              "141  TCGA-HT-7694  1995.04.04       1036  ...     0.090456  719.23800        LGG\n",
              "142  TCGA-HT-8018  1997.04.11       2093  ...     0.054307    2.00000        LGG\n",
              "143  TCGA-HT-8111  1998.03.30       1929  ...     0.126712    7.06744        LGG\n",
              "144  TCGA-HT-8114  1998.10.30       8755  ...     0.107433   15.52240        LGG\n",
              "145  TCGA-HT-8563  1998.12.09      11757  ...     0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 587 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "22b1fea1-be9e-4d6e-b454-366a7858a3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Date', 'VOLUME_ET', 'VOLUME_NET', 'VOLUME_ED', 'VOLUME_TC',\n",
              "       'VOLUME_WT', 'VOLUME_BRAIN', 'VOLUME_ET_OVER_NET', 'VOLUME_ET_OVER_ED',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=587)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fwd5eG6-OwL",
        "colab_type": "text"
      },
      "source": [
        "##Encode labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4dpKt6lia-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "labels_enc = encoder.fit_transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QEgnmw_EFl2L"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztWE5G71Fl2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4c821de-49ad-4fae-c60b-b7fbaea89c30"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mfSvItJFl2i",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lFKekeMCFl2p",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2ii7-J9Fl26",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(584,), kernel_regularizer=regularizers.l2(l=0.05)))\n",
        "  #model.add(layers.Dropout(0.01))\n",
        "  #model.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(l=0.05)))\n",
        "  #model.add(layers.Dropout(0.01))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.05, momentum=0.9)\n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  rmsprop = RMSprop(lr=0.001)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lBjwz4bMFl3G",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, factor=0.1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_CdS5EKbFl3W",
        "colab": {}
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, train_labels_enc, validation_data=(val_data_stand, val_labels_enc), \n",
        "                      epochs= num_epochs, batch_size=92, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P2soYx08D7Iw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_index, test_index in skf.split(data, labels_enc):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kyODTMZDD6wU",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m_zTCw2eD5vo",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mkMu7mEID21f",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "0024694a-b25f-4dea-97c1-4b5af1384129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "cvscores = []\n",
        "\n",
        "for train_index, val_index in skf.split(data, labels_enc):\n",
        " \n",
        "  partial_train_data = data.iloc[train_index, :]\n",
        "  partial_train_targets = labels_enc[train_index]\n",
        "  \n",
        "  val_data = data.iloc[val_index, :]\n",
        "  val_targets = labels_enc[val_index]\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  partial_train_data_scaled = scaler.fit_transform(partial_train_data)\n",
        "  val_data_scaled = scaler.transform(val_data)\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data_scaled, partial_train_targets, validation_data=(val_data_scaled, val_targets), \n",
        "                      epochs=num_epochs, batch_size=92, callbacks=[red_lr])\n",
        "  \n",
        "  acc_history = history.history['accuracy']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_accuracy']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "  scores = model.evaluate(val_data_scaled, val_targets)\n",
        "  cvscores.append(scores)\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 116 samples, validate on 30 samples\n",
            "Epoch 1/100\n",
            "116/116 [==============================] - 0s 892us/step - loss: 2.6877 - accuracy: 0.4138 - val_loss: 2.2369 - val_accuracy: 0.7000\n",
            "Epoch 2/100\n",
            "116/116 [==============================] - 0s 71us/step - loss: 2.1355 - accuracy: 0.6983 - val_loss: 1.6126 - val_accuracy: 0.8333\n",
            "Epoch 3/100\n",
            "116/116 [==============================] - 0s 66us/step - loss: 1.6045 - accuracy: 0.8276 - val_loss: 1.2394 - val_accuracy: 0.8000\n",
            "Epoch 4/100\n",
            "116/116 [==============================] - 0s 73us/step - loss: 1.2533 - accuracy: 0.7759 - val_loss: 1.0159 - val_accuracy: 0.7667\n",
            "Epoch 5/100\n",
            "116/116 [==============================] - 0s 73us/step - loss: 1.0374 - accuracy: 0.7586 - val_loss: 0.8961 - val_accuracy: 0.9333\n",
            "Epoch 6/100\n",
            "116/116 [==============================] - 0s 78us/step - loss: 0.9409 - accuracy: 0.8793 - val_loss: 0.7927 - val_accuracy: 0.9000\n",
            "Epoch 7/100\n",
            "116/116 [==============================] - 0s 79us/step - loss: 0.8386 - accuracy: 0.8448 - val_loss: 0.7488 - val_accuracy: 0.8333\n",
            "Epoch 8/100\n",
            "116/116 [==============================] - 0s 84us/step - loss: 0.7791 - accuracy: 0.8534 - val_loss: 0.7245 - val_accuracy: 0.9000\n",
            "Epoch 9/100\n",
            "116/116 [==============================] - 0s 73us/step - loss: 0.7696 - accuracy: 0.8966 - val_loss: 0.6482 - val_accuracy: 0.9333\n",
            "Epoch 10/100\n",
            "116/116 [==============================] - 0s 84us/step - loss: 0.6994 - accuracy: 0.8707 - val_loss: 0.6086 - val_accuracy: 0.9000\n",
            "Epoch 11/100\n",
            "116/116 [==============================] - 0s 74us/step - loss: 0.6420 - accuracy: 0.8793 - val_loss: 0.5712 - val_accuracy: 0.9333\n",
            "Epoch 12/100\n",
            "116/116 [==============================] - 0s 79us/step - loss: 0.6053 - accuracy: 0.9052 - val_loss: 0.5255 - val_accuracy: 0.9333\n",
            "Epoch 13/100\n",
            "116/116 [==============================] - 0s 78us/step - loss: 0.5615 - accuracy: 0.8621 - val_loss: 0.4939 - val_accuracy: 0.9333\n",
            "Epoch 14/100\n",
            "116/116 [==============================] - 0s 70us/step - loss: 0.5286 - accuracy: 0.9052 - val_loss: 0.4796 - val_accuracy: 0.9333\n",
            "Epoch 15/100\n",
            "116/116 [==============================] - 0s 78us/step - loss: 0.5028 - accuracy: 0.8793 - val_loss: 0.4588 - val_accuracy: 0.9333\n",
            "Epoch 16/100\n",
            "116/116 [==============================] - 0s 86us/step - loss: 0.4748 - accuracy: 0.8707 - val_loss: 0.4419 - val_accuracy: 0.9333\n",
            "Epoch 17/100\n",
            "116/116 [==============================] - 0s 83us/step - loss: 0.4526 - accuracy: 0.9052 - val_loss: 0.4224 - val_accuracy: 0.9333\n",
            "Epoch 18/100\n",
            "116/116 [==============================] - 0s 131us/step - loss: 0.4356 - accuracy: 0.8966 - val_loss: 0.4061 - val_accuracy: 0.9333\n",
            "Epoch 19/100\n",
            "116/116 [==============================] - 0s 70us/step - loss: 0.4238 - accuracy: 0.8879 - val_loss: 0.3932 - val_accuracy: 0.9333\n",
            "Epoch 20/100\n",
            "116/116 [==============================] - 0s 76us/step - loss: 0.4156 - accuracy: 0.9138 - val_loss: 0.3766 - val_accuracy: 0.9333\n",
            "Epoch 21/100\n",
            "116/116 [==============================] - 0s 82us/step - loss: 0.4077 - accuracy: 0.8793 - val_loss: 0.4424 - val_accuracy: 0.8333\n",
            "Epoch 22/100\n",
            "116/116 [==============================] - 0s 106us/step - loss: 0.4572 - accuracy: 0.8276 - val_loss: 0.3667 - val_accuracy: 0.9333\n",
            "Epoch 23/100\n",
            "116/116 [==============================] - 0s 75us/step - loss: 0.3856 - accuracy: 0.8879 - val_loss: 0.3642 - val_accuracy: 0.9333\n",
            "Epoch 24/100\n",
            "116/116 [==============================] - 0s 90us/step - loss: 0.3727 - accuracy: 0.8707 - val_loss: 0.4114 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "116/116 [==============================] - 0s 77us/step - loss: 0.4096 - accuracy: 0.8534 - val_loss: 0.3738 - val_accuracy: 0.9333\n",
            "Epoch 26/100\n",
            "116/116 [==============================] - 0s 92us/step - loss: 0.3508 - accuracy: 0.9224 - val_loss: 0.5432 - val_accuracy: 0.7667\n",
            "Epoch 27/100\n",
            "116/116 [==============================] - 0s 97us/step - loss: 0.5174 - accuracy: 0.8017 - val_loss: 0.5293 - val_accuracy: 0.8000\n",
            "Epoch 28/100\n",
            "116/116 [==============================] - 0s 98us/step - loss: 0.4742 - accuracy: 0.8448 - val_loss: 0.6029 - val_accuracy: 0.7667\n",
            "Epoch 29/100\n",
            "116/116 [==============================] - 0s 94us/step - loss: 0.6346 - accuracy: 0.7759 - val_loss: 0.3715 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "116/116 [==============================] - 0s 84us/step - loss: 0.4286 - accuracy: 0.8621 - val_loss: 0.4816 - val_accuracy: 0.9667\n",
            "Epoch 31/100\n",
            "116/116 [==============================] - 0s 100us/step - loss: 0.4691 - accuracy: 0.8448 - val_loss: 0.6374 - val_accuracy: 0.8000\n",
            "Epoch 32/100\n",
            "116/116 [==============================] - 0s 79us/step - loss: 0.6413 - accuracy: 0.7845 - val_loss: 0.4650 - val_accuracy: 0.8667\n",
            "Epoch 33/100\n",
            "116/116 [==============================] - 0s 96us/step - loss: 0.4616 - accuracy: 0.8793 - val_loss: 0.5843 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 34/100\n",
            "116/116 [==============================] - 0s 91us/step - loss: 0.6363 - accuracy: 0.7414 - val_loss: 0.5151 - val_accuracy: 0.9000\n",
            "Epoch 35/100\n",
            "116/116 [==============================] - 0s 81us/step - loss: 0.5536 - accuracy: 0.8017 - val_loss: 0.4138 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "116/116 [==============================] - 0s 104us/step - loss: 0.4415 - accuracy: 0.9224 - val_loss: 0.3850 - val_accuracy: 0.9333\n",
            "Epoch 37/100\n",
            "116/116 [==============================] - 0s 87us/step - loss: 0.4040 - accuracy: 0.9138 - val_loss: 0.4125 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "116/116 [==============================] - 0s 108us/step - loss: 0.4253 - accuracy: 0.8793 - val_loss: 0.4443 - val_accuracy: 0.8667\n",
            "Epoch 39/100\n",
            "116/116 [==============================] - 0s 83us/step - loss: 0.4483 - accuracy: 0.8621 - val_loss: 0.4435 - val_accuracy: 0.8667\n",
            "Epoch 40/100\n",
            "116/116 [==============================] - 0s 89us/step - loss: 0.4431 - accuracy: 0.8621 - val_loss: 0.4137 - val_accuracy: 0.8667\n",
            "Epoch 41/100\n",
            "116/116 [==============================] - 0s 84us/step - loss: 0.4153 - accuracy: 0.8793 - val_loss: 0.3770 - val_accuracy: 0.9333\n",
            "Epoch 42/100\n",
            "116/116 [==============================] - 0s 77us/step - loss: 0.3891 - accuracy: 0.9052 - val_loss: 0.3541 - val_accuracy: 0.9333\n",
            "Epoch 43/100\n",
            "116/116 [==============================] - 0s 107us/step - loss: 0.3757 - accuracy: 0.9224 - val_loss: 0.3512 - val_accuracy: 0.9333\n",
            "Epoch 44/100\n",
            "116/116 [==============================] - 0s 109us/step - loss: 0.3868 - accuracy: 0.9138 - val_loss: 0.3527 - val_accuracy: 0.9333\n",
            "Epoch 45/100\n",
            "116/116 [==============================] - 0s 75us/step - loss: 0.3894 - accuracy: 0.9310 - val_loss: 0.3448 - val_accuracy: 0.9333\n",
            "Epoch 46/100\n",
            "116/116 [==============================] - 0s 98us/step - loss: 0.3699 - accuracy: 0.9138 - val_loss: 0.3483 - val_accuracy: 0.9333\n",
            "Epoch 47/100\n",
            "116/116 [==============================] - 0s 116us/step - loss: 0.3620 - accuracy: 0.9138 - val_loss: 0.3670 - val_accuracy: 0.9333\n",
            "Epoch 48/100\n",
            "116/116 [==============================] - 0s 109us/step - loss: 0.3698 - accuracy: 0.8966 - val_loss: 0.3772 - val_accuracy: 0.9000\n",
            "Epoch 49/100\n",
            "116/116 [==============================] - 0s 104us/step - loss: 0.3761 - accuracy: 0.8879 - val_loss: 0.3721 - val_accuracy: 0.9000\n",
            "Epoch 50/100\n",
            "116/116 [==============================] - 0s 102us/step - loss: 0.3685 - accuracy: 0.8879 - val_loss: 0.3520 - val_accuracy: 0.9333\n",
            "Epoch 51/100\n",
            "116/116 [==============================] - 0s 74us/step - loss: 0.3593 - accuracy: 0.9224 - val_loss: 0.3393 - val_accuracy: 0.9333\n",
            "Epoch 52/100\n",
            "116/116 [==============================] - 0s 75us/step - loss: 0.3453 - accuracy: 0.9310 - val_loss: 0.3382 - val_accuracy: 0.9333\n",
            "Epoch 53/100\n",
            "116/116 [==============================] - 0s 95us/step - loss: 0.3436 - accuracy: 0.9310 - val_loss: 0.3396 - val_accuracy: 0.9333\n",
            "Epoch 54/100\n",
            "116/116 [==============================] - 0s 99us/step - loss: 0.3420 - accuracy: 0.9310 - val_loss: 0.3422 - val_accuracy: 0.9333\n",
            "Epoch 55/100\n",
            "116/116 [==============================] - 0s 91us/step - loss: 0.3397 - accuracy: 0.9310 - val_loss: 0.3460 - val_accuracy: 0.9333\n",
            "Epoch 56/100\n",
            "116/116 [==============================] - 0s 79us/step - loss: 0.3369 - accuracy: 0.9310 - val_loss: 0.3497 - val_accuracy: 0.9333\n",
            "Epoch 57/100\n",
            "116/116 [==============================] - 0s 96us/step - loss: 0.3349 - accuracy: 0.9310 - val_loss: 0.3520 - val_accuracy: 0.9333\n",
            "Epoch 58/100\n",
            "116/116 [==============================] - 0s 117us/step - loss: 0.3338 - accuracy: 0.9310 - val_loss: 0.3536 - val_accuracy: 0.9333\n",
            "Epoch 59/100\n",
            "116/116 [==============================] - 0s 75us/step - loss: 0.3329 - accuracy: 0.9310 - val_loss: 0.3549 - val_accuracy: 0.9333\n",
            "Epoch 60/100\n",
            "116/116 [==============================] - 0s 93us/step - loss: 0.3316 - accuracy: 0.9310 - val_loss: 0.3544 - val_accuracy: 0.9333\n",
            "Epoch 61/100\n",
            "116/116 [==============================] - 0s 86us/step - loss: 0.3296 - accuracy: 0.9310 - val_loss: 0.3547 - val_accuracy: 0.9333\n",
            "Epoch 62/100\n",
            "116/116 [==============================] - 0s 83us/step - loss: 0.3292 - accuracy: 0.9224 - val_loss: 0.3539 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 63/100\n",
            "116/116 [==============================] - 0s 80us/step - loss: 0.3289 - accuracy: 0.9224 - val_loss: 0.3535 - val_accuracy: 0.9333\n",
            "Epoch 64/100\n",
            "116/116 [==============================] - 0s 76us/step - loss: 0.3286 - accuracy: 0.9224 - val_loss: 0.3527 - val_accuracy: 0.9333\n",
            "Epoch 65/100\n",
            "116/116 [==============================] - 0s 76us/step - loss: 0.3279 - accuracy: 0.9224 - val_loss: 0.3517 - val_accuracy: 0.9333\n",
            "Epoch 66/100\n",
            "116/116 [==============================] - 0s 72us/step - loss: 0.3276 - accuracy: 0.9224 - val_loss: 0.3507 - val_accuracy: 0.9333\n",
            "Epoch 67/100\n",
            "116/116 [==============================] - 0s 84us/step - loss: 0.3269 - accuracy: 0.9310 - val_loss: 0.3498 - val_accuracy: 0.9333\n",
            "Epoch 68/100\n",
            "116/116 [==============================] - 0s 91us/step - loss: 0.3268 - accuracy: 0.9310 - val_loss: 0.3491 - val_accuracy: 0.9333\n",
            "Epoch 69/100\n",
            "116/116 [==============================] - 0s 73us/step - loss: 0.3265 - accuracy: 0.9310 - val_loss: 0.3487 - val_accuracy: 0.9333\n",
            "Epoch 70/100\n",
            "116/116 [==============================] - 0s 74us/step - loss: 0.3263 - accuracy: 0.9310 - val_loss: 0.3484 - val_accuracy: 0.9333\n",
            "Epoch 71/100\n",
            "116/116 [==============================] - 0s 87us/step - loss: 0.3262 - accuracy: 0.9310 - val_loss: 0.3480 - val_accuracy: 0.9333\n",
            "Epoch 72/100\n",
            "116/116 [==============================] - 0s 81us/step - loss: 0.3260 - accuracy: 0.9310 - val_loss: 0.3476 - val_accuracy: 0.9333\n",
            "Epoch 73/100\n",
            "116/116 [==============================] - 0s 90us/step - loss: 0.3258 - accuracy: 0.9310 - val_loss: 0.3472 - val_accuracy: 0.9333\n",
            "Epoch 74/100\n",
            "116/116 [==============================] - 0s 78us/step - loss: 0.3256 - accuracy: 0.9310 - val_loss: 0.3470 - val_accuracy: 0.9333\n",
            "Epoch 75/100\n",
            "116/116 [==============================] - 0s 90us/step - loss: 0.3255 - accuracy: 0.9310 - val_loss: 0.3468 - val_accuracy: 0.9333\n",
            "Epoch 76/100\n",
            "116/116 [==============================] - 0s 97us/step - loss: 0.3256 - accuracy: 0.9310 - val_loss: 0.3466 - val_accuracy: 0.9333\n",
            "Epoch 77/100\n",
            "116/116 [==============================] - 0s 92us/step - loss: 0.3253 - accuracy: 0.9310 - val_loss: 0.3462 - val_accuracy: 0.9333\n",
            "Epoch 78/100\n",
            "116/116 [==============================] - 0s 100us/step - loss: 0.3251 - accuracy: 0.9310 - val_loss: 0.3459 - val_accuracy: 0.9333\n",
            "Epoch 79/100\n",
            "116/116 [==============================] - 0s 77us/step - loss: 0.3249 - accuracy: 0.9310 - val_loss: 0.3456 - val_accuracy: 0.9333\n",
            "Epoch 80/100\n",
            "116/116 [==============================] - 0s 90us/step - loss: 0.3249 - accuracy: 0.9310 - val_loss: 0.3453 - val_accuracy: 0.9333\n",
            "Epoch 81/100\n",
            "116/116 [==============================] - 0s 89us/step - loss: 0.3247 - accuracy: 0.9310 - val_loss: 0.3451 - val_accuracy: 0.9333\n",
            "Epoch 82/100\n",
            "116/116 [==============================] - 0s 77us/step - loss: 0.3245 - accuracy: 0.9310 - val_loss: 0.3449 - val_accuracy: 0.9333\n",
            "Epoch 83/100\n",
            "116/116 [==============================] - 0s 93us/step - loss: 0.3243 - accuracy: 0.9310 - val_loss: 0.3444 - val_accuracy: 0.9333\n",
            "Epoch 84/100\n",
            "116/116 [==============================] - 0s 76us/step - loss: 0.3241 - accuracy: 0.9310 - val_loss: 0.3440 - val_accuracy: 0.9333\n",
            "Epoch 85/100\n",
            "116/116 [==============================] - 0s 77us/step - loss: 0.3243 - accuracy: 0.9310 - val_loss: 0.3437 - val_accuracy: 0.9333\n",
            "Epoch 86/100\n",
            "116/116 [==============================] - 0s 82us/step - loss: 0.3243 - accuracy: 0.9310 - val_loss: 0.3434 - val_accuracy: 0.9333\n",
            "Epoch 87/100\n",
            "116/116 [==============================] - 0s 87us/step - loss: 0.3243 - accuracy: 0.9310 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
            "Epoch 88/100\n",
            "116/116 [==============================] - 0s 85us/step - loss: 0.3242 - accuracy: 0.9310 - val_loss: 0.3431 - val_accuracy: 0.9333\n",
            "Epoch 89/100\n",
            "116/116 [==============================] - 0s 88us/step - loss: 0.3240 - accuracy: 0.9310 - val_loss: 0.3430 - val_accuracy: 0.9333\n",
            "Epoch 90/100\n",
            "116/116 [==============================] - 0s 105us/step - loss: 0.3238 - accuracy: 0.9310 - val_loss: 0.3430 - val_accuracy: 0.9333\n",
            "Epoch 91/100\n",
            "116/116 [==============================] - 0s 73us/step - loss: 0.3236 - accuracy: 0.9310 - val_loss: 0.3431 - val_accuracy: 0.9333\n",
            "Epoch 92/100\n",
            "116/116 [==============================] - 0s 79us/step - loss: 0.3235 - accuracy: 0.9310 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
            "Epoch 93/100\n",
            "116/116 [==============================] - 0s 87us/step - loss: 0.3233 - accuracy: 0.9310 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
            "Epoch 94/100\n",
            "116/116 [==============================] - 0s 90us/step - loss: 0.3231 - accuracy: 0.9310 - val_loss: 0.3431 - val_accuracy: 0.9333\n",
            "Epoch 95/100\n",
            "116/116 [==============================] - 0s 79us/step - loss: 0.3231 - accuracy: 0.9310 - val_loss: 0.3430 - val_accuracy: 0.9333\n",
            "Epoch 96/100\n",
            "116/116 [==============================] - 0s 147us/step - loss: 0.3229 - accuracy: 0.9310 - val_loss: 0.3430 - val_accuracy: 0.9333\n",
            "Epoch 97/100\n",
            "116/116 [==============================] - 0s 115us/step - loss: 0.3229 - accuracy: 0.9310 - val_loss: 0.3430 - val_accuracy: 0.9333\n",
            "Epoch 98/100\n",
            "116/116 [==============================] - 0s 74us/step - loss: 0.3229 - accuracy: 0.9310 - val_loss: 0.3431 - val_accuracy: 0.9333\n",
            "Epoch 99/100\n",
            "116/116 [==============================] - 0s 84us/step - loss: 0.3231 - accuracy: 0.9310 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
            "Epoch 100/100\n",
            "116/116 [==============================] - 0s 97us/step - loss: 0.3228 - accuracy: 0.9310 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
            "30/30 [==============================] - 0s 60us/step\n",
            "Train on 117 samples, validate on 29 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 0s 990us/step - loss: 2.6747 - accuracy: 0.6838 - val_loss: 1.9547 - val_accuracy: 0.7241\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 65us/step - loss: 1.9131 - accuracy: 0.7009 - val_loss: 1.5970 - val_accuracy: 0.7586\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 1.5177 - accuracy: 0.7778 - val_loss: 1.1710 - val_accuracy: 0.7931\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 1.0873 - accuracy: 0.8462 - val_loss: 1.0229 - val_accuracy: 0.7241\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.9312 - accuracy: 0.7265 - val_loss: 0.9414 - val_accuracy: 0.7241\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.8520 - accuracy: 0.7607 - val_loss: 0.8674 - val_accuracy: 0.7931\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.7378 - accuracy: 0.8718 - val_loss: 0.9044 - val_accuracy: 0.7586\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.7457 - accuracy: 0.8205 - val_loss: 0.8387 - val_accuracy: 0.7586\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 93us/step - loss: 0.6824 - accuracy: 0.8974 - val_loss: 0.8136 - val_accuracy: 0.7586\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.6567 - accuracy: 0.8974 - val_loss: 0.7776 - val_accuracy: 0.7586\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.6071 - accuracy: 0.9145 - val_loss: 0.7373 - val_accuracy: 0.7586\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.5504 - accuracy: 0.9145 - val_loss: 0.7420 - val_accuracy: 0.7931\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 108us/step - loss: 0.5342 - accuracy: 0.8718 - val_loss: 0.6797 - val_accuracy: 0.7931\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.4773 - accuracy: 0.9145 - val_loss: 0.6330 - val_accuracy: 0.7931\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.4548 - accuracy: 0.9231 - val_loss: 0.6176 - val_accuracy: 0.7931\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.4222 - accuracy: 0.9060 - val_loss: 0.6649 - val_accuracy: 0.7931\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.4401 - accuracy: 0.8632 - val_loss: 0.5812 - val_accuracy: 0.7931\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.4008 - accuracy: 0.9231 - val_loss: 0.5694 - val_accuracy: 0.7931\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.3780 - accuracy: 0.9231 - val_loss: 0.5843 - val_accuracy: 0.7931\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 101us/step - loss: 0.3936 - accuracy: 0.8889 - val_loss: 0.6034 - val_accuracy: 0.7931\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 114us/step - loss: 0.3835 - accuracy: 0.8718 - val_loss: 0.5650 - val_accuracy: 0.7931\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.3608 - accuracy: 0.9316 - val_loss: 0.5560 - val_accuracy: 0.8276\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3527 - accuracy: 0.9573 - val_loss: 0.5930 - val_accuracy: 0.7586\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 71us/step - loss: 0.3540 - accuracy: 0.8974 - val_loss: 0.5462 - val_accuracy: 0.8276\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3396 - accuracy: 0.9402 - val_loss: 0.6281 - val_accuracy: 0.7586\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.3713 - accuracy: 0.8803 - val_loss: 0.5352 - val_accuracy: 0.7931\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.3175 - accuracy: 0.9316 - val_loss: 0.5278 - val_accuracy: 0.8621\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.3166 - accuracy: 0.9402 - val_loss: 0.5725 - val_accuracy: 0.7586\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.3183 - accuracy: 0.9316 - val_loss: 0.5465 - val_accuracy: 0.8621\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3115 - accuracy: 0.9487 - val_loss: 0.6759 - val_accuracy: 0.7586\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3632 - accuracy: 0.8632 - val_loss: 0.6323 - val_accuracy: 0.7586\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.3573 - accuracy: 0.9231 - val_loss: 0.7402 - val_accuracy: 0.7586\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.4020 - accuracy: 0.8291 - val_loss: 0.7669 - val_accuracy: 0.5862\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.4730 - accuracy: 0.8034 - val_loss: 0.5639 - val_accuracy: 0.7586\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.3003 - accuracy: 0.9231 - val_loss: 0.5485 - val_accuracy: 0.8276\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.2964 - accuracy: 0.9402 - val_loss: 0.5905 - val_accuracy: 0.8621\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.3291 - accuracy: 0.9402 - val_loss: 0.6662 - val_accuracy: 0.7931\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 68us/step - loss: 0.3510 - accuracy: 0.8632 - val_loss: 0.6434 - val_accuracy: 0.7931\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.3312 - accuracy: 0.8803 - val_loss: 0.5783 - val_accuracy: 0.7586\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.2911 - accuracy: 0.9316 - val_loss: 0.5420 - val_accuracy: 0.7931\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 101us/step - loss: 0.2856 - accuracy: 0.9573 - val_loss: 0.5566 - val_accuracy: 0.8621\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.3151 - accuracy: 0.9573 - val_loss: 0.5642 - val_accuracy: 0.8276\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3201 - accuracy: 0.9487 - val_loss: 0.5389 - val_accuracy: 0.8621\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 125us/step - loss: 0.2930 - accuracy: 0.9573 - val_loss: 0.5375 - val_accuracy: 0.7586\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.2780 - accuracy: 0.9402 - val_loss: 0.5688 - val_accuracy: 0.7931\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 102us/step - loss: 0.2925 - accuracy: 0.9145 - val_loss: 0.5876 - val_accuracy: 0.7931\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 102us/step - loss: 0.2981 - accuracy: 0.9060 - val_loss: 0.5633 - val_accuracy: 0.7931\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 117us/step - loss: 0.2868 - accuracy: 0.9145 - val_loss: 0.5588 - val_accuracy: 0.7931\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.2851 - accuracy: 0.9145 - val_loss: 0.5539 - val_accuracy: 0.7931\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 119us/step - loss: 0.2823 - accuracy: 0.9231 - val_loss: 0.5490 - val_accuracy: 0.7931\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.2804 - accuracy: 0.9316 - val_loss: 0.5434 - val_accuracy: 0.7586\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.2785 - accuracy: 0.9316 - val_loss: 0.5384 - val_accuracy: 0.7586\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.2764 - accuracy: 0.9316 - val_loss: 0.5341 - val_accuracy: 0.7586\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.2757 - accuracy: 0.9487 - val_loss: 0.5305 - val_accuracy: 0.7586\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 0.2753 - accuracy: 0.9487 - val_loss: 0.5281 - val_accuracy: 0.7931\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2748 - accuracy: 0.9487 - val_loss: 0.5266 - val_accuracy: 0.7931\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.2747 - accuracy: 0.9487 - val_loss: 0.5256 - val_accuracy: 0.7931\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.2747 - accuracy: 0.9487 - val_loss: 0.5249 - val_accuracy: 0.7931\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.2747 - accuracy: 0.9487 - val_loss: 0.5246 - val_accuracy: 0.7931\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.2745 - accuracy: 0.9487 - val_loss: 0.5243 - val_accuracy: 0.7931\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.2742 - accuracy: 0.9487 - val_loss: 0.5243 - val_accuracy: 0.7931\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2739 - accuracy: 0.9487 - val_loss: 0.5245 - val_accuracy: 0.7931\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2736 - accuracy: 0.9487 - val_loss: 0.5248 - val_accuracy: 0.7931\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 98us/step - loss: 0.2733 - accuracy: 0.9487 - val_loss: 0.5250 - val_accuracy: 0.7931\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 93us/step - loss: 0.2729 - accuracy: 0.9487 - val_loss: 0.5254 - val_accuracy: 0.7931\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.2728 - accuracy: 0.9487 - val_loss: 0.5258 - val_accuracy: 0.7931\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.2724 - accuracy: 0.9487 - val_loss: 0.5259 - val_accuracy: 0.7586\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.2722 - accuracy: 0.9487 - val_loss: 0.5261 - val_accuracy: 0.7586\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2720 - accuracy: 0.9487 - val_loss: 0.5264 - val_accuracy: 0.7586\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.2718 - accuracy: 0.9487 - val_loss: 0.5263 - val_accuracy: 0.7586\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 124us/step - loss: 0.2716 - accuracy: 0.9487 - val_loss: 0.5259 - val_accuracy: 0.7586\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 0.2713 - accuracy: 0.9487 - val_loss: 0.5250 - val_accuracy: 0.7931\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.2712 - accuracy: 0.9487 - val_loss: 0.5244 - val_accuracy: 0.7931\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.2711 - accuracy: 0.9487 - val_loss: 0.5239 - val_accuracy: 0.7931\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 113us/step - loss: 0.2710 - accuracy: 0.9487 - val_loss: 0.5235 - val_accuracy: 0.7931\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.2709 - accuracy: 0.9487 - val_loss: 0.5231 - val_accuracy: 0.7931\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 108us/step - loss: 0.2709 - accuracy: 0.9487 - val_loss: 0.5228 - val_accuracy: 0.7931\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 109us/step - loss: 0.2708 - accuracy: 0.9487 - val_loss: 0.5227 - val_accuracy: 0.7931\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.2707 - accuracy: 0.9487 - val_loss: 0.5228 - val_accuracy: 0.7931\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.2705 - accuracy: 0.9487 - val_loss: 0.5229 - val_accuracy: 0.7931\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 103us/step - loss: 0.2704 - accuracy: 0.9487 - val_loss: 0.5232 - val_accuracy: 0.7931\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.2704 - accuracy: 0.9487 - val_loss: 0.5236 - val_accuracy: 0.7931\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 119us/step - loss: 0.2701 - accuracy: 0.9487 - val_loss: 0.5236 - val_accuracy: 0.7931\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 107us/step - loss: 0.2700 - accuracy: 0.9487 - val_loss: 0.5235 - val_accuracy: 0.7931\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 102us/step - loss: 0.2699 - accuracy: 0.9487 - val_loss: 0.5235 - val_accuracy: 0.7931\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.2698 - accuracy: 0.9487 - val_loss: 0.5238 - val_accuracy: 0.7931\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.2697 - accuracy: 0.9487 - val_loss: 0.5244 - val_accuracy: 0.7586\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.2697 - accuracy: 0.9487 - val_loss: 0.5248 - val_accuracy: 0.7586\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 111us/step - loss: 0.2696 - accuracy: 0.9487 - val_loss: 0.5246 - val_accuracy: 0.7586\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.2695 - accuracy: 0.9487 - val_loss: 0.5239 - val_accuracy: 0.7586\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2694 - accuracy: 0.9487 - val_loss: 0.5234 - val_accuracy: 0.7931\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.2693 - accuracy: 0.9487 - val_loss: 0.5232 - val_accuracy: 0.7931\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 100us/step - loss: 0.2692 - accuracy: 0.9487 - val_loss: 0.5230 - val_accuracy: 0.7931\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.2690 - accuracy: 0.9487 - val_loss: 0.5222 - val_accuracy: 0.7931\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2689 - accuracy: 0.9487 - val_loss: 0.5209 - val_accuracy: 0.7931\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.2689 - accuracy: 0.9487 - val_loss: 0.5199 - val_accuracy: 0.7931\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.2689 - accuracy: 0.9487 - val_loss: 0.5192 - val_accuracy: 0.7931\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.2691 - accuracy: 0.9487 - val_loss: 0.5186 - val_accuracy: 0.7931\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2693 - accuracy: 0.9487 - val_loss: 0.5184 - val_accuracy: 0.7931\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.2690 - accuracy: 0.9487 - val_loss: 0.5187 - val_accuracy: 0.7931\n",
            "29/29 [==============================] - 0s 46us/step\n",
            "Train on 117 samples, validate on 29 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 0s 911us/step - loss: 2.9527 - accuracy: 0.3077 - val_loss: 2.4143 - val_accuracy: 0.6897\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 2.2921 - accuracy: 0.7094 - val_loss: 1.7475 - val_accuracy: 0.3793\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 1.7347 - accuracy: 0.3504 - val_loss: 1.4831 - val_accuracy: 0.3793\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 1.4733 - accuracy: 0.3419 - val_loss: 1.3323 - val_accuracy: 0.8276\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 1.3247 - accuracy: 0.7949 - val_loss: 1.2265 - val_accuracy: 0.8276\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 1.2215 - accuracy: 0.7863 - val_loss: 1.1102 - val_accuracy: 0.7586\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 1.1059 - accuracy: 0.7265 - val_loss: 0.9937 - val_accuracy: 0.6897\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.9985 - accuracy: 0.7094 - val_loss: 0.9195 - val_accuracy: 0.7241\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.9362 - accuracy: 0.7094 - val_loss: 0.8476 - val_accuracy: 0.7586\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.8893 - accuracy: 0.7094 - val_loss: 0.7761 - val_accuracy: 0.8276\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.8133 - accuracy: 0.8120 - val_loss: 0.7532 - val_accuracy: 0.8966\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.7871 - accuracy: 0.8803 - val_loss: 0.6525 - val_accuracy: 0.8621\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.7064 - accuracy: 0.8205 - val_loss: 0.6028 - val_accuracy: 0.8621\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.6600 - accuracy: 0.8205 - val_loss: 0.5658 - val_accuracy: 0.8966\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.6204 - accuracy: 0.8718 - val_loss: 0.5329 - val_accuracy: 0.8621\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.5941 - accuracy: 0.8205 - val_loss: 0.5040 - val_accuracy: 0.8621\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 67us/step - loss: 0.5709 - accuracy: 0.8376 - val_loss: 0.4747 - val_accuracy: 0.8966\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.5379 - accuracy: 0.8889 - val_loss: 0.4623 - val_accuracy: 0.8966\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.5235 - accuracy: 0.8889 - val_loss: 0.4493 - val_accuracy: 0.8966\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 132us/step - loss: 0.5224 - accuracy: 0.8291 - val_loss: 0.4243 - val_accuracy: 0.8966\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.4908 - accuracy: 0.8803 - val_loss: 0.4440 - val_accuracy: 0.9310\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 67us/step - loss: 0.4944 - accuracy: 0.9145 - val_loss: 0.4159 - val_accuracy: 0.8966\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.4782 - accuracy: 0.8547 - val_loss: 0.4031 - val_accuracy: 0.8966\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.4560 - accuracy: 0.8889 - val_loss: 0.3911 - val_accuracy: 0.8966\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.4429 - accuracy: 0.9145 - val_loss: 0.3741 - val_accuracy: 0.8966\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.4300 - accuracy: 0.8889 - val_loss: 0.3934 - val_accuracy: 0.9310\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 114us/step - loss: 0.4414 - accuracy: 0.9231 - val_loss: 0.3676 - val_accuracy: 0.8621\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.4303 - accuracy: 0.8632 - val_loss: 0.3483 - val_accuracy: 0.9310\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.4029 - accuracy: 0.9231 - val_loss: 0.3437 - val_accuracy: 0.8966\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.4165 - accuracy: 0.8889 - val_loss: 0.3299 - val_accuracy: 0.8966\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 117us/step - loss: 0.3913 - accuracy: 0.9060 - val_loss: 0.3375 - val_accuracy: 0.8966\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.3939 - accuracy: 0.8889 - val_loss: 0.3328 - val_accuracy: 0.8966\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.4050 - accuracy: 0.9145 - val_loss: 0.4542 - val_accuracy: 0.8966\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.4776 - accuracy: 0.8632 - val_loss: 0.3469 - val_accuracy: 0.8966\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.3954 - accuracy: 0.8889 - val_loss: 0.3239 - val_accuracy: 0.8966\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 100us/step - loss: 0.3703 - accuracy: 0.9316 - val_loss: 0.3343 - val_accuracy: 0.8966\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.3892 - accuracy: 0.8803 - val_loss: 0.3334 - val_accuracy: 0.9655\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.3851 - accuracy: 0.9316 - val_loss: 0.3562 - val_accuracy: 0.8621\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.4120 - accuracy: 0.8462 - val_loss: 0.4034 - val_accuracy: 0.8966\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.4144 - accuracy: 0.9060 - val_loss: 0.5010 - val_accuracy: 0.8276\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.5951 - accuracy: 0.7607 - val_loss: 0.3301 - val_accuracy: 0.8966\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.3923 - accuracy: 0.9231 - val_loss: 0.3354 - val_accuracy: 0.9655\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3871 - accuracy: 0.9145 - val_loss: 0.3176 - val_accuracy: 0.8966\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.4062 - accuracy: 0.8803 - val_loss: 0.2985 - val_accuracy: 0.9310\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 108us/step - loss: 0.3771 - accuracy: 0.8974 - val_loss: 0.2928 - val_accuracy: 0.9655\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.3642 - accuracy: 0.9231 - val_loss: 0.3158 - val_accuracy: 0.9655\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3936 - accuracy: 0.9316 - val_loss: 0.2915 - val_accuracy: 0.8966\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.3658 - accuracy: 0.8889 - val_loss: 0.3650 - val_accuracy: 0.8966\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.4446 - accuracy: 0.8718 - val_loss: 0.2902 - val_accuracy: 0.9310\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.3592 - accuracy: 0.8974 - val_loss: 0.4523 - val_accuracy: 0.8621\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.4892 - accuracy: 0.8376 - val_loss: 0.4437 - val_accuracy: 0.8621\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 93us/step - loss: 0.5345 - accuracy: 0.7863 - val_loss: 0.2847 - val_accuracy: 0.9655\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 0.3465 - accuracy: 0.9231 - val_loss: 0.2988 - val_accuracy: 0.9655\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.3590 - accuracy: 0.9316 - val_loss: 0.2971 - val_accuracy: 0.8966\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.3382 - accuracy: 0.9231 - val_loss: 0.4047 - val_accuracy: 0.8966\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.4639 - accuracy: 0.8718 - val_loss: 0.2907 - val_accuracy: 0.9655\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.3337 - accuracy: 0.9316 - val_loss: 0.2868 - val_accuracy: 0.9655\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3525 - accuracy: 0.9145 - val_loss: 0.3340 - val_accuracy: 0.9310\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.3798 - accuracy: 0.9316 - val_loss: 0.3317 - val_accuracy: 0.8621\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.3999 - accuracy: 0.8462 - val_loss: 0.2795 - val_accuracy: 0.9655\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.3335 - accuracy: 0.9402 - val_loss: 0.3255 - val_accuracy: 0.9310\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 103us/step - loss: 0.3562 - accuracy: 0.9402 - val_loss: 0.3647 - val_accuracy: 0.8621\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.4123 - accuracy: 0.8291 - val_loss: 0.3435 - val_accuracy: 0.9310\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.3679 - accuracy: 0.9145 - val_loss: 0.3233 - val_accuracy: 0.8966\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.3807 - accuracy: 0.8632 - val_loss: 0.2923 - val_accuracy: 0.8966\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 104us/step - loss: 0.3476 - accuracy: 0.8974 - val_loss: 0.3313 - val_accuracy: 0.8966\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.3523 - accuracy: 0.9145 - val_loss: 0.3463 - val_accuracy: 0.8966\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.4130 - accuracy: 0.8547 - val_loss: 0.2823 - val_accuracy: 0.9655\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.3843 - accuracy: 0.9060 - val_loss: 0.3210 - val_accuracy: 0.8966\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.4438 - accuracy: 0.8291 - val_loss: 0.2691 - val_accuracy: 0.8966\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.3334 - accuracy: 0.9060 - val_loss: 0.4759 - val_accuracy: 0.7931\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.5091 - accuracy: 0.7949 - val_loss: 0.4803 - val_accuracy: 0.8621\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.5834 - accuracy: 0.7863 - val_loss: 0.2919 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 99us/step - loss: 0.3959 - accuracy: 0.9060 - val_loss: 0.3668 - val_accuracy: 0.8966\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 102us/step - loss: 0.3842 - accuracy: 0.8974 - val_loss: 0.4479 - val_accuracy: 0.8621\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.5426 - accuracy: 0.7949 - val_loss: 0.3174 - val_accuracy: 0.8966\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3850 - accuracy: 0.8889 - val_loss: 0.4585 - val_accuracy: 0.8621\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.4487 - accuracy: 0.8632 - val_loss: 0.4157 - val_accuracy: 0.8966\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.5141 - accuracy: 0.8462 - val_loss: 0.3525 - val_accuracy: 0.8966\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.4079 - accuracy: 0.8889 - val_loss: 0.4248 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.4812 - accuracy: 0.8718 - val_loss: 0.3770 - val_accuracy: 0.8966\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.4282 - accuracy: 0.8974 - val_loss: 0.3085 - val_accuracy: 0.9655\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.3610 - accuracy: 0.9231 - val_loss: 0.2909 - val_accuracy: 0.9310\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3595 - accuracy: 0.9060 - val_loss: 0.3120 - val_accuracy: 0.8966\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.3872 - accuracy: 0.8974 - val_loss: 0.3258 - val_accuracy: 0.8966\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 101us/step - loss: 0.4017 - accuracy: 0.8718 - val_loss: 0.3122 - val_accuracy: 0.8966\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3834 - accuracy: 0.8889 - val_loss: 0.2877 - val_accuracy: 0.8966\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 103us/step - loss: 0.3523 - accuracy: 0.9145 - val_loss: 0.2832 - val_accuracy: 0.9655\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 96us/step - loss: 0.3414 - accuracy: 0.9231 - val_loss: 0.2984 - val_accuracy: 0.9655\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 103us/step - loss: 0.3583 - accuracy: 0.9231 - val_loss: 0.3081 - val_accuracy: 0.9655\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.3639 - accuracy: 0.9231 - val_loss: 0.3065 - val_accuracy: 0.9655\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 120us/step - loss: 0.3620 - accuracy: 0.9231 - val_loss: 0.3032 - val_accuracy: 0.9655\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.3586 - accuracy: 0.9145 - val_loss: 0.2984 - val_accuracy: 0.9655\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 114us/step - loss: 0.3538 - accuracy: 0.9231 - val_loss: 0.2933 - val_accuracy: 0.9655\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.3499 - accuracy: 0.9231 - val_loss: 0.2885 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 112us/step - loss: 0.3453 - accuracy: 0.9231 - val_loss: 0.2848 - val_accuracy: 0.9655\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 110us/step - loss: 0.3414 - accuracy: 0.9316 - val_loss: 0.2820 - val_accuracy: 0.9655\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 101us/step - loss: 0.3392 - accuracy: 0.9316 - val_loss: 0.2798 - val_accuracy: 0.9655\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 99us/step - loss: 0.3379 - accuracy: 0.9231 - val_loss: 0.2784 - val_accuracy: 0.9655\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 0.3372 - accuracy: 0.9316 - val_loss: 0.2778 - val_accuracy: 0.9655\n",
            "29/29 [==============================] - 0s 71us/step\n",
            "Train on 117 samples, validate on 29 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 0s 920us/step - loss: 2.4898 - accuracy: 0.7009 - val_loss: 2.0725 - val_accuracy: 0.6897\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 1.8677 - accuracy: 0.7009 - val_loss: 1.3157 - val_accuracy: 0.7241\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 1.2534 - accuracy: 0.7949 - val_loss: 0.9988 - val_accuracy: 0.7586\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.9327 - accuracy: 0.8291 - val_loss: 0.8342 - val_accuracy: 0.6897\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.7507 - accuracy: 0.7179 - val_loss: 0.7902 - val_accuracy: 0.6897\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 69us/step - loss: 0.6973 - accuracy: 0.7265 - val_loss: 0.7485 - val_accuracy: 0.7241\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 68us/step - loss: 0.6662 - accuracy: 0.7778 - val_loss: 0.7428 - val_accuracy: 0.7241\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 0.6718 - accuracy: 0.8718 - val_loss: 0.7375 - val_accuracy: 0.7931\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.6522 - accuracy: 0.8718 - val_loss: 0.7360 - val_accuracy: 0.7586\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.6230 - accuracy: 0.8462 - val_loss: 0.6971 - val_accuracy: 0.7241\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.5726 - accuracy: 0.8632 - val_loss: 0.6622 - val_accuracy: 0.7931\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.5304 - accuracy: 0.8718 - val_loss: 0.6211 - val_accuracy: 0.8621\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 0.4890 - accuracy: 0.9060 - val_loss: 0.6384 - val_accuracy: 0.7241\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.4646 - accuracy: 0.8889 - val_loss: 0.5943 - val_accuracy: 0.8621\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 0.4225 - accuracy: 0.9231 - val_loss: 0.6023 - val_accuracy: 0.8621\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 67us/step - loss: 0.4296 - accuracy: 0.9316 - val_loss: 0.6475 - val_accuracy: 0.7931\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.4308 - accuracy: 0.9231 - val_loss: 0.6668 - val_accuracy: 0.7241\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.4035 - accuracy: 0.8803 - val_loss: 0.6514 - val_accuracy: 0.7586\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.3735 - accuracy: 0.9145 - val_loss: 0.6673 - val_accuracy: 0.7586\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.3948 - accuracy: 0.9402 - val_loss: 0.7495 - val_accuracy: 0.6897\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.4329 - accuracy: 0.8205 - val_loss: 0.6131 - val_accuracy: 0.8276\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3707 - accuracy: 0.9487 - val_loss: 0.6119 - val_accuracy: 0.7241\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 114us/step - loss: 0.3493 - accuracy: 0.9060 - val_loss: 0.6331 - val_accuracy: 0.7586\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3727 - accuracy: 0.8974 - val_loss: 0.5561 - val_accuracy: 0.7931\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.3348 - accuracy: 0.9316 - val_loss: 0.5587 - val_accuracy: 0.8276\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.3248 - accuracy: 0.9231 - val_loss: 0.5468 - val_accuracy: 0.7931\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.3187 - accuracy: 0.9487 - val_loss: 0.6134 - val_accuracy: 0.7586\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.3507 - accuracy: 0.8974 - val_loss: 0.5867 - val_accuracy: 0.7241\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.3150 - accuracy: 0.9060 - val_loss: 0.6586 - val_accuracy: 0.7586\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.4048 - accuracy: 0.8974 - val_loss: 0.6214 - val_accuracy: 0.7586\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.3161 - accuracy: 0.9060 - val_loss: 0.6360 - val_accuracy: 0.7931\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3576 - accuracy: 0.9145 - val_loss: 0.8278 - val_accuracy: 0.6897\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.4760 - accuracy: 0.8291 - val_loss: 0.5835 - val_accuracy: 0.7931\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.3818 - accuracy: 0.8974 - val_loss: 0.6494 - val_accuracy: 0.7586\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 104us/step - loss: 0.3887 - accuracy: 0.8632 - val_loss: 0.7489 - val_accuracy: 0.6897\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.4258 - accuracy: 0.8718 - val_loss: 0.7056 - val_accuracy: 0.7241\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.4858 - accuracy: 0.8291 - val_loss: 0.6282 - val_accuracy: 0.8276\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3886 - accuracy: 0.9231 - val_loss: 0.5781 - val_accuracy: 0.8276\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.3131 - accuracy: 0.9231 - val_loss: 0.6300 - val_accuracy: 0.7241\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.3421 - accuracy: 0.9060 - val_loss: 0.6937 - val_accuracy: 0.7586\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3845 - accuracy: 0.8889 - val_loss: 0.7016 - val_accuracy: 0.7586\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 0.3853 - accuracy: 0.8889 - val_loss: 0.6508 - val_accuracy: 0.7586\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 69us/step - loss: 0.3432 - accuracy: 0.9060 - val_loss: 0.5886 - val_accuracy: 0.8276\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 132us/step - loss: 0.3044 - accuracy: 0.9402 - val_loss: 0.5766 - val_accuracy: 0.8276\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.3218 - accuracy: 0.9402 - val_loss: 0.6039 - val_accuracy: 0.7931\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.3545 - accuracy: 0.9402 - val_loss: 0.5949 - val_accuracy: 0.7931\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3393 - accuracy: 0.9316 - val_loss: 0.5908 - val_accuracy: 0.7931\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.3329 - accuracy: 0.9487 - val_loss: 0.5853 - val_accuracy: 0.8276\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.3261 - accuracy: 0.9487 - val_loss: 0.5801 - val_accuracy: 0.8276\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.3171 - accuracy: 0.9487 - val_loss: 0.5762 - val_accuracy: 0.8276\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.3118 - accuracy: 0.9487 - val_loss: 0.5740 - val_accuracy: 0.8276\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.3060 - accuracy: 0.9487 - val_loss: 0.5735 - val_accuracy: 0.8276\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3023 - accuracy: 0.9402 - val_loss: 0.5744 - val_accuracy: 0.8276\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.3005 - accuracy: 0.9316 - val_loss: 0.5763 - val_accuracy: 0.8276\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.3003 - accuracy: 0.9316 - val_loss: 0.5785 - val_accuracy: 0.8621\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.2996 - accuracy: 0.9316 - val_loss: 0.5800 - val_accuracy: 0.8621\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 100us/step - loss: 0.3003 - accuracy: 0.9402 - val_loss: 0.5809 - val_accuracy: 0.8621\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3000 - accuracy: 0.9402 - val_loss: 0.5812 - val_accuracy: 0.8276\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.2999 - accuracy: 0.9402 - val_loss: 0.5812 - val_accuracy: 0.8276\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 98us/step - loss: 0.2997 - accuracy: 0.9316 - val_loss: 0.5810 - val_accuracy: 0.8621\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2994 - accuracy: 0.9402 - val_loss: 0.5805 - val_accuracy: 0.8621\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2989 - accuracy: 0.9402 - val_loss: 0.5798 - val_accuracy: 0.8621\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 109us/step - loss: 0.2986 - accuracy: 0.9402 - val_loss: 0.5789 - val_accuracy: 0.8621\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.2979 - accuracy: 0.9316 - val_loss: 0.5782 - val_accuracy: 0.8621\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.2976 - accuracy: 0.9316 - val_loss: 0.5774 - val_accuracy: 0.8621\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 74us/step - loss: 0.2973 - accuracy: 0.9316 - val_loss: 0.5767 - val_accuracy: 0.8621\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2974 - accuracy: 0.9316 - val_loss: 0.5763 - val_accuracy: 0.8276\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 105us/step - loss: 0.2972 - accuracy: 0.9316 - val_loss: 0.5761 - val_accuracy: 0.8276\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.2969 - accuracy: 0.9316 - val_loss: 0.5760 - val_accuracy: 0.8276\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.2967 - accuracy: 0.9316 - val_loss: 0.5761 - val_accuracy: 0.8276\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.2965 - accuracy: 0.9316 - val_loss: 0.5761 - val_accuracy: 0.8276\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2962 - accuracy: 0.9316 - val_loss: 0.5761 - val_accuracy: 0.8621\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.2961 - accuracy: 0.9316 - val_loss: 0.5761 - val_accuracy: 0.8621\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.2957 - accuracy: 0.9316 - val_loss: 0.5759 - val_accuracy: 0.8621\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.2956 - accuracy: 0.9316 - val_loss: 0.5756 - val_accuracy: 0.8621\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.2953 - accuracy: 0.9316 - val_loss: 0.5754 - val_accuracy: 0.8621\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2951 - accuracy: 0.9316 - val_loss: 0.5751 - val_accuracy: 0.8621\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.2949 - accuracy: 0.9316 - val_loss: 0.5748 - val_accuracy: 0.8621\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.2948 - accuracy: 0.9316 - val_loss: 0.5746 - val_accuracy: 0.8276\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.2946 - accuracy: 0.9316 - val_loss: 0.5743 - val_accuracy: 0.8276\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.2945 - accuracy: 0.9316 - val_loss: 0.5740 - val_accuracy: 0.8276\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2945 - accuracy: 0.9316 - val_loss: 0.5737 - val_accuracy: 0.8276\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.2943 - accuracy: 0.9316 - val_loss: 0.5735 - val_accuracy: 0.8276\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.2942 - accuracy: 0.9316 - val_loss: 0.5733 - val_accuracy: 0.8276\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2940 - accuracy: 0.9316 - val_loss: 0.5733 - val_accuracy: 0.8276\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.2937 - accuracy: 0.9316 - val_loss: 0.5733 - val_accuracy: 0.8276\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.2935 - accuracy: 0.9316 - val_loss: 0.5733 - val_accuracy: 0.8621\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.2932 - accuracy: 0.9316 - val_loss: 0.5732 - val_accuracy: 0.8621\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.2929 - accuracy: 0.9316 - val_loss: 0.5731 - val_accuracy: 0.8621\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.2927 - accuracy: 0.9316 - val_loss: 0.5732 - val_accuracy: 0.8621\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 0.2927 - accuracy: 0.9316 - val_loss: 0.5734 - val_accuracy: 0.8621\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 122us/step - loss: 0.2924 - accuracy: 0.9402 - val_loss: 0.5735 - val_accuracy: 0.8621\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 117us/step - loss: 0.2922 - accuracy: 0.9402 - val_loss: 0.5737 - val_accuracy: 0.8621\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 93us/step - loss: 0.2921 - accuracy: 0.9402 - val_loss: 0.5738 - val_accuracy: 0.8621\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 103us/step - loss: 0.2920 - accuracy: 0.9402 - val_loss: 0.5739 - val_accuracy: 0.8621\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.2919 - accuracy: 0.9402 - val_loss: 0.5739 - val_accuracy: 0.8621\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.2918 - accuracy: 0.9402 - val_loss: 0.5739 - val_accuracy: 0.8621\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 105us/step - loss: 0.2917 - accuracy: 0.9402 - val_loss: 0.5738 - val_accuracy: 0.8621\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 93us/step - loss: 0.2915 - accuracy: 0.9402 - val_loss: 0.5735 - val_accuracy: 0.8621\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 104us/step - loss: 0.2912 - accuracy: 0.9402 - val_loss: 0.5731 - val_accuracy: 0.8621\n",
            "29/29 [==============================] - 0s 46us/step\n",
            "Train on 117 samples, validate on 29 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 0s 899us/step - loss: 2.7375 - accuracy: 0.5897 - val_loss: 2.1215 - val_accuracy: 0.6897\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 2.0757 - accuracy: 0.7094 - val_loss: 1.5819 - val_accuracy: 0.8966\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 1.5482 - accuracy: 0.8718 - val_loss: 1.2737 - val_accuracy: 0.9310\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 1.2548 - accuracy: 0.8291 - val_loss: 1.0903 - val_accuracy: 0.7586\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 67us/step - loss: 1.0702 - accuracy: 0.8034 - val_loss: 0.9831 - val_accuracy: 0.9310\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 69us/step - loss: 0.9621 - accuracy: 0.8462 - val_loss: 0.9121 - val_accuracy: 0.8621\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.8946 - accuracy: 0.8120 - val_loss: 0.8656 - val_accuracy: 0.8966\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.8336 - accuracy: 0.8974 - val_loss: 0.8296 - val_accuracy: 0.9310\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.7865 - accuracy: 0.8632 - val_loss: 0.7563 - val_accuracy: 0.8966\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.7223 - accuracy: 0.8632 - val_loss: 0.7295 - val_accuracy: 0.8621\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.6636 - accuracy: 0.8889 - val_loss: 0.6700 - val_accuracy: 0.8621\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 71us/step - loss: 0.6038 - accuracy: 0.8803 - val_loss: 0.6131 - val_accuracy: 0.8966\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.5756 - accuracy: 0.8718 - val_loss: 0.5733 - val_accuracy: 0.8966\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.5220 - accuracy: 0.8889 - val_loss: 0.5548 - val_accuracy: 0.8621\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 70us/step - loss: 0.4844 - accuracy: 0.9060 - val_loss: 0.6060 - val_accuracy: 0.8621\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.5160 - accuracy: 0.9060 - val_loss: 0.5016 - val_accuracy: 0.8621\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.4360 - accuracy: 0.9316 - val_loss: 0.4927 - val_accuracy: 0.8621\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.4119 - accuracy: 0.9316 - val_loss: 0.4882 - val_accuracy: 0.8966\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.4056 - accuracy: 0.8803 - val_loss: 0.5705 - val_accuracy: 0.8621\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.4164 - accuracy: 0.9231 - val_loss: 0.4721 - val_accuracy: 0.8621\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.3733 - accuracy: 0.9231 - val_loss: 0.4399 - val_accuracy: 0.8621\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 81us/step - loss: 0.3637 - accuracy: 0.9060 - val_loss: 0.4510 - val_accuracy: 0.8966\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 89us/step - loss: 0.3546 - accuracy: 0.9402 - val_loss: 0.3997 - val_accuracy: 0.8621\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.3431 - accuracy: 0.9145 - val_loss: 0.4149 - val_accuracy: 0.8966\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.3333 - accuracy: 0.9402 - val_loss: 0.4032 - val_accuracy: 0.8966\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.3614 - accuracy: 0.9060 - val_loss: 0.5014 - val_accuracy: 0.7931\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.4400 - accuracy: 0.8205 - val_loss: 0.6940 - val_accuracy: 0.8276\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.4132 - accuracy: 0.8974 - val_loss: 0.4964 - val_accuracy: 0.8966\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 96us/step - loss: 0.3395 - accuracy: 0.8974 - val_loss: 0.7185 - val_accuracy: 0.7241\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.4583 - accuracy: 0.8547 - val_loss: 0.4238 - val_accuracy: 0.8966\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 85us/step - loss: 0.3224 - accuracy: 0.8974 - val_loss: 0.6894 - val_accuracy: 0.6552\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 71us/step - loss: 0.4466 - accuracy: 0.8376 - val_loss: 0.4925 - val_accuracy: 0.8621\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.4789 - accuracy: 0.8291 - val_loss: 0.5022 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 86us/step - loss: 0.3476 - accuracy: 0.9573 - val_loss: 0.5149 - val_accuracy: 0.8621\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.3520 - accuracy: 0.9573 - val_loss: 0.4667 - val_accuracy: 0.8966\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 98us/step - loss: 0.3219 - accuracy: 0.9402 - val_loss: 0.4150 - val_accuracy: 0.8621\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 108us/step - loss: 0.3193 - accuracy: 0.9231 - val_loss: 0.3975 - val_accuracy: 0.8621\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 103us/step - loss: 0.3287 - accuracy: 0.9145 - val_loss: 0.3929 - val_accuracy: 0.8621\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.3238 - accuracy: 0.9145 - val_loss: 0.3968 - val_accuracy: 0.8621\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3068 - accuracy: 0.9402 - val_loss: 0.4173 - val_accuracy: 0.8966\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 93us/step - loss: 0.3086 - accuracy: 0.9402 - val_loss: 0.4411 - val_accuracy: 0.8966\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.3153 - accuracy: 0.9402 - val_loss: 0.4339 - val_accuracy: 0.8966\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.3116 - accuracy: 0.9402 - val_loss: 0.4075 - val_accuracy: 0.8966\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.3023 - accuracy: 0.9402 - val_loss: 0.3854 - val_accuracy: 0.8621\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.3023 - accuracy: 0.9402 - val_loss: 0.3762 - val_accuracy: 0.8621\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.3040 - accuracy: 0.9316 - val_loss: 0.3751 - val_accuracy: 0.8621\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.3018 - accuracy: 0.9316 - val_loss: 0.3783 - val_accuracy: 0.8966\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2974 - accuracy: 0.9402 - val_loss: 0.3803 - val_accuracy: 0.8966\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.2960 - accuracy: 0.9402 - val_loss: 0.3824 - val_accuracy: 0.8966\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2952 - accuracy: 0.9402 - val_loss: 0.3879 - val_accuracy: 0.8966\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 79us/step - loss: 0.2964 - accuracy: 0.9487 - val_loss: 0.3895 - val_accuracy: 0.8966\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 0.2962 - accuracy: 0.9487 - val_loss: 0.3757 - val_accuracy: 0.8966\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.2942 - accuracy: 0.9316 - val_loss: 0.3624 - val_accuracy: 0.8966\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.2921 - accuracy: 0.9316 - val_loss: 0.3559 - val_accuracy: 0.8966\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2925 - accuracy: 0.9231 - val_loss: 0.3535 - val_accuracy: 0.8966\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 98us/step - loss: 0.2907 - accuracy: 0.9316 - val_loss: 0.3596 - val_accuracy: 0.8966\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 69us/step - loss: 0.2918 - accuracy: 0.9316 - val_loss: 0.3643 - val_accuracy: 0.8966\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.2919 - accuracy: 0.9402 - val_loss: 0.3585 - val_accuracy: 0.8966\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.2899 - accuracy: 0.9402 - val_loss: 0.3596 - val_accuracy: 0.8966\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 100us/step - loss: 0.2893 - accuracy: 0.9402 - val_loss: 0.3601 - val_accuracy: 0.8966\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.2882 - accuracy: 0.9402 - val_loss: 0.3504 - val_accuracy: 0.8966\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.2895 - accuracy: 0.9316 - val_loss: 0.3479 - val_accuracy: 0.9310\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 83us/step - loss: 0.2872 - accuracy: 0.9316 - val_loss: 0.3558 - val_accuracy: 0.8966\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 92us/step - loss: 0.2858 - accuracy: 0.9316 - val_loss: 0.3595 - val_accuracy: 0.8966\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.2863 - accuracy: 0.9316 - val_loss: 0.3641 - val_accuracy: 0.8966\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.2864 - accuracy: 0.9402 - val_loss: 0.3710 - val_accuracy: 0.8966\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 98us/step - loss: 0.2861 - accuracy: 0.9402 - val_loss: 0.3603 - val_accuracy: 0.8966\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.2863 - accuracy: 0.9316 - val_loss: 0.3531 - val_accuracy: 0.8621\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 82us/step - loss: 0.2864 - accuracy: 0.9316 - val_loss: 0.3537 - val_accuracy: 0.8966\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.2850 - accuracy: 0.9316 - val_loss: 0.3568 - val_accuracy: 0.8966\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2839 - accuracy: 0.9316 - val_loss: 0.3652 - val_accuracy: 0.8966\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 80us/step - loss: 0.2824 - accuracy: 0.9402 - val_loss: 0.3684 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 78us/step - loss: 0.2816 - accuracy: 0.9316 - val_loss: 0.3681 - val_accuracy: 0.8966\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2816 - accuracy: 0.9316 - val_loss: 0.3677 - val_accuracy: 0.8966\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2815 - accuracy: 0.9316 - val_loss: 0.3676 - val_accuracy: 0.8966\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.2815 - accuracy: 0.9316 - val_loss: 0.3676 - val_accuracy: 0.8966\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2816 - accuracy: 0.9316 - val_loss: 0.3676 - val_accuracy: 0.8966\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.2815 - accuracy: 0.9316 - val_loss: 0.3670 - val_accuracy: 0.8966\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 97us/step - loss: 0.2817 - accuracy: 0.9316 - val_loss: 0.3667 - val_accuracy: 0.8966\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 95us/step - loss: 0.2817 - accuracy: 0.9316 - val_loss: 0.3670 - val_accuracy: 0.8966\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.2818 - accuracy: 0.9316 - val_loss: 0.3679 - val_accuracy: 0.8966\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.2816 - accuracy: 0.9316 - val_loss: 0.3694 - val_accuracy: 0.8966\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 84us/step - loss: 0.2814 - accuracy: 0.9316 - val_loss: 0.3710 - val_accuracy: 0.8966\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2814 - accuracy: 0.9316 - val_loss: 0.3720 - val_accuracy: 0.8966\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.2811 - accuracy: 0.9316 - val_loss: 0.3723 - val_accuracy: 0.8966\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.2810 - accuracy: 0.9316 - val_loss: 0.3724 - val_accuracy: 0.8966\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 66us/step - loss: 0.2810 - accuracy: 0.9402 - val_loss: 0.3730 - val_accuracy: 0.8966\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 72us/step - loss: 0.2810 - accuracy: 0.9402 - val_loss: 0.3733 - val_accuracy: 0.8966\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 94us/step - loss: 0.2811 - accuracy: 0.9402 - val_loss: 0.3735 - val_accuracy: 0.8966\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 77us/step - loss: 0.2812 - accuracy: 0.9402 - val_loss: 0.3728 - val_accuracy: 0.8966\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 73us/step - loss: 0.2808 - accuracy: 0.9402 - val_loss: 0.3704 - val_accuracy: 0.8966\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 90us/step - loss: 0.2810 - accuracy: 0.9402 - val_loss: 0.3679 - val_accuracy: 0.8966\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 109us/step - loss: 0.2809 - accuracy: 0.9316 - val_loss: 0.3669 - val_accuracy: 0.8966\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 67us/step - loss: 0.2808 - accuracy: 0.9316 - val_loss: 0.3663 - val_accuracy: 0.8966\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 104us/step - loss: 0.2808 - accuracy: 0.9316 - val_loss: 0.3655 - val_accuracy: 0.8966\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 87us/step - loss: 0.2809 - accuracy: 0.9316 - val_loss: 0.3651 - val_accuracy: 0.8966\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 91us/step - loss: 0.2809 - accuracy: 0.9316 - val_loss: 0.3647 - val_accuracy: 0.8966\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 75us/step - loss: 0.2809 - accuracy: 0.9316 - val_loss: 0.3645 - val_accuracy: 0.8966\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 76us/step - loss: 0.2809 - accuracy: 0.9316 - val_loss: 0.3652 - val_accuracy: 0.8966\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 88us/step - loss: 0.2806 - accuracy: 0.9316 - val_loss: 0.3667 - val_accuracy: 0.8966\n",
            "29/29 [==============================] - 0s 32us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A8kE7OOMDzS_",
        "colab": {}
      },
      "source": [
        "history_dict = history.history "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "923e9d41-1e64-4a59-92d3-1bd2beda51df",
        "id": "DgBwfUt3DzCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy', 'lr'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1ae0e922-b7ec-4465-91ed-d804a588c264",
        "id": "b0VXd2L4DyuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "cvscores"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.34324508905410767, 0.9333333373069763],\n",
              " [0.5186513066291809, 0.7931034564971924],\n",
              " [0.277778685092926, 0.9655172228813171],\n",
              " [0.5730763673782349, 0.8620689511299133],\n",
              " [0.3666653633117676, 0.8965517282485962]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y7aOoQ8oDyEu",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "802945ca-f078-4dc4-9258-21571f20a2fc",
        "id": "AEuPCv9hDwIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "b6a34df1-6249-42d3-9845-a558789efd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f699d0ed6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "b5e19338-8f3a-47a1-d389-958be80db018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f699d058c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}