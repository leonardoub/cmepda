{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brats_classification_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqAs+o1eVGPBPyO0NCw7el",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/brats_classification_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkyGJ1ldXJ8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "1d9fe96b-d5c7-4612-8477-b71cb97884ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "95fb6857-763c-49d9-8e76-2fd0be1fcb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>INTENSITY_Mean_NET_FLAIR</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>227.7510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>209.7901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>165.1014</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>207.5531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>139.3234</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>195.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>176.3511</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>152.2227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>161.5295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>163.4821</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 707 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               ID        Date  VOLUME_ET  ...  TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0    TCGA-02-0006  1996.08.23       1662  ...     0.052741    2.00000        GBM\n",
              "1    TCGA-02-0009  1997.06.14       4362  ...     0.094336   91.47360        GBM\n",
              "2    TCGA-02-0011  1998.02.01      33404  ...     0.096035  272.42900        GBM\n",
              "3    TCGA-02-0027  1999.03.28      12114  ...     0.096470  128.46800        GBM\n",
              "4    TCGA-02-0033  1997.05.26      34538  ...     0.096894  240.77800        GBM\n",
              "..            ...         ...        ...  ...          ...        ...        ...\n",
              "141  TCGA-HT-7694  1995.04.04       1036  ...     0.090456  719.23800        LGG\n",
              "142  TCGA-HT-8018  1997.04.11       2093  ...     0.054307    2.00000        LGG\n",
              "143  TCGA-HT-8111  1998.03.30       1929  ...     0.126712    7.06744        LGG\n",
              "144  TCGA-HT-8114  1998.10.30       8755  ...     0.107433   15.52240        LGG\n",
              "145  TCGA-HT-8563  1998.12.09      11757  ...     0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 707 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "8cfd6e49-2152-449f-9bb3-0a887375e3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Date', 'VOLUME_ET', 'VOLUME_NET', 'VOLUME_ED', 'VOLUME_TC',\n",
              "       'VOLUME_WT', 'VOLUME_BRAIN', 'VOLUME_ET_OVER_NET', 'VOLUME_ET_OVER_ED',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=707)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkF1sf1E578v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "labels_enc = encoder.fit_transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BoXqyfbpydt",
        "colab_type": "text"
      },
      "source": [
        "#NO K-FOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(data, labels_enc, test_size=0.2, stratify=labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=0.2, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6iyOqcBq0RC"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "125463af-b960-4792-8cf6-9563c5e440be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "outputId": "d4536425-7ece-4ea7-e9f7-be5f9b46319a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv9VpoZl6u_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        " \n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGgKscDi8Yxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf1fd90-34be-4e6a-a1a1-8e7b30e666d3"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.71875   , 1.64285714])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRei87g-88mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights_dict = dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNHf3d5c8_Dh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c2c517d-ed88-41d8-e6b5-edc1349ae331"
      },
      "source": [
        "class_weights_dict"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.71875, 1: 1.6428571428571428}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPtAmHH-3cBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T45_PAo3VN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(40,), kernel_regularizer=regularizers.l2(l=0.005)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[get_f1])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15a63087-a236-49b3-be62-ec94d19f4712",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0c9fd6c7-5f26-46de-ebdd-47dafcb6b832",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, y_train, validation_data=(val_data_stand_pca, y_val), \n",
        "                      epochs= num_epochs, batch_size=92, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['get_f1']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_get_f1']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 1.0453 - get_f1: 0.3908 - val_loss: 1.0112 - val_get_f1: 0.2500\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 1.0334 - get_f1: 0.3908 - val_loss: 1.0036 - val_get_f1: 0.2609\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 1.0216 - get_f1: 0.4091 - val_loss: 0.9968 - val_get_f1: 0.2609\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 1.0110 - get_f1: 0.4091 - val_loss: 0.9895 - val_get_f1: 0.2609\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.9995 - get_f1: 0.4091 - val_loss: 0.9824 - val_get_f1: 0.2609\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.9886 - get_f1: 0.4091 - val_loss: 0.9753 - val_get_f1: 0.2609\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.9777 - get_f1: 0.4091 - val_loss: 0.9681 - val_get_f1: 0.2500\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.9667 - get_f1: 0.4091 - val_loss: 0.9611 - val_get_f1: 0.2500\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.9560 - get_f1: 0.4091 - val_loss: 0.9543 - val_get_f1: 0.2609\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.9456 - get_f1: 0.4091 - val_loss: 0.9475 - val_get_f1: 0.2727\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.9352 - get_f1: 0.4091 - val_loss: 0.9408 - val_get_f1: 0.2727\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.9249 - get_f1: 0.4091 - val_loss: 0.9340 - val_get_f1: 0.2727\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.9148 - get_f1: 0.4091 - val_loss: 0.9275 - val_get_f1: 0.2727\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.9049 - get_f1: 0.4138 - val_loss: 0.9210 - val_get_f1: 0.2727\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.8951 - get_f1: 0.4138 - val_loss: 0.9146 - val_get_f1: 0.2727\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.8854 - get_f1: 0.4138 - val_loss: 0.9083 - val_get_f1: 0.2727\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.8759 - get_f1: 0.4138 - val_loss: 0.9021 - val_get_f1: 0.2727\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.8666 - get_f1: 0.4318 - val_loss: 0.8959 - val_get_f1: 0.2727\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.8573 - get_f1: 0.4318 - val_loss: 0.8898 - val_get_f1: 0.2727\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.8482 - get_f1: 0.4368 - val_loss: 0.8838 - val_get_f1: 0.2727\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.8393 - get_f1: 0.4419 - val_loss: 0.8779 - val_get_f1: 0.2727\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.8305 - get_f1: 0.4419 - val_loss: 0.8720 - val_get_f1: 0.2857\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.8218 - get_f1: 0.4471 - val_loss: 0.8662 - val_get_f1: 0.3636\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.8132 - get_f1: 0.4471 - val_loss: 0.8605 - val_get_f1: 0.3636\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.8048 - get_f1: 0.4651 - val_loss: 0.8548 - val_get_f1: 0.3810\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7965 - get_f1: 0.4651 - val_loss: 0.8492 - val_get_f1: 0.4000\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7883 - get_f1: 0.4941 - val_loss: 0.8438 - val_get_f1: 0.4000\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7803 - get_f1: 0.5000 - val_loss: 0.8384 - val_get_f1: 0.4000\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7725 - get_f1: 0.5122 - val_loss: 0.8331 - val_get_f1: 0.4000\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7647 - get_f1: 0.5301 - val_loss: 0.8279 - val_get_f1: 0.4000\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.7571 - get_f1: 0.5301 - val_loss: 0.8227 - val_get_f1: 0.4000\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.7496 - get_f1: 0.5366 - val_loss: 0.8178 - val_get_f1: 0.4000\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.7422 - get_f1: 0.5542 - val_loss: 0.8128 - val_get_f1: 0.4000\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7350 - get_f1: 0.5542 - val_loss: 0.8079 - val_get_f1: 0.4211\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.7279 - get_f1: 0.5714 - val_loss: 0.8029 - val_get_f1: 0.4211\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.7208 - get_f1: 0.6353 - val_loss: 0.7981 - val_get_f1: 0.4211\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.7138 - get_f1: 0.6429 - val_loss: 0.7933 - val_get_f1: 0.4211\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.7069 - get_f1: 0.6429 - val_loss: 0.7884 - val_get_f1: 0.4211\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.7002 - get_f1: 0.6506 - val_loss: 0.7836 - val_get_f1: 0.4211\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.6935 - get_f1: 0.6506 - val_loss: 0.7787 - val_get_f1: 0.4211\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6870 - get_f1: 0.6667 - val_loss: 0.7738 - val_get_f1: 0.4211\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.6805 - get_f1: 0.6667 - val_loss: 0.7690 - val_get_f1: 0.4444\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6743 - get_f1: 0.6667 - val_loss: 0.7642 - val_get_f1: 0.4444\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6681 - get_f1: 0.6667 - val_loss: 0.7594 - val_get_f1: 0.4706\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.6619 - get_f1: 0.6829 - val_loss: 0.7546 - val_get_f1: 0.4706\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.6558 - get_f1: 0.6914 - val_loss: 0.7498 - val_get_f1: 0.4706\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6498 - get_f1: 0.7089 - val_loss: 0.7450 - val_get_f1: 0.4706\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.6439 - get_f1: 0.7089 - val_loss: 0.7403 - val_get_f1: 0.4706\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6380 - get_f1: 0.7089 - val_loss: 0.7357 - val_get_f1: 0.5556\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.6323 - get_f1: 0.7179 - val_loss: 0.7311 - val_get_f1: 0.5882\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.6265 - get_f1: 0.7273 - val_loss: 0.7266 - val_get_f1: 0.5882\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.6209 - get_f1: 0.7273 - val_loss: 0.7221 - val_get_f1: 0.5882\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6152 - get_f1: 0.7273 - val_loss: 0.7177 - val_get_f1: 0.5882\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.6097 - get_f1: 0.7368 - val_loss: 0.7134 - val_get_f1: 0.5882\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.6042 - get_f1: 0.7368 - val_loss: 0.7091 - val_get_f1: 0.5882\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.5988 - get_f1: 0.7467 - val_loss: 0.7048 - val_get_f1: 0.5882\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5933 - get_f1: 0.7671 - val_loss: 0.7006 - val_get_f1: 0.5882\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.5880 - get_f1: 0.7778 - val_loss: 0.6965 - val_get_f1: 0.5882\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5828 - get_f1: 0.7887 - val_loss: 0.6924 - val_get_f1: 0.5882\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5776 - get_f1: 0.7887 - val_loss: 0.6884 - val_get_f1: 0.5882\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5724 - get_f1: 0.7887 - val_loss: 0.6844 - val_get_f1: 0.5882\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5672 - get_f1: 0.7887 - val_loss: 0.6805 - val_get_f1: 0.5882\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5622 - get_f1: 0.7887 - val_loss: 0.6765 - val_get_f1: 0.6250\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5572 - get_f1: 0.7887 - val_loss: 0.6727 - val_get_f1: 0.6250\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5522 - get_f1: 0.8116 - val_loss: 0.6688 - val_get_f1: 0.6250\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5473 - get_f1: 0.8116 - val_loss: 0.6651 - val_get_f1: 0.6250\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5424 - get_f1: 0.8116 - val_loss: 0.6613 - val_get_f1: 0.6250\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.5376 - get_f1: 0.8235 - val_loss: 0.6577 - val_get_f1: 0.6250\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5328 - get_f1: 0.8235 - val_loss: 0.6541 - val_get_f1: 0.6250\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5281 - get_f1: 0.8358 - val_loss: 0.6506 - val_get_f1: 0.6250\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5234 - get_f1: 0.8485 - val_loss: 0.6471 - val_get_f1: 0.6250\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5187 - get_f1: 0.8615 - val_loss: 0.6436 - val_get_f1: 0.6250\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.5142 - get_f1: 0.8615 - val_loss: 0.6402 - val_get_f1: 0.6250\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 93us/step - loss: 0.5096 - get_f1: 0.8615 - val_loss: 0.6369 - val_get_f1: 0.6667\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.5051 - get_f1: 0.8615 - val_loss: 0.6336 - val_get_f1: 0.6667\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.5006 - get_f1: 0.8750 - val_loss: 0.6304 - val_get_f1: 0.6667\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.4961 - get_f1: 0.8750 - val_loss: 0.6272 - val_get_f1: 0.6667\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.4917 - get_f1: 0.8889 - val_loss: 0.6240 - val_get_f1: 0.6667\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.4873 - get_f1: 0.8889 - val_loss: 0.6209 - val_get_f1: 0.6667\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.4830 - get_f1: 0.9032 - val_loss: 0.6179 - val_get_f1: 0.6667\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4787 - get_f1: 0.9032 - val_loss: 0.6149 - val_get_f1: 0.6667\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.4745 - get_f1: 0.9032 - val_loss: 0.6120 - val_get_f1: 0.6667\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4704 - get_f1: 0.9032 - val_loss: 0.6091 - val_get_f1: 0.6667\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.4663 - get_f1: 0.9032 - val_loss: 0.6062 - val_get_f1: 0.6667\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4622 - get_f1: 0.9032 - val_loss: 0.6034 - val_get_f1: 0.6667\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4581 - get_f1: 0.9032 - val_loss: 0.6006 - val_get_f1: 0.6667\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4541 - get_f1: 0.9032 - val_loss: 0.5978 - val_get_f1: 0.6667\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.4502 - get_f1: 0.9032 - val_loss: 0.5951 - val_get_f1: 0.6667\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4463 - get_f1: 0.9333 - val_loss: 0.5924 - val_get_f1: 0.7143\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4425 - get_f1: 0.9333 - val_loss: 0.5896 - val_get_f1: 0.7143\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4387 - get_f1: 0.9333 - val_loss: 0.5870 - val_get_f1: 0.7143\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.4349 - get_f1: 0.9655 - val_loss: 0.5843 - val_get_f1: 0.8000\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.4312 - get_f1: 0.9655 - val_loss: 0.5817 - val_get_f1: 0.8000\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4275 - get_f1: 0.9655 - val_loss: 0.5791 - val_get_f1: 0.8000\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.4239 - get_f1: 0.9655 - val_loss: 0.5766 - val_get_f1: 0.8000\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.4204 - get_f1: 0.9655 - val_loss: 0.5741 - val_get_f1: 0.8000\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4168 - get_f1: 0.9655 - val_loss: 0.5716 - val_get_f1: 0.8000\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4133 - get_f1: 0.9655 - val_loss: 0.5691 - val_get_f1: 0.8000\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4099 - get_f1: 0.9825 - val_loss: 0.5666 - val_get_f1: 0.8000\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4065 - get_f1: 0.9825 - val_loss: 0.5643 - val_get_f1: 0.8000\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.4031 - get_f1: 0.9825 - val_loss: 0.5619 - val_get_f1: 0.8000\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3998 - get_f1: 0.9825 - val_loss: 0.5596 - val_get_f1: 0.8000\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3965 - get_f1: 0.9825 - val_loss: 0.5574 - val_get_f1: 0.8000\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.3932 - get_f1: 0.9825 - val_loss: 0.5552 - val_get_f1: 0.8000\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3900 - get_f1: 0.9825 - val_loss: 0.5531 - val_get_f1: 0.8000\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3868 - get_f1: 0.9825 - val_loss: 0.5510 - val_get_f1: 0.8000\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3837 - get_f1: 0.9825 - val_loss: 0.5489 - val_get_f1: 0.8000\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3806 - get_f1: 1.0000 - val_loss: 0.5468 - val_get_f1: 0.8000\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.3776 - get_f1: 1.0000 - val_loss: 0.5448 - val_get_f1: 0.8000\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3746 - get_f1: 1.0000 - val_loss: 0.5428 - val_get_f1: 0.8000\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3716 - get_f1: 1.0000 - val_loss: 0.5409 - val_get_f1: 0.8571\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3686 - get_f1: 1.0000 - val_loss: 0.5389 - val_get_f1: 0.8571\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3657 - get_f1: 1.0000 - val_loss: 0.5369 - val_get_f1: 0.8571\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3628 - get_f1: 1.0000 - val_loss: 0.5349 - val_get_f1: 0.8571\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3600 - get_f1: 1.0000 - val_loss: 0.5329 - val_get_f1: 0.8571\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3572 - get_f1: 1.0000 - val_loss: 0.5310 - val_get_f1: 0.8571\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3544 - get_f1: 1.0000 - val_loss: 0.5291 - val_get_f1: 0.8571\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3517 - get_f1: 1.0000 - val_loss: 0.5273 - val_get_f1: 0.8571\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3490 - get_f1: 1.0000 - val_loss: 0.5255 - val_get_f1: 0.8571\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3463 - get_f1: 1.0000 - val_loss: 0.5237 - val_get_f1: 0.8571\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3437 - get_f1: 1.0000 - val_loss: 0.5219 - val_get_f1: 0.8571\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3411 - get_f1: 1.0000 - val_loss: 0.5202 - val_get_f1: 0.8571\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3386 - get_f1: 1.0000 - val_loss: 0.5185 - val_get_f1: 0.8571\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3360 - get_f1: 1.0000 - val_loss: 0.5167 - val_get_f1: 0.8571\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3335 - get_f1: 1.0000 - val_loss: 0.5150 - val_get_f1: 0.8571\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3311 - get_f1: 1.0000 - val_loss: 0.5134 - val_get_f1: 0.8571\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3286 - get_f1: 1.0000 - val_loss: 0.5117 - val_get_f1: 0.8571\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3262 - get_f1: 1.0000 - val_loss: 0.5101 - val_get_f1: 0.8571\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3239 - get_f1: 1.0000 - val_loss: 0.5085 - val_get_f1: 0.8571\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3215 - get_f1: 1.0000 - val_loss: 0.5069 - val_get_f1: 0.8571\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3192 - get_f1: 1.0000 - val_loss: 0.5053 - val_get_f1: 0.8571\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3169 - get_f1: 1.0000 - val_loss: 0.5038 - val_get_f1: 0.8571\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3147 - get_f1: 1.0000 - val_loss: 0.5022 - val_get_f1: 0.8571\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3124 - get_f1: 1.0000 - val_loss: 0.5007 - val_get_f1: 0.8571\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3103 - get_f1: 1.0000 - val_loss: 0.4992 - val_get_f1: 0.8571\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3081 - get_f1: 1.0000 - val_loss: 0.4977 - val_get_f1: 0.8571\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3059 - get_f1: 1.0000 - val_loss: 0.4963 - val_get_f1: 0.8571\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3038 - get_f1: 1.0000 - val_loss: 0.4949 - val_get_f1: 0.8571\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3017 - get_f1: 1.0000 - val_loss: 0.4935 - val_get_f1: 0.8571\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2997 - get_f1: 1.0000 - val_loss: 0.4921 - val_get_f1: 0.8571\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2977 - get_f1: 1.0000 - val_loss: 0.4907 - val_get_f1: 0.8571\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2957 - get_f1: 1.0000 - val_loss: 0.4893 - val_get_f1: 0.8571\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2937 - get_f1: 1.0000 - val_loss: 0.4880 - val_get_f1: 0.8571\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2917 - get_f1: 1.0000 - val_loss: 0.4866 - val_get_f1: 0.8571\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2898 - get_f1: 1.0000 - val_loss: 0.4853 - val_get_f1: 0.8571\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2879 - get_f1: 1.0000 - val_loss: 0.4840 - val_get_f1: 0.8571\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2860 - get_f1: 1.0000 - val_loss: 0.4827 - val_get_f1: 0.8571\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2842 - get_f1: 1.0000 - val_loss: 0.4815 - val_get_f1: 0.8571\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2823 - get_f1: 1.0000 - val_loss: 0.4802 - val_get_f1: 0.8571\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2805 - get_f1: 1.0000 - val_loss: 0.4790 - val_get_f1: 0.8571\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2788 - get_f1: 1.0000 - val_loss: 0.4778 - val_get_f1: 0.8571\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2770 - get_f1: 1.0000 - val_loss: 0.4766 - val_get_f1: 0.8571\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 89us/step - loss: 0.2753 - get_f1: 1.0000 - val_loss: 0.4755 - val_get_f1: 0.8571\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2736 - get_f1: 1.0000 - val_loss: 0.4744 - val_get_f1: 0.8571\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2720 - get_f1: 1.0000 - val_loss: 0.4734 - val_get_f1: 0.8571\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2703 - get_f1: 1.0000 - val_loss: 0.4724 - val_get_f1: 0.8571\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2687 - get_f1: 1.0000 - val_loss: 0.4714 - val_get_f1: 0.8571\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2671 - get_f1: 1.0000 - val_loss: 0.4704 - val_get_f1: 0.8571\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2655 - get_f1: 1.0000 - val_loss: 0.4695 - val_get_f1: 0.8571\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2639 - get_f1: 1.0000 - val_loss: 0.4686 - val_get_f1: 0.8571\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2624 - get_f1: 1.0000 - val_loss: 0.4677 - val_get_f1: 0.8571\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2609 - get_f1: 1.0000 - val_loss: 0.4668 - val_get_f1: 0.8571\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2593 - get_f1: 1.0000 - val_loss: 0.4659 - val_get_f1: 0.8571\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2579 - get_f1: 1.0000 - val_loss: 0.4650 - val_get_f1: 0.8571\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2564 - get_f1: 1.0000 - val_loss: 0.4642 - val_get_f1: 0.8571\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2549 - get_f1: 1.0000 - val_loss: 0.4633 - val_get_f1: 0.8571\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2535 - get_f1: 1.0000 - val_loss: 0.4626 - val_get_f1: 0.8571\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2521 - get_f1: 1.0000 - val_loss: 0.4618 - val_get_f1: 0.8571\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2507 - get_f1: 1.0000 - val_loss: 0.4611 - val_get_f1: 0.8571\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2493 - get_f1: 1.0000 - val_loss: 0.4604 - val_get_f1: 0.8571\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2479 - get_f1: 1.0000 - val_loss: 0.4598 - val_get_f1: 0.8571\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2465 - get_f1: 1.0000 - val_loss: 0.4591 - val_get_f1: 0.8571\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2452 - get_f1: 1.0000 - val_loss: 0.4584 - val_get_f1: 0.8571\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2439 - get_f1: 1.0000 - val_loss: 0.4577 - val_get_f1: 0.8571\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2426 - get_f1: 1.0000 - val_loss: 0.4571 - val_get_f1: 0.8571\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2413 - get_f1: 1.0000 - val_loss: 0.4564 - val_get_f1: 0.8571\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2400 - get_f1: 1.0000 - val_loss: 0.4557 - val_get_f1: 0.8571\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2388 - get_f1: 1.0000 - val_loss: 0.4551 - val_get_f1: 0.8571\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2375 - get_f1: 1.0000 - val_loss: 0.4544 - val_get_f1: 0.8571\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2363 - get_f1: 1.0000 - val_loss: 0.4538 - val_get_f1: 0.8571\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2351 - get_f1: 1.0000 - val_loss: 0.4532 - val_get_f1: 0.8571\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2339 - get_f1: 1.0000 - val_loss: 0.4526 - val_get_f1: 0.8571\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2327 - get_f1: 1.0000 - val_loss: 0.4520 - val_get_f1: 0.8571\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2315 - get_f1: 1.0000 - val_loss: 0.4513 - val_get_f1: 0.8571\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2304 - get_f1: 1.0000 - val_loss: 0.4507 - val_get_f1: 0.8571\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2293 - get_f1: 1.0000 - val_loss: 0.4501 - val_get_f1: 0.8571\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2281 - get_f1: 1.0000 - val_loss: 0.4495 - val_get_f1: 0.8571\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2270 - get_f1: 1.0000 - val_loss: 0.4489 - val_get_f1: 0.8571\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2259 - get_f1: 1.0000 - val_loss: 0.4483 - val_get_f1: 0.8571\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2248 - get_f1: 1.0000 - val_loss: 0.4478 - val_get_f1: 0.8571\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2237 - get_f1: 1.0000 - val_loss: 0.4472 - val_get_f1: 0.8571\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2227 - get_f1: 1.0000 - val_loss: 0.4467 - val_get_f1: 0.8571\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2216 - get_f1: 1.0000 - val_loss: 0.4461 - val_get_f1: 0.8571\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2206 - get_f1: 1.0000 - val_loss: 0.4456 - val_get_f1: 0.8571\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2195 - get_f1: 1.0000 - val_loss: 0.4450 - val_get_f1: 0.8571\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2185 - get_f1: 1.0000 - val_loss: 0.4445 - val_get_f1: 0.8571\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2175 - get_f1: 1.0000 - val_loss: 0.4440 - val_get_f1: 0.8571\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2165 - get_f1: 1.0000 - val_loss: 0.4435 - val_get_f1: 0.8571\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2155 - get_f1: 1.0000 - val_loss: 0.4430 - val_get_f1: 0.8571\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2145 - get_f1: 1.0000 - val_loss: 0.4426 - val_get_f1: 0.8571\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2136 - get_f1: 1.0000 - val_loss: 0.4421 - val_get_f1: 0.8571\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2126 - get_f1: 1.0000 - val_loss: 0.4417 - val_get_f1: 0.8571\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2117 - get_f1: 1.0000 - val_loss: 0.4412 - val_get_f1: 0.8571\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2107 - get_f1: 1.0000 - val_loss: 0.4407 - val_get_f1: 0.8571\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2098 - get_f1: 1.0000 - val_loss: 0.4403 - val_get_f1: 0.8571\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2089 - get_f1: 1.0000 - val_loss: 0.4398 - val_get_f1: 0.8571\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2080 - get_f1: 1.0000 - val_loss: 0.4394 - val_get_f1: 0.8571\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2071 - get_f1: 1.0000 - val_loss: 0.4390 - val_get_f1: 0.8571\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2062 - get_f1: 1.0000 - val_loss: 0.4386 - val_get_f1: 0.8571\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 93us/step - loss: 0.2053 - get_f1: 1.0000 - val_loss: 0.4382 - val_get_f1: 0.8571\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2044 - get_f1: 1.0000 - val_loss: 0.4378 - val_get_f1: 0.8571\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2036 - get_f1: 1.0000 - val_loss: 0.4374 - val_get_f1: 0.8571\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2027 - get_f1: 1.0000 - val_loss: 0.4370 - val_get_f1: 0.8571\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2018 - get_f1: 1.0000 - val_loss: 0.4366 - val_get_f1: 0.8571\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2010 - get_f1: 1.0000 - val_loss: 0.4362 - val_get_f1: 0.8571\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 119us/step - loss: 0.2002 - get_f1: 1.0000 - val_loss: 0.4358 - val_get_f1: 0.8571\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1993 - get_f1: 1.0000 - val_loss: 0.4354 - val_get_f1: 0.8571\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1985 - get_f1: 1.0000 - val_loss: 0.4350 - val_get_f1: 0.8571\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1977 - get_f1: 1.0000 - val_loss: 0.4347 - val_get_f1: 0.8571\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1969 - get_f1: 1.0000 - val_loss: 0.4343 - val_get_f1: 0.8571\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1961 - get_f1: 1.0000 - val_loss: 0.4340 - val_get_f1: 0.8571\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1953 - get_f1: 1.0000 - val_loss: 0.4337 - val_get_f1: 0.8571\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1946 - get_f1: 1.0000 - val_loss: 0.4333 - val_get_f1: 0.8571\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1938 - get_f1: 1.0000 - val_loss: 0.4330 - val_get_f1: 0.8571\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1930 - get_f1: 1.0000 - val_loss: 0.4327 - val_get_f1: 0.8571\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1923 - get_f1: 1.0000 - val_loss: 0.4324 - val_get_f1: 0.8571\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1915 - get_f1: 1.0000 - val_loss: 0.4321 - val_get_f1: 0.8571\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1908 - get_f1: 1.0000 - val_loss: 0.4317 - val_get_f1: 0.8571\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1900 - get_f1: 1.0000 - val_loss: 0.4314 - val_get_f1: 0.8571\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1893 - get_f1: 1.0000 - val_loss: 0.4310 - val_get_f1: 0.8571\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.1886 - get_f1: 1.0000 - val_loss: 0.4307 - val_get_f1: 0.8571\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1878 - get_f1: 1.0000 - val_loss: 0.4303 - val_get_f1: 0.8571\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1871 - get_f1: 1.0000 - val_loss: 0.4300 - val_get_f1: 0.8571\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1864 - get_f1: 1.0000 - val_loss: 0.4296 - val_get_f1: 0.8571\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1857 - get_f1: 1.0000 - val_loss: 0.4293 - val_get_f1: 0.8571\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1850 - get_f1: 1.0000 - val_loss: 0.4289 - val_get_f1: 0.8571\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1843 - get_f1: 1.0000 - val_loss: 0.4286 - val_get_f1: 0.8571\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1836 - get_f1: 1.0000 - val_loss: 0.4283 - val_get_f1: 0.8571\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1829 - get_f1: 1.0000 - val_loss: 0.4280 - val_get_f1: 0.8571\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1823 - get_f1: 1.0000 - val_loss: 0.4276 - val_get_f1: 0.8571\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1816 - get_f1: 1.0000 - val_loss: 0.4273 - val_get_f1: 0.8571\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1809 - get_f1: 1.0000 - val_loss: 0.4270 - val_get_f1: 0.8571\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1802 - get_f1: 1.0000 - val_loss: 0.4267 - val_get_f1: 0.8000\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1796 - get_f1: 1.0000 - val_loss: 0.4265 - val_get_f1: 0.8000\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1789 - get_f1: 1.0000 - val_loss: 0.4262 - val_get_f1: 0.8000\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1783 - get_f1: 1.0000 - val_loss: 0.4259 - val_get_f1: 0.8000\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1776 - get_f1: 1.0000 - val_loss: 0.4256 - val_get_f1: 0.8000\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1770 - get_f1: 1.0000 - val_loss: 0.4253 - val_get_f1: 0.8000\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1764 - get_f1: 1.0000 - val_loss: 0.4250 - val_get_f1: 0.8000\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1757 - get_f1: 1.0000 - val_loss: 0.4247 - val_get_f1: 0.8000\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1751 - get_f1: 1.0000 - val_loss: 0.4244 - val_get_f1: 0.8000\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1745 - get_f1: 1.0000 - val_loss: 0.4241 - val_get_f1: 0.8000\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1739 - get_f1: 1.0000 - val_loss: 0.4239 - val_get_f1: 0.8000\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1733 - get_f1: 1.0000 - val_loss: 0.4236 - val_get_f1: 0.8000\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1726 - get_f1: 1.0000 - val_loss: 0.4234 - val_get_f1: 0.8000\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1720 - get_f1: 1.0000 - val_loss: 0.4232 - val_get_f1: 0.8000\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1714 - get_f1: 1.0000 - val_loss: 0.4229 - val_get_f1: 0.8000\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1708 - get_f1: 1.0000 - val_loss: 0.4227 - val_get_f1: 0.8000\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1702 - get_f1: 1.0000 - val_loss: 0.4225 - val_get_f1: 0.8000\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1697 - get_f1: 1.0000 - val_loss: 0.4223 - val_get_f1: 0.8000\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.1691 - get_f1: 1.0000 - val_loss: 0.4220 - val_get_f1: 0.8000\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.1685 - get_f1: 1.0000 - val_loss: 0.4218 - val_get_f1: 0.8000\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1679 - get_f1: 1.0000 - val_loss: 0.4215 - val_get_f1: 0.8000\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1673 - get_f1: 1.0000 - val_loss: 0.4213 - val_get_f1: 0.8000\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1668 - get_f1: 1.0000 - val_loss: 0.4210 - val_get_f1: 0.8000\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1662 - get_f1: 1.0000 - val_loss: 0.4208 - val_get_f1: 0.8000\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1656 - get_f1: 1.0000 - val_loss: 0.4206 - val_get_f1: 0.8000\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1651 - get_f1: 1.0000 - val_loss: 0.4204 - val_get_f1: 0.8000\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1645 - get_f1: 1.0000 - val_loss: 0.4203 - val_get_f1: 0.8000\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.1640 - get_f1: 1.0000 - val_loss: 0.4201 - val_get_f1: 0.8000\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1634 - get_f1: 1.0000 - val_loss: 0.4199 - val_get_f1: 0.8000\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1629 - get_f1: 1.0000 - val_loss: 0.4196 - val_get_f1: 0.8000\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1623 - get_f1: 1.0000 - val_loss: 0.4194 - val_get_f1: 0.8000\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1618 - get_f1: 1.0000 - val_loss: 0.4192 - val_get_f1: 0.8000\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1613 - get_f1: 1.0000 - val_loss: 0.4191 - val_get_f1: 0.8000\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.1607 - get_f1: 1.0000 - val_loss: 0.4189 - val_get_f1: 0.8000\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1602 - get_f1: 1.0000 - val_loss: 0.4187 - val_get_f1: 0.8000\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1597 - get_f1: 1.0000 - val_loss: 0.4185 - val_get_f1: 0.8000\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1592 - get_f1: 1.0000 - val_loss: 0.4182 - val_get_f1: 0.8000\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1587 - get_f1: 1.0000 - val_loss: 0.4180 - val_get_f1: 0.8000\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1581 - get_f1: 1.0000 - val_loss: 0.4178 - val_get_f1: 0.8000\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1576 - get_f1: 1.0000 - val_loss: 0.4176 - val_get_f1: 0.8000\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1571 - get_f1: 1.0000 - val_loss: 0.4174 - val_get_f1: 0.8000\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1566 - get_f1: 1.0000 - val_loss: 0.4172 - val_get_f1: 0.8000\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1561 - get_f1: 1.0000 - val_loss: 0.4171 - val_get_f1: 0.8000\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1556 - get_f1: 1.0000 - val_loss: 0.4169 - val_get_f1: 0.8000\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1551 - get_f1: 1.0000 - val_loss: 0.4167 - val_get_f1: 0.8000\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1546 - get_f1: 1.0000 - val_loss: 0.4165 - val_get_f1: 0.8000\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1541 - get_f1: 1.0000 - val_loss: 0.4164 - val_get_f1: 0.8000\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1536 - get_f1: 1.0000 - val_loss: 0.4162 - val_get_f1: 0.8000\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1531 - get_f1: 1.0000 - val_loss: 0.4161 - val_get_f1: 0.8000\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1526 - get_f1: 1.0000 - val_loss: 0.4159 - val_get_f1: 0.8000\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1522 - get_f1: 1.0000 - val_loss: 0.4158 - val_get_f1: 0.8000\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1517 - get_f1: 1.0000 - val_loss: 0.4156 - val_get_f1: 0.8000\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1512 - get_f1: 1.0000 - val_loss: 0.4155 - val_get_f1: 0.8000\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1507 - get_f1: 1.0000 - val_loss: 0.4154 - val_get_f1: 0.8000\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1503 - get_f1: 1.0000 - val_loss: 0.4153 - val_get_f1: 0.8000\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1498 - get_f1: 1.0000 - val_loss: 0.4153 - val_get_f1: 0.8000\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1493 - get_f1: 1.0000 - val_loss: 0.4152 - val_get_f1: 0.8000\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.1489 - get_f1: 1.0000 - val_loss: 0.4151 - val_get_f1: 0.8000\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1484 - get_f1: 1.0000 - val_loss: 0.4149 - val_get_f1: 0.8000\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1479 - get_f1: 1.0000 - val_loss: 0.4148 - val_get_f1: 0.8000\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1475 - get_f1: 1.0000 - val_loss: 0.4147 - val_get_f1: 0.8000\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1470 - get_f1: 1.0000 - val_loss: 0.4145 - val_get_f1: 0.8000\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1466 - get_f1: 1.0000 - val_loss: 0.4143 - val_get_f1: 0.8000\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1461 - get_f1: 1.0000 - val_loss: 0.4142 - val_get_f1: 0.8000\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1457 - get_f1: 1.0000 - val_loss: 0.4141 - val_get_f1: 0.8000\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1453 - get_f1: 1.0000 - val_loss: 0.4139 - val_get_f1: 0.8000\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1448 - get_f1: 1.0000 - val_loss: 0.4138 - val_get_f1: 0.8000\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1444 - get_f1: 1.0000 - val_loss: 0.4137 - val_get_f1: 0.8000\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1440 - get_f1: 1.0000 - val_loss: 0.4136 - val_get_f1: 0.8000\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1435 - get_f1: 1.0000 - val_loss: 0.4135 - val_get_f1: 0.8000\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1431 - get_f1: 1.0000 - val_loss: 0.4134 - val_get_f1: 0.8000\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1427 - get_f1: 1.0000 - val_loss: 0.4133 - val_get_f1: 0.8000\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1423 - get_f1: 1.0000 - val_loss: 0.4132 - val_get_f1: 0.8000\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1419 - get_f1: 1.0000 - val_loss: 0.4131 - val_get_f1: 0.8000\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1414 - get_f1: 1.0000 - val_loss: 0.4129 - val_get_f1: 0.8000\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.1410 - get_f1: 1.0000 - val_loss: 0.4128 - val_get_f1: 0.8000\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.1406 - get_f1: 1.0000 - val_loss: 0.4127 - val_get_f1: 0.8000\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1402 - get_f1: 1.0000 - val_loss: 0.4126 - val_get_f1: 0.8000\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1398 - get_f1: 1.0000 - val_loss: 0.4125 - val_get_f1: 0.8000\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1394 - get_f1: 1.0000 - val_loss: 0.4124 - val_get_f1: 0.8000\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1390 - get_f1: 1.0000 - val_loss: 0.4123 - val_get_f1: 0.8000\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1386 - get_f1: 1.0000 - val_loss: 0.4122 - val_get_f1: 0.8000\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1382 - get_f1: 1.0000 - val_loss: 0.4121 - val_get_f1: 0.8000\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1378 - get_f1: 1.0000 - val_loss: 0.4120 - val_get_f1: 0.8000\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1374 - get_f1: 1.0000 - val_loss: 0.4119 - val_get_f1: 0.8000\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1371 - get_f1: 1.0000 - val_loss: 0.4117 - val_get_f1: 0.8000\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1367 - get_f1: 1.0000 - val_loss: 0.4116 - val_get_f1: 0.8000\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1363 - get_f1: 1.0000 - val_loss: 0.4115 - val_get_f1: 0.8000\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1359 - get_f1: 1.0000 - val_loss: 0.4114 - val_get_f1: 0.8000\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1355 - get_f1: 1.0000 - val_loss: 0.4113 - val_get_f1: 0.8000\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1352 - get_f1: 1.0000 - val_loss: 0.4112 - val_get_f1: 0.8000\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1348 - get_f1: 1.0000 - val_loss: 0.4111 - val_get_f1: 0.8000\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1344 - get_f1: 1.0000 - val_loss: 0.4110 - val_get_f1: 0.8000\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 99us/step - loss: 0.1340 - get_f1: 1.0000 - val_loss: 0.4108 - val_get_f1: 0.8000\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1337 - get_f1: 1.0000 - val_loss: 0.4107 - val_get_f1: 0.8000\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1333 - get_f1: 1.0000 - val_loss: 0.4106 - val_get_f1: 0.8000\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1330 - get_f1: 1.0000 - val_loss: 0.4105 - val_get_f1: 0.8000\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1326 - get_f1: 1.0000 - val_loss: 0.4104 - val_get_f1: 0.8000\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1322 - get_f1: 1.0000 - val_loss: 0.4103 - val_get_f1: 0.8000\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.1319 - get_f1: 1.0000 - val_loss: 0.4102 - val_get_f1: 0.8000\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1315 - get_f1: 1.0000 - val_loss: 0.4101 - val_get_f1: 0.8000\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1312 - get_f1: 1.0000 - val_loss: 0.4100 - val_get_f1: 0.8000\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1308 - get_f1: 1.0000 - val_loss: 0.4100 - val_get_f1: 0.8000\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1305 - get_f1: 1.0000 - val_loss: 0.4099 - val_get_f1: 0.8000\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1301 - get_f1: 1.0000 - val_loss: 0.4098 - val_get_f1: 0.8000\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1298 - get_f1: 1.0000 - val_loss: 0.4097 - val_get_f1: 0.8000\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1295 - get_f1: 1.0000 - val_loss: 0.4097 - val_get_f1: 0.8000\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1291 - get_f1: 1.0000 - val_loss: 0.4096 - val_get_f1: 0.8000\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1288 - get_f1: 1.0000 - val_loss: 0.4095 - val_get_f1: 0.8000\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1284 - get_f1: 1.0000 - val_loss: 0.4094 - val_get_f1: 0.8000\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1281 - get_f1: 1.0000 - val_loss: 0.4093 - val_get_f1: 0.8000\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1278 - get_f1: 1.0000 - val_loss: 0.4092 - val_get_f1: 0.8000\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1274 - get_f1: 1.0000 - val_loss: 0.4092 - val_get_f1: 0.8000\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1271 - get_f1: 1.0000 - val_loss: 0.4091 - val_get_f1: 0.8000\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1268 - get_f1: 1.0000 - val_loss: 0.4090 - val_get_f1: 0.8000\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1265 - get_f1: 1.0000 - val_loss: 0.4089 - val_get_f1: 0.8000\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1261 - get_f1: 1.0000 - val_loss: 0.4088 - val_get_f1: 0.8000\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1258 - get_f1: 1.0000 - val_loss: 0.4087 - val_get_f1: 0.8000\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1255 - get_f1: 1.0000 - val_loss: 0.4087 - val_get_f1: 0.8000\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1252 - get_f1: 1.0000 - val_loss: 0.4086 - val_get_f1: 0.8000\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.1248 - get_f1: 1.0000 - val_loss: 0.4085 - val_get_f1: 0.8000\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1245 - get_f1: 1.0000 - val_loss: 0.4084 - val_get_f1: 0.8000\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1242 - get_f1: 1.0000 - val_loss: 0.4083 - val_get_f1: 0.8000\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1239 - get_f1: 1.0000 - val_loss: 0.4082 - val_get_f1: 0.8000\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1236 - get_f1: 1.0000 - val_loss: 0.4081 - val_get_f1: 0.8000\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1233 - get_f1: 1.0000 - val_loss: 0.4081 - val_get_f1: 0.8000\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1229 - get_f1: 1.0000 - val_loss: 0.4080 - val_get_f1: 0.8000\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1226 - get_f1: 1.0000 - val_loss: 0.4079 - val_get_f1: 0.8000\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.1223 - get_f1: 1.0000 - val_loss: 0.4078 - val_get_f1: 0.8000\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.1220 - get_f1: 1.0000 - val_loss: 0.4077 - val_get_f1: 0.8000\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1217 - get_f1: 1.0000 - val_loss: 0.4076 - val_get_f1: 0.8000\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1214 - get_f1: 1.0000 - val_loss: 0.4074 - val_get_f1: 0.8000\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1211 - get_f1: 1.0000 - val_loss: 0.4073 - val_get_f1: 0.8000\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1208 - get_f1: 1.0000 - val_loss: 0.4072 - val_get_f1: 0.8000\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1205 - get_f1: 1.0000 - val_loss: 0.4071 - val_get_f1: 0.8000\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1202 - get_f1: 1.0000 - val_loss: 0.4070 - val_get_f1: 0.8000\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1199 - get_f1: 1.0000 - val_loss: 0.4069 - val_get_f1: 0.8000\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1196 - get_f1: 1.0000 - val_loss: 0.4067 - val_get_f1: 0.8000\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1193 - get_f1: 1.0000 - val_loss: 0.4066 - val_get_f1: 0.8000\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1190 - get_f1: 1.0000 - val_loss: 0.4065 - val_get_f1: 0.8000\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1187 - get_f1: 1.0000 - val_loss: 0.4065 - val_get_f1: 0.8000\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1184 - get_f1: 1.0000 - val_loss: 0.4064 - val_get_f1: 0.8000\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1181 - get_f1: 1.0000 - val_loss: 0.4063 - val_get_f1: 0.8000\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1179 - get_f1: 1.0000 - val_loss: 0.4063 - val_get_f1: 0.8000\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.1176 - get_f1: 1.0000 - val_loss: 0.4062 - val_get_f1: 0.8000\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1173 - get_f1: 1.0000 - val_loss: 0.4061 - val_get_f1: 0.8000\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1170 - get_f1: 1.0000 - val_loss: 0.4060 - val_get_f1: 0.8000\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1167 - get_f1: 1.0000 - val_loss: 0.4059 - val_get_f1: 0.8000\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1165 - get_f1: 1.0000 - val_loss: 0.4059 - val_get_f1: 0.8000\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1162 - get_f1: 1.0000 - val_loss: 0.4058 - val_get_f1: 0.8000\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1159 - get_f1: 1.0000 - val_loss: 0.4058 - val_get_f1: 0.8000\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.1156 - get_f1: 1.0000 - val_loss: 0.4057 - val_get_f1: 0.8000\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.1153 - get_f1: 1.0000 - val_loss: 0.4056 - val_get_f1: 0.8000\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1151 - get_f1: 1.0000 - val_loss: 0.4056 - val_get_f1: 0.8000\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1148 - get_f1: 1.0000 - val_loss: 0.4055 - val_get_f1: 0.8000\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1145 - get_f1: 1.0000 - val_loss: 0.4055 - val_get_f1: 0.8000\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1142 - get_f1: 1.0000 - val_loss: 0.4055 - val_get_f1: 0.8000\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1140 - get_f1: 1.0000 - val_loss: 0.4054 - val_get_f1: 0.8000\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1137 - get_f1: 1.0000 - val_loss: 0.4054 - val_get_f1: 0.8000\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1134 - get_f1: 1.0000 - val_loss: 0.4053 - val_get_f1: 0.8000\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1132 - get_f1: 1.0000 - val_loss: 0.4053 - val_get_f1: 0.8000\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1129 - get_f1: 1.0000 - val_loss: 0.4052 - val_get_f1: 0.8000\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.1126 - get_f1: 1.0000 - val_loss: 0.4052 - val_get_f1: 0.8000\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.1124 - get_f1: 1.0000 - val_loss: 0.4052 - val_get_f1: 0.8000\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1121 - get_f1: 1.0000 - val_loss: 0.4051 - val_get_f1: 0.8000\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1118 - get_f1: 1.0000 - val_loss: 0.4051 - val_get_f1: 0.8000\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1116 - get_f1: 1.0000 - val_loss: 0.4051 - val_get_f1: 0.8000\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1113 - get_f1: 1.0000 - val_loss: 0.4051 - val_get_f1: 0.8000\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1111 - get_f1: 1.0000 - val_loss: 0.4050 - val_get_f1: 0.8000\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1108 - get_f1: 1.0000 - val_loss: 0.4050 - val_get_f1: 0.8000\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1105 - get_f1: 1.0000 - val_loss: 0.4049 - val_get_f1: 0.8000\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1103 - get_f1: 1.0000 - val_loss: 0.4049 - val_get_f1: 0.8000\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1100 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1098 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.1095 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1093 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1091 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1088 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1086 - get_f1: 1.0000 - val_loss: 0.4048 - val_get_f1: 0.8000\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1083 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1081 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1078 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1076 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1074 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.1071 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1069 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1066 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1064 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1062 - get_f1: 1.0000 - val_loss: 0.4047 - val_get_f1: 0.8000\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1059 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1057 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "\n",
            "Epoch 00433: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1055 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1054 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1054 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1054 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1054 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1053 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1053 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1053 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1053 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1053 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "\n",
            "Epoch 00443: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1052 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1052 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1052 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1052 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1051 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1051 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1051 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1051 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1050 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.1050 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1050 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1050 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1050 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1049 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1049 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1049 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1049 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1048 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1048 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1048 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1048 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1047 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1047 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1047 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.1047 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1047 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 104us/step - loss: 0.1046 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1046 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1046 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1046 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1045 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1045 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1045 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1045 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1044 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1044 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1044 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1044 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1043 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1043 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1043 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1043 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1043 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1042 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1042 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1042 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1042 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1041 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1041 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1041 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1041 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1040 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1040 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1040 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1040 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1039 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1039 - get_f1: 1.0000 - val_loss: 0.4046 - val_get_f1: 0.8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMwsavnM4r_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5c23275-d344-45d2-d0d0-c7eb90e782f8"
      },
      "source": [
        "\n",
        "history.history.keys()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_get_f1', 'loss', 'get_f1'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "437f422b-1cb4-4f6e-e8e7-884ade5646ef",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5f27195cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6484bd29-c934-4aa9-e183-eb5d26a30edf",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5f267f7d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c0589efe-c833-4a2d-9c5e-647b88ebdb2f",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, train_labels_dec, epochs= num_epochs, batch_size=92, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, test_labels_dec)\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 909us/step - loss: 0.9272 - get_f1: 0.4138\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.9208 - get_f1: 0.4138\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.9124 - get_f1: 0.4138\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.9040 - get_f1: 0.4138\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8963 - get_f1: 0.4235\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8884 - get_f1: 0.4235\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.8806 - get_f1: 0.4235\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8729 - get_f1: 0.4286\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.8651 - get_f1: 0.4286\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.8575 - get_f1: 0.4471\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8499 - get_f1: 0.4471\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8424 - get_f1: 0.4471\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8350 - get_f1: 0.4706\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8277 - get_f1: 0.4706\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.8205 - get_f1: 0.4706\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.8134 - get_f1: 0.4762\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8063 - get_f1: 0.4762\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.7993 - get_f1: 0.4762\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.7924 - get_f1: 0.4762\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.7856 - get_f1: 0.5000\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.7789 - get_f1: 0.5000\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.7723 - get_f1: 0.5060\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.7658 - get_f1: 0.5122\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7594 - get_f1: 0.5122\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7530 - get_f1: 0.5122\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.7467 - get_f1: 0.5301\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.7404 - get_f1: 0.5301\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7343 - get_f1: 0.5366\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.7282 - get_f1: 0.5366\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.7222 - get_f1: 0.5366\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7163 - get_f1: 0.5366\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.7103 - get_f1: 0.5432\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.7046 - get_f1: 0.5432\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.6988 - get_f1: 0.5432\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6932 - get_f1: 0.5432\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6876 - get_f1: 0.5750\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6820 - get_f1: 0.5926\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6765 - get_f1: 0.5926\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.6711 - get_f1: 0.6000\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6657 - get_f1: 0.6000\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.6604 - get_f1: 0.6000\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6552 - get_f1: 0.6000\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.6500 - get_f1: 0.6250\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6448 - get_f1: 0.6250\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.6397 - get_f1: 0.6250\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6348 - get_f1: 0.6250\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.6298 - get_f1: 0.6500\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6249 - get_f1: 0.6500\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6201 - get_f1: 0.6582\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.6153 - get_f1: 0.6582\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.6105 - get_f1: 0.6582\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6058 - get_f1: 0.6667\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6012 - get_f1: 0.6667\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5966 - get_f1: 0.6667\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5920 - get_f1: 0.6667\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.5875 - get_f1: 0.6667\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5830 - get_f1: 0.6753\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.5785 - get_f1: 0.6923\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.5742 - get_f1: 0.7013\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5698 - get_f1: 0.7105\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5654 - get_f1: 0.7105\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.5611 - get_f1: 0.7200\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5568 - get_f1: 0.7397\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5526 - get_f1: 0.7606\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5484 - get_f1: 0.7606\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5443 - get_f1: 0.7606\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5401 - get_f1: 0.7606\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5360 - get_f1: 0.7606\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5319 - get_f1: 0.7714\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5279 - get_f1: 0.7714\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.5239 - get_f1: 0.7826\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5199 - get_f1: 0.7941\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5159 - get_f1: 0.7941\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.5119 - get_f1: 0.7941\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.5080 - get_f1: 0.7941\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.5041 - get_f1: 0.8116\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.5002 - get_f1: 0.8116\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4964 - get_f1: 0.8235\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4926 - get_f1: 0.8235\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.4888 - get_f1: 0.8235\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4851 - get_f1: 0.8235\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4813 - get_f1: 0.8358\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4776 - get_f1: 0.8485\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4739 - get_f1: 0.8485\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4703 - get_f1: 0.8485\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4666 - get_f1: 0.8485\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4630 - get_f1: 0.8615\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4595 - get_f1: 0.8615\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4559 - get_f1: 0.8750\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4524 - get_f1: 0.8750\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4490 - get_f1: 0.8750\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4455 - get_f1: 0.8750\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4421 - get_f1: 0.8750\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.4386 - get_f1: 0.8750\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4353 - get_f1: 0.8889\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4319 - get_f1: 0.8889\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.4286 - get_f1: 0.9032\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4253 - get_f1: 0.9032\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4220 - get_f1: 0.9032\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4188 - get_f1: 0.9032\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.4155 - get_f1: 0.9032\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4123 - get_f1: 0.9032\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4092 - get_f1: 0.9032\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4061 - get_f1: 0.9180\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4030 - get_f1: 0.9180\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3999 - get_f1: 0.9180\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.3969 - get_f1: 0.9180\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.3938 - get_f1: 0.9180\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.3908 - get_f1: 0.9180\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3878 - get_f1: 0.9180\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3848 - get_f1: 0.9180\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3819 - get_f1: 0.9333\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3789 - get_f1: 0.9333\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3760 - get_f1: 0.9333\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3731 - get_f1: 0.9333\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3702 - get_f1: 0.9492\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3674 - get_f1: 0.9492\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3645 - get_f1: 0.9492\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3617 - get_f1: 0.9492\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3590 - get_f1: 0.9492\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3562 - get_f1: 0.9492\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3535 - get_f1: 0.9492\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3508 - get_f1: 0.9492\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3482 - get_f1: 0.9655\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3456 - get_f1: 0.9655\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3430 - get_f1: 0.9655\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3404 - get_f1: 0.9825\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3379 - get_f1: 0.9825\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3353 - get_f1: 0.9825\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3329 - get_f1: 0.9825\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3304 - get_f1: 0.9825\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3280 - get_f1: 0.9825\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3256 - get_f1: 0.9825\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3232 - get_f1: 0.9825\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3209 - get_f1: 0.9825\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3186 - get_f1: 0.9825\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3163 - get_f1: 0.9825\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.3141 - get_f1: 0.9825\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.3118 - get_f1: 0.9825\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.3096 - get_f1: 0.9825\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.3075 - get_f1: 0.9825\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3053 - get_f1: 0.9825\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3032 - get_f1: 0.9825\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3011 - get_f1: 1.0000\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2990 - get_f1: 1.0000\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2969 - get_f1: 1.0000\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2949 - get_f1: 1.0000\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2929 - get_f1: 1.0000\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2909 - get_f1: 1.0000\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2889 - get_f1: 1.0000\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.2870 - get_f1: 1.0000\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2851 - get_f1: 1.0000\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2832 - get_f1: 1.0000\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2813 - get_f1: 1.0000\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2795 - get_f1: 1.0000\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2776 - get_f1: 1.0000\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.2758 - get_f1: 1.0000\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2740 - get_f1: 1.0000\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2723 - get_f1: 1.0000\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2705 - get_f1: 1.0000\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2688 - get_f1: 1.0000\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2671 - get_f1: 1.0000\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2654 - get_f1: 1.0000\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2637 - get_f1: 1.0000\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.2620 - get_f1: 1.0000\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2604 - get_f1: 1.0000\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2587 - get_f1: 1.0000\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2572 - get_f1: 1.0000\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2556 - get_f1: 1.0000\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2540 - get_f1: 1.0000\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2525 - get_f1: 1.0000\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2509 - get_f1: 1.0000\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2494 - get_f1: 1.0000\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2479 - get_f1: 1.0000\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2464 - get_f1: 1.0000\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.2450 - get_f1: 1.0000\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2436 - get_f1: 1.0000\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2422 - get_f1: 1.0000\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2407 - get_f1: 1.0000\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2394 - get_f1: 1.0000\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2380 - get_f1: 1.0000\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2366 - get_f1: 1.0000\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2353 - get_f1: 1.0000\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2340 - get_f1: 1.0000\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2327 - get_f1: 1.0000\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2314 - get_f1: 1.0000\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2301 - get_f1: 1.0000\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2289 - get_f1: 1.0000\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2276 - get_f1: 1.0000\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2264 - get_f1: 1.0000\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2252 - get_f1: 1.0000\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2240 - get_f1: 1.0000\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2228 - get_f1: 1.0000\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2216 - get_f1: 1.0000\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2204 - get_f1: 1.0000\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2193 - get_f1: 1.0000\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2182 - get_f1: 1.0000\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2170 - get_f1: 1.0000\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2159 - get_f1: 1.0000\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2148 - get_f1: 1.0000\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2138 - get_f1: 1.0000\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2127 - get_f1: 1.0000\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2116 - get_f1: 1.0000\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2106 - get_f1: 1.0000\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.2096 - get_f1: 1.0000\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2086 - get_f1: 1.0000\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2076 - get_f1: 1.0000\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2066 - get_f1: 1.0000\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2056 - get_f1: 1.0000\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2046 - get_f1: 1.0000\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2036 - get_f1: 1.0000\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2026 - get_f1: 1.0000\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2017 - get_f1: 1.0000\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2007 - get_f1: 1.0000\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1998 - get_f1: 1.0000\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1989 - get_f1: 1.0000\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1979 - get_f1: 1.0000\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1970 - get_f1: 1.0000\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1961 - get_f1: 1.0000\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1952 - get_f1: 1.0000\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1943 - get_f1: 1.0000\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1934 - get_f1: 1.0000\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1926 - get_f1: 1.0000\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1917 - get_f1: 1.0000\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1908 - get_f1: 1.0000\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1900 - get_f1: 1.0000\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1891 - get_f1: 1.0000\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1883 - get_f1: 1.0000\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1875 - get_f1: 1.0000\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1867 - get_f1: 1.0000\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1859 - get_f1: 1.0000\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1851 - get_f1: 1.0000\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1843 - get_f1: 1.0000\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1835 - get_f1: 1.0000\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1827 - get_f1: 1.0000\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1819 - get_f1: 1.0000\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1812 - get_f1: 1.0000\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1804 - get_f1: 1.0000\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1796 - get_f1: 1.0000\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1789 - get_f1: 1.0000\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1782 - get_f1: 1.0000\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1774 - get_f1: 1.0000\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1767 - get_f1: 1.0000\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1760 - get_f1: 1.0000\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1753 - get_f1: 1.0000\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1746 - get_f1: 1.0000\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1739 - get_f1: 1.0000\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1733 - get_f1: 1.0000\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1726 - get_f1: 1.0000\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1719 - get_f1: 1.0000\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1712 - get_f1: 1.0000\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1706 - get_f1: 1.0000\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1699 - get_f1: 1.0000\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1693 - get_f1: 1.0000\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1687 - get_f1: 1.0000\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1680 - get_f1: 1.0000\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1674 - get_f1: 1.0000\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1668 - get_f1: 1.0000\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1662 - get_f1: 1.0000\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1656 - get_f1: 1.0000\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1650 - get_f1: 1.0000\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1644 - get_f1: 1.0000\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1638 - get_f1: 1.0000\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1632 - get_f1: 1.0000\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1626 - get_f1: 1.0000\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1620 - get_f1: 1.0000\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1614 - get_f1: 1.0000\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1609 - get_f1: 1.0000\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1603 - get_f1: 1.0000\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1598 - get_f1: 1.0000\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1592 - get_f1: 1.0000\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1587 - get_f1: 1.0000\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1581 - get_f1: 1.0000\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1576 - get_f1: 1.0000\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1570 - get_f1: 1.0000\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1565 - get_f1: 1.0000\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1559 - get_f1: 1.0000\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1554 - get_f1: 1.0000\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1549 - get_f1: 1.0000\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1544 - get_f1: 1.0000\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1538 - get_f1: 1.0000\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1533 - get_f1: 1.0000\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1528 - get_f1: 1.0000\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1523 - get_f1: 1.0000\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1518 - get_f1: 1.0000\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1513 - get_f1: 1.0000\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1508 - get_f1: 1.0000\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1503 - get_f1: 1.0000\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1498 - get_f1: 1.0000\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1494 - get_f1: 1.0000\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1489 - get_f1: 1.0000\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1484 - get_f1: 1.0000\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1480 - get_f1: 1.0000\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1475 - get_f1: 1.0000\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1470 - get_f1: 1.0000\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1466 - get_f1: 1.0000\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1461 - get_f1: 1.0000\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1457 - get_f1: 1.0000\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1452 - get_f1: 1.0000\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1447 - get_f1: 1.0000\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1443 - get_f1: 1.0000\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1439 - get_f1: 1.0000\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1434 - get_f1: 1.0000\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1430 - get_f1: 1.0000\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1426 - get_f1: 1.0000\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1421 - get_f1: 1.0000\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1417 - get_f1: 1.0000\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1413 - get_f1: 1.0000\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1409 - get_f1: 1.0000\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1404 - get_f1: 1.0000\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1400 - get_f1: 1.0000\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1396 - get_f1: 1.0000\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1392 - get_f1: 1.0000\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1388 - get_f1: 1.0000\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1384 - get_f1: 1.0000\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1380 - get_f1: 1.0000\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1376 - get_f1: 1.0000\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1372 - get_f1: 1.0000\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1368 - get_f1: 1.0000\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1364 - get_f1: 1.0000\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1360 - get_f1: 1.0000\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1356 - get_f1: 1.0000\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1352 - get_f1: 1.0000\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1348 - get_f1: 1.0000\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1345 - get_f1: 1.0000\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1341 - get_f1: 1.0000\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1337 - get_f1: 1.0000\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1333 - get_f1: 1.0000\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1330 - get_f1: 1.0000\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1326 - get_f1: 1.0000\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1322 - get_f1: 1.0000\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1319 - get_f1: 1.0000\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1315 - get_f1: 1.0000\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1311 - get_f1: 1.0000\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1308 - get_f1: 1.0000\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1304 - get_f1: 1.0000\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1300 - get_f1: 1.0000\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1297 - get_f1: 1.0000\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1293 - get_f1: 1.0000\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1290 - get_f1: 1.0000\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1286 - get_f1: 1.0000\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1283 - get_f1: 1.0000\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1279 - get_f1: 1.0000\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1276 - get_f1: 1.0000\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1272 - get_f1: 1.0000\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1269 - get_f1: 1.0000\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1265 - get_f1: 1.0000\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1262 - get_f1: 1.0000\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1259 - get_f1: 1.0000\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1255 - get_f1: 1.0000\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1252 - get_f1: 1.0000\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1249 - get_f1: 1.0000\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1246 - get_f1: 1.0000\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1242 - get_f1: 1.0000\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1239 - get_f1: 1.0000\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1236 - get_f1: 1.0000\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1233 - get_f1: 1.0000\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1230 - get_f1: 1.0000\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1227 - get_f1: 1.0000\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1224 - get_f1: 1.0000\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1221 - get_f1: 1.0000\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1218 - get_f1: 1.0000\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1215 - get_f1: 1.0000\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1212 - get_f1: 1.0000\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1209 - get_f1: 1.0000\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1206 - get_f1: 1.0000\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1203 - get_f1: 1.0000\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1200 - get_f1: 1.0000\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1197 - get_f1: 1.0000\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1194 - get_f1: 1.0000\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1191 - get_f1: 1.0000\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1188 - get_f1: 1.0000\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1185 - get_f1: 1.0000\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1182 - get_f1: 1.0000\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1179 - get_f1: 1.0000\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1177 - get_f1: 1.0000\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1174 - get_f1: 1.0000\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1171 - get_f1: 1.0000\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1168 - get_f1: 1.0000\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1165 - get_f1: 1.0000\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1162 - get_f1: 1.0000\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1160 - get_f1: 1.0000\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1157 - get_f1: 1.0000\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1154 - get_f1: 1.0000\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1151 - get_f1: 1.0000\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1149 - get_f1: 1.0000\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1146 - get_f1: 1.0000\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1143 - get_f1: 1.0000\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1140 - get_f1: 1.0000\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1138 - get_f1: 1.0000\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1135 - get_f1: 1.0000\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1132 - get_f1: 1.0000\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1130 - get_f1: 1.0000\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1127 - get_f1: 1.0000\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1124 - get_f1: 1.0000\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1122 - get_f1: 1.0000\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1119 - get_f1: 1.0000\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1116 - get_f1: 1.0000\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1114 - get_f1: 1.0000\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1111 - get_f1: 1.0000\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1109 - get_f1: 1.0000\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1106 - get_f1: 1.0000\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1104 - get_f1: 1.0000\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1101 - get_f1: 1.0000\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1098 - get_f1: 1.0000\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.1096 - get_f1: 1.0000\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1093 - get_f1: 1.0000\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1091 - get_f1: 1.0000\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1089 - get_f1: 1.0000\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1086 - get_f1: 1.0000\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1084 - get_f1: 1.0000\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1081 - get_f1: 1.0000\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1079 - get_f1: 1.0000\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1076 - get_f1: 1.0000\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1074 - get_f1: 1.0000\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1072 - get_f1: 1.0000\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1069 - get_f1: 1.0000\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1067 - get_f1: 1.0000\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1064 - get_f1: 1.0000\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1062 - get_f1: 1.0000\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1060 - get_f1: 1.0000\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1057 - get_f1: 1.0000\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1055 - get_f1: 1.0000\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1053 - get_f1: 1.0000\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1051 - get_f1: 1.0000\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1048 - get_f1: 1.0000\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1046 - get_f1: 1.0000\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1044 - get_f1: 1.0000\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1042 - get_f1: 1.0000\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1039 - get_f1: 1.0000\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1037 - get_f1: 1.0000\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1035 - get_f1: 1.0000\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1033 - get_f1: 1.0000\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1031 - get_f1: 1.0000\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1029 - get_f1: 1.0000\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1026 - get_f1: 1.0000\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1024 - get_f1: 1.0000\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1022 - get_f1: 1.0000\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1020 - get_f1: 1.0000\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1018 - get_f1: 1.0000\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1016 - get_f1: 1.0000\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1014 - get_f1: 1.0000\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1012 - get_f1: 1.0000\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1009 - get_f1: 1.0000\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1007 - get_f1: 1.0000\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1005 - get_f1: 1.0000\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1003 - get_f1: 1.0000\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1001 - get_f1: 1.0000\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0999 - get_f1: 1.0000\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0997 - get_f1: 1.0000\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0995 - get_f1: 1.0000\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0993 - get_f1: 1.0000\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0991 - get_f1: 1.0000\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0989 - get_f1: 1.0000\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0987 - get_f1: 1.0000\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0985 - get_f1: 1.0000\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0983 - get_f1: 1.0000\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0981 - get_f1: 1.0000\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0979 - get_f1: 1.0000\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0977 - get_f1: 1.0000\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0975 - get_f1: 1.0000\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0973 - get_f1: 1.0000\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0971 - get_f1: 1.0000\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.0969 - get_f1: 1.0000\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0967 - get_f1: 1.0000\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0966 - get_f1: 1.0000\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0964 - get_f1: 1.0000\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0962 - get_f1: 1.0000\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0960 - get_f1: 1.0000\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.0958 - get_f1: 1.0000\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0956 - get_f1: 1.0000\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0954 - get_f1: 1.0000\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0953 - get_f1: 1.0000\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0951 - get_f1: 1.0000\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0949 - get_f1: 1.0000\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0947 - get_f1: 1.0000\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0945 - get_f1: 1.0000\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.0943 - get_f1: 1.0000\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0942 - get_f1: 1.0000\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0940 - get_f1: 1.0000\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0938 - get_f1: 1.0000\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0936 - get_f1: 1.0000\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0934 - get_f1: 1.0000\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.0933 - get_f1: 1.0000\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0931 - get_f1: 1.0000\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0929 - get_f1: 1.0000\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.0927 - get_f1: 1.0000\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0926 - get_f1: 1.0000\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.0924 - get_f1: 1.0000\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0922 - get_f1: 1.0000\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0920 - get_f1: 1.0000\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0919 - get_f1: 1.0000\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0917 - get_f1: 1.0000\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0915 - get_f1: 1.0000\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0914 - get_f1: 1.0000\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0912 - get_f1: 1.0000\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0910 - get_f1: 1.0000\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.0908 - get_f1: 1.0000\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0907 - get_f1: 1.0000\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0905 - get_f1: 1.0000\n",
            "30/30 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a3c23aff-e07d-4863-b6eb-06f5b2d83bd9",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'get_f1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e52b983e-631e-4527-ba41-890bdf6abd0a",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}