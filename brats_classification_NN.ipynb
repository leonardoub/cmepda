{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brats_classification_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJfVuHcWdflbb7Uaa4GG9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/brats_classification_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkyGJ1ldXJ8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "791cccbd-c674-4707-ec63-2afeed8e33c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_without_HISTO_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "03a2dd93-670b-4b05-b79a-19151407c1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>INTENSITY_Mean_NET_FLAIR</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>227.7510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>209.7901</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>165.1014</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>207.5531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>139.3234</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>195.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>176.3511</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>152.2227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>161.5295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>163.4821</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows Ã— 587 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               ID        Date  VOLUME_ET  ...  TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0    TCGA-02-0006  1996.08.23       1662  ...     0.052741    2.00000        GBM\n",
              "1    TCGA-02-0009  1997.06.14       4362  ...     0.094336   91.47360        GBM\n",
              "2    TCGA-02-0011  1998.02.01      33404  ...     0.096035  272.42900        GBM\n",
              "3    TCGA-02-0027  1999.03.28      12114  ...     0.096470  128.46800        GBM\n",
              "4    TCGA-02-0033  1997.05.26      34538  ...     0.096894  240.77800        GBM\n",
              "..            ...         ...        ...  ...          ...        ...        ...\n",
              "141  TCGA-HT-7694  1995.04.04       1036  ...     0.090456  719.23800        LGG\n",
              "142  TCGA-HT-8018  1997.04.11       2093  ...     0.054307    2.00000        LGG\n",
              "143  TCGA-HT-8111  1998.03.30       1929  ...     0.126712    7.06744        LGG\n",
              "144  TCGA-HT-8114  1998.10.30       8755  ...     0.107433   15.52240        LGG\n",
              "145  TCGA-HT-8563  1998.12.09      11757  ...     0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 587 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "a6fd28d0-588f-4ac4-857b-9fccaf983290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Date', 'VOLUME_ET', 'VOLUME_NET', 'VOLUME_ED', 'VOLUME_TC',\n",
              "       'VOLUME_WT', 'VOLUME_BRAIN', 'VOLUME_ET_OVER_NET', 'VOLUME_ET_OVER_ED',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=587)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BoXqyfbpydt",
        "colab_type": "text"
      },
      "source": [
        "#NO K-FOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=0.2, stratify=y_train_big, random_state=3)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76RgJTpcGf3E",
        "colab_type": "code",
        "outputId": "6653c5a7-0d22-44d7-b4a5-b01dcbd2159a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 584)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 379
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6iyOqcBq0RC"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "ca818452-688b-4652-a56b-5fbbb7ad1166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = MinMaxScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(30,), kernel_regularizer=regularizers.l2(l=0.005)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f3a77c5f-8043-4361-cd6b-6a52feb8cfcc",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "51404ad2-dea1-4648-d986-1421c8e2ee9c",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=92)\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.8018 - accuracy: 0.6902 - val_loss: 0.8329 - val_accuracy: 0.6875\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.7987 - accuracy: 0.7011 - val_loss: 0.8296 - val_accuracy: 0.7083\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.7953 - accuracy: 0.7011 - val_loss: 0.8263 - val_accuracy: 0.7083\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.7919 - accuracy: 0.7011 - val_loss: 0.8230 - val_accuracy: 0.7083\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.7885 - accuracy: 0.7011 - val_loss: 0.8198 - val_accuracy: 0.7083\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7853 - accuracy: 0.6957 - val_loss: 0.8166 - val_accuracy: 0.7083\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.7822 - accuracy: 0.6957 - val_loss: 0.8137 - val_accuracy: 0.7083\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7793 - accuracy: 0.6957 - val_loss: 0.8109 - val_accuracy: 0.7083\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.7765 - accuracy: 0.6957 - val_loss: 0.8083 - val_accuracy: 0.7083\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.7739 - accuracy: 0.6957 - val_loss: 0.8059 - val_accuracy: 0.7083\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.7714 - accuracy: 0.6957 - val_loss: 0.8037 - val_accuracy: 0.7083\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.7690 - accuracy: 0.6957 - val_loss: 0.8015 - val_accuracy: 0.7083\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.7668 - accuracy: 0.6957 - val_loss: 0.7995 - val_accuracy: 0.7083\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.7647 - accuracy: 0.6957 - val_loss: 0.7976 - val_accuracy: 0.7083\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.7627 - accuracy: 0.6957 - val_loss: 0.7958 - val_accuracy: 0.7083\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.7607 - accuracy: 0.6957 - val_loss: 0.7941 - val_accuracy: 0.7083\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.7589 - accuracy: 0.6957 - val_loss: 0.7925 - val_accuracy: 0.7083\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.7571 - accuracy: 0.6957 - val_loss: 0.7910 - val_accuracy: 0.7083\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.7553 - accuracy: 0.6957 - val_loss: 0.7895 - val_accuracy: 0.7083\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.7536 - accuracy: 0.6957 - val_loss: 0.7881 - val_accuracy: 0.7083\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7520 - accuracy: 0.6957 - val_loss: 0.7868 - val_accuracy: 0.7083\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.7503 - accuracy: 0.6957 - val_loss: 0.7855 - val_accuracy: 0.7083\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.7487 - accuracy: 0.6957 - val_loss: 0.7842 - val_accuracy: 0.7083\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.7471 - accuracy: 0.6957 - val_loss: 0.7829 - val_accuracy: 0.7083\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.7454 - accuracy: 0.6957 - val_loss: 0.7816 - val_accuracy: 0.7083\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.7438 - accuracy: 0.6957 - val_loss: 0.7804 - val_accuracy: 0.7083\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.7422 - accuracy: 0.6957 - val_loss: 0.7791 - val_accuracy: 0.7083\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7407 - accuracy: 0.6957 - val_loss: 0.7780 - val_accuracy: 0.7083\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.7391 - accuracy: 0.6957 - val_loss: 0.7768 - val_accuracy: 0.7083\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.7375 - accuracy: 0.6957 - val_loss: 0.7756 - val_accuracy: 0.7083\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.7360 - accuracy: 0.6957 - val_loss: 0.7744 - val_accuracy: 0.7083\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.7345 - accuracy: 0.6957 - val_loss: 0.7732 - val_accuracy: 0.7083\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.7329 - accuracy: 0.6957 - val_loss: 0.7720 - val_accuracy: 0.7083\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.7314 - accuracy: 0.6957 - val_loss: 0.7708 - val_accuracy: 0.7083\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.7299 - accuracy: 0.6957 - val_loss: 0.7696 - val_accuracy: 0.7083\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.7284 - accuracy: 0.6957 - val_loss: 0.7684 - val_accuracy: 0.7083\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.7269 - accuracy: 0.6957 - val_loss: 0.7672 - val_accuracy: 0.7083\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.7254 - accuracy: 0.6957 - val_loss: 0.7660 - val_accuracy: 0.7083\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.7239 - accuracy: 0.6957 - val_loss: 0.7648 - val_accuracy: 0.7083\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.7224 - accuracy: 0.6957 - val_loss: 0.7636 - val_accuracy: 0.7083\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.7209 - accuracy: 0.6957 - val_loss: 0.7624 - val_accuracy: 0.7083\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.7194 - accuracy: 0.6957 - val_loss: 0.7613 - val_accuracy: 0.7083\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.7180 - accuracy: 0.6957 - val_loss: 0.7601 - val_accuracy: 0.7083\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.7165 - accuracy: 0.6957 - val_loss: 0.7590 - val_accuracy: 0.7083\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.7150 - accuracy: 0.6957 - val_loss: 0.7579 - val_accuracy: 0.7083\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7135 - accuracy: 0.6957 - val_loss: 0.7568 - val_accuracy: 0.7083\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.7121 - accuracy: 0.6957 - val_loss: 0.7556 - val_accuracy: 0.7083\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.7106 - accuracy: 0.6957 - val_loss: 0.7546 - val_accuracy: 0.7083\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.7091 - accuracy: 0.6957 - val_loss: 0.7536 - val_accuracy: 0.7083\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.7077 - accuracy: 0.6957 - val_loss: 0.7525 - val_accuracy: 0.7083\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7063 - accuracy: 0.6957 - val_loss: 0.7515 - val_accuracy: 0.7083\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7049 - accuracy: 0.6957 - val_loss: 0.7504 - val_accuracy: 0.7083\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.7035 - accuracy: 0.6957 - val_loss: 0.7494 - val_accuracy: 0.7083\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.7021 - accuracy: 0.6957 - val_loss: 0.7483 - val_accuracy: 0.7083\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7008 - accuracy: 0.6957 - val_loss: 0.7473 - val_accuracy: 0.7083\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.6994 - accuracy: 0.6957 - val_loss: 0.7463 - val_accuracy: 0.7083\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.6980 - accuracy: 0.6957 - val_loss: 0.7452 - val_accuracy: 0.7083\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.6966 - accuracy: 0.6957 - val_loss: 0.7442 - val_accuracy: 0.7083\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6953 - accuracy: 0.6957 - val_loss: 0.7432 - val_accuracy: 0.7083\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6939 - accuracy: 0.6957 - val_loss: 0.7422 - val_accuracy: 0.7083\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6925 - accuracy: 0.6957 - val_loss: 0.7413 - val_accuracy: 0.7083\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6911 - accuracy: 0.6957 - val_loss: 0.7404 - val_accuracy: 0.7083\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 86us/step - loss: 0.6898 - accuracy: 0.6957 - val_loss: 0.7395 - val_accuracy: 0.7083\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.6884 - accuracy: 0.6957 - val_loss: 0.7386 - val_accuracy: 0.7083\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6870 - accuracy: 0.6957 - val_loss: 0.7377 - val_accuracy: 0.7083\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.6856 - accuracy: 0.6957 - val_loss: 0.7368 - val_accuracy: 0.7083\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.6843 - accuracy: 0.6957 - val_loss: 0.7359 - val_accuracy: 0.7083\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.6829 - accuracy: 0.6957 - val_loss: 0.7350 - val_accuracy: 0.7083\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6816 - accuracy: 0.6957 - val_loss: 0.7341 - val_accuracy: 0.7083\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.6802 - accuracy: 0.6957 - val_loss: 0.7331 - val_accuracy: 0.7083\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.6789 - accuracy: 0.6957 - val_loss: 0.7322 - val_accuracy: 0.7083\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6775 - accuracy: 0.6957 - val_loss: 0.7312 - val_accuracy: 0.7083\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6762 - accuracy: 0.6957 - val_loss: 0.7302 - val_accuracy: 0.7083\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6748 - accuracy: 0.6957 - val_loss: 0.7292 - val_accuracy: 0.7083\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.6735 - accuracy: 0.6957 - val_loss: 0.7282 - val_accuracy: 0.7083\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6722 - accuracy: 0.6957 - val_loss: 0.7273 - val_accuracy: 0.7083\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.6709 - accuracy: 0.6957 - val_loss: 0.7264 - val_accuracy: 0.7083\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6696 - accuracy: 0.6957 - val_loss: 0.7256 - val_accuracy: 0.7083\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6683 - accuracy: 0.6957 - val_loss: 0.7248 - val_accuracy: 0.7083\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.6670 - accuracy: 0.6957 - val_loss: 0.7240 - val_accuracy: 0.7083\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6656 - accuracy: 0.6957 - val_loss: 0.7232 - val_accuracy: 0.7083\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.6643 - accuracy: 0.6957 - val_loss: 0.7225 - val_accuracy: 0.7083\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6630 - accuracy: 0.6957 - val_loss: 0.7217 - val_accuracy: 0.7083\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6617 - accuracy: 0.6957 - val_loss: 0.7210 - val_accuracy: 0.7083\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.6604 - accuracy: 0.6957 - val_loss: 0.7201 - val_accuracy: 0.7083\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6590 - accuracy: 0.6957 - val_loss: 0.7193 - val_accuracy: 0.7083\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.6577 - accuracy: 0.6957 - val_loss: 0.7185 - val_accuracy: 0.7083\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6564 - accuracy: 0.6957 - val_loss: 0.7176 - val_accuracy: 0.7083\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.6551 - accuracy: 0.6957 - val_loss: 0.7168 - val_accuracy: 0.7083\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.6537 - accuracy: 0.6957 - val_loss: 0.7160 - val_accuracy: 0.7083\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.6524 - accuracy: 0.6957 - val_loss: 0.7153 - val_accuracy: 0.7083\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6511 - accuracy: 0.6957 - val_loss: 0.7145 - val_accuracy: 0.7083\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.6498 - accuracy: 0.6957 - val_loss: 0.7138 - val_accuracy: 0.7083\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.6484 - accuracy: 0.6957 - val_loss: 0.7130 - val_accuracy: 0.7083\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6471 - accuracy: 0.6957 - val_loss: 0.7122 - val_accuracy: 0.7083\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.6458 - accuracy: 0.6957 - val_loss: 0.7114 - val_accuracy: 0.7083\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6445 - accuracy: 0.6957 - val_loss: 0.7106 - val_accuracy: 0.7083\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6432 - accuracy: 0.6957 - val_loss: 0.7099 - val_accuracy: 0.7083\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.6419 - accuracy: 0.6957 - val_loss: 0.7092 - val_accuracy: 0.7083\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6406 - accuracy: 0.6957 - val_loss: 0.7085 - val_accuracy: 0.7083\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6392 - accuracy: 0.6957 - val_loss: 0.7078 - val_accuracy: 0.7083\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6379 - accuracy: 0.6957 - val_loss: 0.7071 - val_accuracy: 0.7083\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6366 - accuracy: 0.6957 - val_loss: 0.7064 - val_accuracy: 0.7083\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.6353 - accuracy: 0.6957 - val_loss: 0.7056 - val_accuracy: 0.7083\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6340 - accuracy: 0.6957 - val_loss: 0.7048 - val_accuracy: 0.7083\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6327 - accuracy: 0.6957 - val_loss: 0.7040 - val_accuracy: 0.7083\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6313 - accuracy: 0.6957 - val_loss: 0.7031 - val_accuracy: 0.7083\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6300 - accuracy: 0.6957 - val_loss: 0.7023 - val_accuracy: 0.7083\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6287 - accuracy: 0.6957 - val_loss: 0.7016 - val_accuracy: 0.7083\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6274 - accuracy: 0.6957 - val_loss: 0.7008 - val_accuracy: 0.7083\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6261 - accuracy: 0.6957 - val_loss: 0.7001 - val_accuracy: 0.7083\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.6247 - accuracy: 0.6957 - val_loss: 0.6993 - val_accuracy: 0.7083\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6234 - accuracy: 0.6957 - val_loss: 0.6985 - val_accuracy: 0.7083\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.6221 - accuracy: 0.6957 - val_loss: 0.6976 - val_accuracy: 0.7083\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.6207 - accuracy: 0.6957 - val_loss: 0.6967 - val_accuracy: 0.7083\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.6193 - accuracy: 0.6957 - val_loss: 0.6958 - val_accuracy: 0.7083\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6180 - accuracy: 0.6957 - val_loss: 0.6948 - val_accuracy: 0.7083\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.6166 - accuracy: 0.6957 - val_loss: 0.6937 - val_accuracy: 0.7083\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6151 - accuracy: 0.6957 - val_loss: 0.6926 - val_accuracy: 0.7083\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6137 - accuracy: 0.6957 - val_loss: 0.6914 - val_accuracy: 0.7083\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6123 - accuracy: 0.6957 - val_loss: 0.6903 - val_accuracy: 0.7083\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.6108 - accuracy: 0.6957 - val_loss: 0.6892 - val_accuracy: 0.7083\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6094 - accuracy: 0.6957 - val_loss: 0.6881 - val_accuracy: 0.7083\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.6079 - accuracy: 0.6957 - val_loss: 0.6871 - val_accuracy: 0.7083\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6065 - accuracy: 0.6957 - val_loss: 0.6861 - val_accuracy: 0.7083\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.6051 - accuracy: 0.6957 - val_loss: 0.6852 - val_accuracy: 0.7083\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.6037 - accuracy: 0.6957 - val_loss: 0.6842 - val_accuracy: 0.7083\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.6023 - accuracy: 0.6957 - val_loss: 0.6834 - val_accuracy: 0.7083\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.6009 - accuracy: 0.6957 - val_loss: 0.6826 - val_accuracy: 0.7083\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5996 - accuracy: 0.6957 - val_loss: 0.6818 - val_accuracy: 0.7083\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5982 - accuracy: 0.6957 - val_loss: 0.6810 - val_accuracy: 0.7083\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5968 - accuracy: 0.6957 - val_loss: 0.6802 - val_accuracy: 0.7083\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5954 - accuracy: 0.6957 - val_loss: 0.6795 - val_accuracy: 0.7083\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5941 - accuracy: 0.6957 - val_loss: 0.6787 - val_accuracy: 0.7083\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5927 - accuracy: 0.6957 - val_loss: 0.6780 - val_accuracy: 0.7083\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5913 - accuracy: 0.6957 - val_loss: 0.6773 - val_accuracy: 0.7083\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.5899 - accuracy: 0.6957 - val_loss: 0.6766 - val_accuracy: 0.7083\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5886 - accuracy: 0.6957 - val_loss: 0.6760 - val_accuracy: 0.7083\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5872 - accuracy: 0.6957 - val_loss: 0.6754 - val_accuracy: 0.7083\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5858 - accuracy: 0.6957 - val_loss: 0.6748 - val_accuracy: 0.7083\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5844 - accuracy: 0.6957 - val_loss: 0.6743 - val_accuracy: 0.7083\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5830 - accuracy: 0.6957 - val_loss: 0.6738 - val_accuracy: 0.7083\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5817 - accuracy: 0.6957 - val_loss: 0.6732 - val_accuracy: 0.7083\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5803 - accuracy: 0.6957 - val_loss: 0.6727 - val_accuracy: 0.7083\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5789 - accuracy: 0.7065 - val_loss: 0.6722 - val_accuracy: 0.7083\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5775 - accuracy: 0.7065 - val_loss: 0.6718 - val_accuracy: 0.7083\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5761 - accuracy: 0.7120 - val_loss: 0.6713 - val_accuracy: 0.7083\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5748 - accuracy: 0.7120 - val_loss: 0.6708 - val_accuracy: 0.7083\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5734 - accuracy: 0.7120 - val_loss: 0.6704 - val_accuracy: 0.7083\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5720 - accuracy: 0.7120 - val_loss: 0.6699 - val_accuracy: 0.7083\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.5706 - accuracy: 0.7174 - val_loss: 0.6695 - val_accuracy: 0.7083\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.5693 - accuracy: 0.7174 - val_loss: 0.6690 - val_accuracy: 0.7083\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5679 - accuracy: 0.7174 - val_loss: 0.6685 - val_accuracy: 0.7083\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5665 - accuracy: 0.7174 - val_loss: 0.6681 - val_accuracy: 0.7083\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5652 - accuracy: 0.7228 - val_loss: 0.6676 - val_accuracy: 0.7083\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5638 - accuracy: 0.7228 - val_loss: 0.6671 - val_accuracy: 0.7083\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.5624 - accuracy: 0.7228 - val_loss: 0.6665 - val_accuracy: 0.7083\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5611 - accuracy: 0.7283 - val_loss: 0.6660 - val_accuracy: 0.7083\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5597 - accuracy: 0.7283 - val_loss: 0.6654 - val_accuracy: 0.7083\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.5584 - accuracy: 0.7283 - val_loss: 0.6648 - val_accuracy: 0.7083\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5570 - accuracy: 0.7283 - val_loss: 0.6642 - val_accuracy: 0.7083\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5556 - accuracy: 0.7391 - val_loss: 0.6636 - val_accuracy: 0.7083\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5543 - accuracy: 0.7446 - val_loss: 0.6630 - val_accuracy: 0.7083\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5529 - accuracy: 0.7500 - val_loss: 0.6624 - val_accuracy: 0.7083\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5516 - accuracy: 0.7500 - val_loss: 0.6618 - val_accuracy: 0.7083\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.5502 - accuracy: 0.7500 - val_loss: 0.6612 - val_accuracy: 0.7083\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5488 - accuracy: 0.7500 - val_loss: 0.6606 - val_accuracy: 0.7083\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5475 - accuracy: 0.7554 - val_loss: 0.6599 - val_accuracy: 0.7083\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5461 - accuracy: 0.7554 - val_loss: 0.6593 - val_accuracy: 0.7083\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5448 - accuracy: 0.7554 - val_loss: 0.6586 - val_accuracy: 0.7083\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5434 - accuracy: 0.7554 - val_loss: 0.6580 - val_accuracy: 0.7083\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5420 - accuracy: 0.7609 - val_loss: 0.6573 - val_accuracy: 0.7083\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5407 - accuracy: 0.7663 - val_loss: 0.6566 - val_accuracy: 0.7083\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5393 - accuracy: 0.7663 - val_loss: 0.6560 - val_accuracy: 0.7083\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5380 - accuracy: 0.7663 - val_loss: 0.6553 - val_accuracy: 0.7083\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5366 - accuracy: 0.7717 - val_loss: 0.6547 - val_accuracy: 0.7083\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5353 - accuracy: 0.7772 - val_loss: 0.6541 - val_accuracy: 0.7083\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5339 - accuracy: 0.7826 - val_loss: 0.6535 - val_accuracy: 0.7083\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5325 - accuracy: 0.7826 - val_loss: 0.6530 - val_accuracy: 0.7083\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5312 - accuracy: 0.7826 - val_loss: 0.6524 - val_accuracy: 0.7083\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5298 - accuracy: 0.7826 - val_loss: 0.6518 - val_accuracy: 0.7083\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5285 - accuracy: 0.7826 - val_loss: 0.6512 - val_accuracy: 0.7083\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5272 - accuracy: 0.7826 - val_loss: 0.6506 - val_accuracy: 0.7083\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5258 - accuracy: 0.7826 - val_loss: 0.6501 - val_accuracy: 0.7083\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5245 - accuracy: 0.7772 - val_loss: 0.6495 - val_accuracy: 0.7083\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5231 - accuracy: 0.7772 - val_loss: 0.6490 - val_accuracy: 0.7083\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5218 - accuracy: 0.7772 - val_loss: 0.6485 - val_accuracy: 0.7083\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5205 - accuracy: 0.7826 - val_loss: 0.6479 - val_accuracy: 0.7083\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5191 - accuracy: 0.7935 - val_loss: 0.6474 - val_accuracy: 0.7083\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5178 - accuracy: 0.8043 - val_loss: 0.6469 - val_accuracy: 0.7083\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.5165 - accuracy: 0.8043 - val_loss: 0.6464 - val_accuracy: 0.7083\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.5151 - accuracy: 0.8043 - val_loss: 0.6458 - val_accuracy: 0.7083\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.5138 - accuracy: 0.8043 - val_loss: 0.6453 - val_accuracy: 0.7083\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.5125 - accuracy: 0.8043 - val_loss: 0.6448 - val_accuracy: 0.7083\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5112 - accuracy: 0.8043 - val_loss: 0.6443 - val_accuracy: 0.7083\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5099 - accuracy: 0.8043 - val_loss: 0.6438 - val_accuracy: 0.7083\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5086 - accuracy: 0.8043 - val_loss: 0.6433 - val_accuracy: 0.7292\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5073 - accuracy: 0.8043 - val_loss: 0.6428 - val_accuracy: 0.7292\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5060 - accuracy: 0.8098 - val_loss: 0.6423 - val_accuracy: 0.7292\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5047 - accuracy: 0.8098 - val_loss: 0.6418 - val_accuracy: 0.7292\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.5034 - accuracy: 0.8098 - val_loss: 0.6413 - val_accuracy: 0.7292\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5021 - accuracy: 0.8098 - val_loss: 0.6408 - val_accuracy: 0.7500\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5008 - accuracy: 0.8098 - val_loss: 0.6402 - val_accuracy: 0.7500\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4996 - accuracy: 0.8098 - val_loss: 0.6397 - val_accuracy: 0.7500\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4983 - accuracy: 0.8152 - val_loss: 0.6392 - val_accuracy: 0.7500\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4970 - accuracy: 0.8152 - val_loss: 0.6387 - val_accuracy: 0.7500\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4958 - accuracy: 0.8152 - val_loss: 0.6382 - val_accuracy: 0.7500\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4945 - accuracy: 0.8152 - val_loss: 0.6377 - val_accuracy: 0.7500\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.4932 - accuracy: 0.8152 - val_loss: 0.6371 - val_accuracy: 0.7500\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4920 - accuracy: 0.8152 - val_loss: 0.6366 - val_accuracy: 0.7500\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 170us/step - loss: 0.4907 - accuracy: 0.8152 - val_loss: 0.6361 - val_accuracy: 0.7500\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.4895 - accuracy: 0.8207 - val_loss: 0.6356 - val_accuracy: 0.7500\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4883 - accuracy: 0.8207 - val_loss: 0.6351 - val_accuracy: 0.7500\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4870 - accuracy: 0.8207 - val_loss: 0.6346 - val_accuracy: 0.7500\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4858 - accuracy: 0.8207 - val_loss: 0.6341 - val_accuracy: 0.7500\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4846 - accuracy: 0.8207 - val_loss: 0.6335 - val_accuracy: 0.7500\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4834 - accuracy: 0.8261 - val_loss: 0.6330 - val_accuracy: 0.7500\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4822 - accuracy: 0.8261 - val_loss: 0.6325 - val_accuracy: 0.7500\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4809 - accuracy: 0.8261 - val_loss: 0.6320 - val_accuracy: 0.7500\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4797 - accuracy: 0.8261 - val_loss: 0.6314 - val_accuracy: 0.7500\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4785 - accuracy: 0.8261 - val_loss: 0.6310 - val_accuracy: 0.7500\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4773 - accuracy: 0.8315 - val_loss: 0.6305 - val_accuracy: 0.7500\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.4761 - accuracy: 0.8315 - val_loss: 0.6300 - val_accuracy: 0.7500\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4750 - accuracy: 0.8315 - val_loss: 0.6295 - val_accuracy: 0.7500\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4738 - accuracy: 0.8315 - val_loss: 0.6290 - val_accuracy: 0.7500\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4726 - accuracy: 0.8315 - val_loss: 0.6285 - val_accuracy: 0.7500\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.4714 - accuracy: 0.8370 - val_loss: 0.6279 - val_accuracy: 0.7500\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4702 - accuracy: 0.8370 - val_loss: 0.6274 - val_accuracy: 0.7500\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4690 - accuracy: 0.8370 - val_loss: 0.6268 - val_accuracy: 0.7500\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4679 - accuracy: 0.8370 - val_loss: 0.6263 - val_accuracy: 0.7500\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4667 - accuracy: 0.8370 - val_loss: 0.6258 - val_accuracy: 0.7500\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4656 - accuracy: 0.8370 - val_loss: 0.6253 - val_accuracy: 0.7500\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4644 - accuracy: 0.8370 - val_loss: 0.6247 - val_accuracy: 0.7500\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4633 - accuracy: 0.8370 - val_loss: 0.6242 - val_accuracy: 0.7500\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4621 - accuracy: 0.8370 - val_loss: 0.6237 - val_accuracy: 0.7500\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4610 - accuracy: 0.8370 - val_loss: 0.6233 - val_accuracy: 0.7500\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4598 - accuracy: 0.8370 - val_loss: 0.6228 - val_accuracy: 0.7500\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4587 - accuracy: 0.8370 - val_loss: 0.6223 - val_accuracy: 0.7500\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4576 - accuracy: 0.8370 - val_loss: 0.6218 - val_accuracy: 0.7500\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4564 - accuracy: 0.8370 - val_loss: 0.6213 - val_accuracy: 0.7500\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4553 - accuracy: 0.8370 - val_loss: 0.6208 - val_accuracy: 0.7500\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4542 - accuracy: 0.8370 - val_loss: 0.6203 - val_accuracy: 0.7500\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4531 - accuracy: 0.8370 - val_loss: 0.6198 - val_accuracy: 0.7500\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4520 - accuracy: 0.8370 - val_loss: 0.6193 - val_accuracy: 0.7500\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4509 - accuracy: 0.8370 - val_loss: 0.6188 - val_accuracy: 0.7500\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4498 - accuracy: 0.8370 - val_loss: 0.6184 - val_accuracy: 0.7500\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4487 - accuracy: 0.8424 - val_loss: 0.6179 - val_accuracy: 0.7500\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4477 - accuracy: 0.8424 - val_loss: 0.6173 - val_accuracy: 0.7500\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4466 - accuracy: 0.8424 - val_loss: 0.6168 - val_accuracy: 0.7500\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4455 - accuracy: 0.8424 - val_loss: 0.6163 - val_accuracy: 0.7500\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4444 - accuracy: 0.8424 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4434 - accuracy: 0.8424 - val_loss: 0.6153 - val_accuracy: 0.7500\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4423 - accuracy: 0.8424 - val_loss: 0.6148 - val_accuracy: 0.7500\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4413 - accuracy: 0.8424 - val_loss: 0.6143 - val_accuracy: 0.7500\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4402 - accuracy: 0.8424 - val_loss: 0.6138 - val_accuracy: 0.7500\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4392 - accuracy: 0.8424 - val_loss: 0.6133 - val_accuracy: 0.7500\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4381 - accuracy: 0.8424 - val_loss: 0.6128 - val_accuracy: 0.7500\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4371 - accuracy: 0.8478 - val_loss: 0.6122 - val_accuracy: 0.7500\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4361 - accuracy: 0.8478 - val_loss: 0.6117 - val_accuracy: 0.7500\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4351 - accuracy: 0.8478 - val_loss: 0.6112 - val_accuracy: 0.7500\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.4340 - accuracy: 0.8478 - val_loss: 0.6107 - val_accuracy: 0.7500\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4330 - accuracy: 0.8587 - val_loss: 0.6102 - val_accuracy: 0.7500\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4320 - accuracy: 0.8587 - val_loss: 0.6097 - val_accuracy: 0.7500\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4310 - accuracy: 0.8641 - val_loss: 0.6091 - val_accuracy: 0.7500\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4300 - accuracy: 0.8641 - val_loss: 0.6086 - val_accuracy: 0.7500\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4290 - accuracy: 0.8641 - val_loss: 0.6081 - val_accuracy: 0.7500\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.4280 - accuracy: 0.8641 - val_loss: 0.6076 - val_accuracy: 0.7500\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4270 - accuracy: 0.8641 - val_loss: 0.6072 - val_accuracy: 0.7500\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.4260 - accuracy: 0.8641 - val_loss: 0.6067 - val_accuracy: 0.7500\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4250 - accuracy: 0.8641 - val_loss: 0.6062 - val_accuracy: 0.7500\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4240 - accuracy: 0.8641 - val_loss: 0.6057 - val_accuracy: 0.7500\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4231 - accuracy: 0.8641 - val_loss: 0.6053 - val_accuracy: 0.7500\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4221 - accuracy: 0.8696 - val_loss: 0.6049 - val_accuracy: 0.7500\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4211 - accuracy: 0.8696 - val_loss: 0.6045 - val_accuracy: 0.7500\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4202 - accuracy: 0.8696 - val_loss: 0.6041 - val_accuracy: 0.7500\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4192 - accuracy: 0.8696 - val_loss: 0.6036 - val_accuracy: 0.7500\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4183 - accuracy: 0.8696 - val_loss: 0.6032 - val_accuracy: 0.7500\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4173 - accuracy: 0.8750 - val_loss: 0.6027 - val_accuracy: 0.7500\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4164 - accuracy: 0.8750 - val_loss: 0.6023 - val_accuracy: 0.7500\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4154 - accuracy: 0.8750 - val_loss: 0.6018 - val_accuracy: 0.7500\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4145 - accuracy: 0.8750 - val_loss: 0.6013 - val_accuracy: 0.7500\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4136 - accuracy: 0.8804 - val_loss: 0.6008 - val_accuracy: 0.7500\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4126 - accuracy: 0.8804 - val_loss: 0.6003 - val_accuracy: 0.7500\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4117 - accuracy: 0.8804 - val_loss: 0.5997 - val_accuracy: 0.7500\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4108 - accuracy: 0.8804 - val_loss: 0.5993 - val_accuracy: 0.7500\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4099 - accuracy: 0.8804 - val_loss: 0.5987 - val_accuracy: 0.7500\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4090 - accuracy: 0.8804 - val_loss: 0.5982 - val_accuracy: 0.7500\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4081 - accuracy: 0.8804 - val_loss: 0.5977 - val_accuracy: 0.7500\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4072 - accuracy: 0.8804 - val_loss: 0.5973 - val_accuracy: 0.7500\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4063 - accuracy: 0.8804 - val_loss: 0.5968 - val_accuracy: 0.7500\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4054 - accuracy: 0.8804 - val_loss: 0.5962 - val_accuracy: 0.7500\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4045 - accuracy: 0.8804 - val_loss: 0.5957 - val_accuracy: 0.7500\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4036 - accuracy: 0.8859 - val_loss: 0.5953 - val_accuracy: 0.7500\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4027 - accuracy: 0.8859 - val_loss: 0.5949 - val_accuracy: 0.7500\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4019 - accuracy: 0.8859 - val_loss: 0.5945 - val_accuracy: 0.7500\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4010 - accuracy: 0.8859 - val_loss: 0.5941 - val_accuracy: 0.7500\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4001 - accuracy: 0.8859 - val_loss: 0.5935 - val_accuracy: 0.7500\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3993 - accuracy: 0.8859 - val_loss: 0.5929 - val_accuracy: 0.7500\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3984 - accuracy: 0.8859 - val_loss: 0.5923 - val_accuracy: 0.7500\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3975 - accuracy: 0.8859 - val_loss: 0.5918 - val_accuracy: 0.7500\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3967 - accuracy: 0.8859 - val_loss: 0.5913 - val_accuracy: 0.7500\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3958 - accuracy: 0.8859 - val_loss: 0.5908 - val_accuracy: 0.7500\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3950 - accuracy: 0.8859 - val_loss: 0.5902 - val_accuracy: 0.7500\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3941 - accuracy: 0.8859 - val_loss: 0.5895 - val_accuracy: 0.7500\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3933 - accuracy: 0.8967 - val_loss: 0.5889 - val_accuracy: 0.7500\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3924 - accuracy: 0.8967 - val_loss: 0.5882 - val_accuracy: 0.7500\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3915 - accuracy: 0.8967 - val_loss: 0.5876 - val_accuracy: 0.7500\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3905 - accuracy: 0.8967 - val_loss: 0.5869 - val_accuracy: 0.7500\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3896 - accuracy: 0.9022 - val_loss: 0.5864 - val_accuracy: 0.7500\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3886 - accuracy: 0.9022 - val_loss: 0.5859 - val_accuracy: 0.7500\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3878 - accuracy: 0.9022 - val_loss: 0.5854 - val_accuracy: 0.7500\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.3869 - accuracy: 0.9022 - val_loss: 0.5848 - val_accuracy: 0.7500\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3860 - accuracy: 0.9076 - val_loss: 0.5842 - val_accuracy: 0.7500\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.3852 - accuracy: 0.9130 - val_loss: 0.5835 - val_accuracy: 0.7500\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3843 - accuracy: 0.9130 - val_loss: 0.5829 - val_accuracy: 0.7500\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3834 - accuracy: 0.9130 - val_loss: 0.5823 - val_accuracy: 0.7500\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3826 - accuracy: 0.9130 - val_loss: 0.5819 - val_accuracy: 0.7500\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3817 - accuracy: 0.9130 - val_loss: 0.5815 - val_accuracy: 0.7500\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3809 - accuracy: 0.9130 - val_loss: 0.5810 - val_accuracy: 0.7500\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3800 - accuracy: 0.9130 - val_loss: 0.5805 - val_accuracy: 0.7500\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3792 - accuracy: 0.9130 - val_loss: 0.5799 - val_accuracy: 0.7500\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3783 - accuracy: 0.9185 - val_loss: 0.5793 - val_accuracy: 0.7500\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3775 - accuracy: 0.9185 - val_loss: 0.5788 - val_accuracy: 0.7500\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3766 - accuracy: 0.9185 - val_loss: 0.5783 - val_accuracy: 0.7500\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3758 - accuracy: 0.9185 - val_loss: 0.5778 - val_accuracy: 0.7500\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3749 - accuracy: 0.9185 - val_loss: 0.5773 - val_accuracy: 0.7500\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3741 - accuracy: 0.9185 - val_loss: 0.5768 - val_accuracy: 0.7500\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3733 - accuracy: 0.9185 - val_loss: 0.5764 - val_accuracy: 0.7500\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3724 - accuracy: 0.9185 - val_loss: 0.5759 - val_accuracy: 0.7500\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3716 - accuracy: 0.9185 - val_loss: 0.5754 - val_accuracy: 0.7500\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3708 - accuracy: 0.9185 - val_loss: 0.5749 - val_accuracy: 0.7500\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3700 - accuracy: 0.9185 - val_loss: 0.5746 - val_accuracy: 0.7500\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3691 - accuracy: 0.9185 - val_loss: 0.5741 - val_accuracy: 0.7500\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3683 - accuracy: 0.9185 - val_loss: 0.5737 - val_accuracy: 0.7500\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3675 - accuracy: 0.9185 - val_loss: 0.5732 - val_accuracy: 0.7500\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3667 - accuracy: 0.9185 - val_loss: 0.5728 - val_accuracy: 0.7500\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3659 - accuracy: 0.9185 - val_loss: 0.5722 - val_accuracy: 0.7500\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3651 - accuracy: 0.9185 - val_loss: 0.5718 - val_accuracy: 0.7500\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3643 - accuracy: 0.9185 - val_loss: 0.5713 - val_accuracy: 0.7500\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3635 - accuracy: 0.9185 - val_loss: 0.5708 - val_accuracy: 0.7500\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3628 - accuracy: 0.9239 - val_loss: 0.5704 - val_accuracy: 0.7500\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3620 - accuracy: 0.9239 - val_loss: 0.5699 - val_accuracy: 0.7500\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3612 - accuracy: 0.9239 - val_loss: 0.5694 - val_accuracy: 0.7500\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3604 - accuracy: 0.9239 - val_loss: 0.5689 - val_accuracy: 0.7500\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3596 - accuracy: 0.9293 - val_loss: 0.5683 - val_accuracy: 0.7500\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3588 - accuracy: 0.9293 - val_loss: 0.5679 - val_accuracy: 0.7500\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3580 - accuracy: 0.9293 - val_loss: 0.5674 - val_accuracy: 0.7500\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3572 - accuracy: 0.9293 - val_loss: 0.5669 - val_accuracy: 0.7500\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3564 - accuracy: 0.9293 - val_loss: 0.5663 - val_accuracy: 0.7500\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3557 - accuracy: 0.9293 - val_loss: 0.5658 - val_accuracy: 0.7500\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3549 - accuracy: 0.9293 - val_loss: 0.5653 - val_accuracy: 0.7500\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3541 - accuracy: 0.9293 - val_loss: 0.5649 - val_accuracy: 0.7500\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3534 - accuracy: 0.9293 - val_loss: 0.5645 - val_accuracy: 0.7500\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3526 - accuracy: 0.9293 - val_loss: 0.5641 - val_accuracy: 0.7500\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.3518 - accuracy: 0.9293 - val_loss: 0.5637 - val_accuracy: 0.7500\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 176us/step - loss: 0.3511 - accuracy: 0.9293 - val_loss: 0.5632 - val_accuracy: 0.7500\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3503 - accuracy: 0.9293 - val_loss: 0.5629 - val_accuracy: 0.7500\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3496 - accuracy: 0.9293 - val_loss: 0.5626 - val_accuracy: 0.7500\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3489 - accuracy: 0.9293 - val_loss: 0.5623 - val_accuracy: 0.7500\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3481 - accuracy: 0.9293 - val_loss: 0.5619 - val_accuracy: 0.7500\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3474 - accuracy: 0.9293 - val_loss: 0.5614 - val_accuracy: 0.7500\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3467 - accuracy: 0.9293 - val_loss: 0.5609 - val_accuracy: 0.7500\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3460 - accuracy: 0.9293 - val_loss: 0.5605 - val_accuracy: 0.7500\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3452 - accuracy: 0.9293 - val_loss: 0.5601 - val_accuracy: 0.7500\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3445 - accuracy: 0.9293 - val_loss: 0.5598 - val_accuracy: 0.7500\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3438 - accuracy: 0.9348 - val_loss: 0.5595 - val_accuracy: 0.7500\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3431 - accuracy: 0.9348 - val_loss: 0.5593 - val_accuracy: 0.7500\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3424 - accuracy: 0.9348 - val_loss: 0.5589 - val_accuracy: 0.7500\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3417 - accuracy: 0.9348 - val_loss: 0.5586 - val_accuracy: 0.7500\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3410 - accuracy: 0.9348 - val_loss: 0.5583 - val_accuracy: 0.7500\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3403 - accuracy: 0.9348 - val_loss: 0.5582 - val_accuracy: 0.7500\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3397 - accuracy: 0.9348 - val_loss: 0.5579 - val_accuracy: 0.7500\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3390 - accuracy: 0.9348 - val_loss: 0.5575 - val_accuracy: 0.7500\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3383 - accuracy: 0.9348 - val_loss: 0.5572 - val_accuracy: 0.7500\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3376 - accuracy: 0.9348 - val_loss: 0.5569 - val_accuracy: 0.7500\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3370 - accuracy: 0.9348 - val_loss: 0.5566 - val_accuracy: 0.7500\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3363 - accuracy: 0.9348 - val_loss: 0.5562 - val_accuracy: 0.7500\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3356 - accuracy: 0.9348 - val_loss: 0.5559 - val_accuracy: 0.7500\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3350 - accuracy: 0.9348 - val_loss: 0.5556 - val_accuracy: 0.7500\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3343 - accuracy: 0.9348 - val_loss: 0.5554 - val_accuracy: 0.7500\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.3337 - accuracy: 0.9348 - val_loss: 0.5553 - val_accuracy: 0.7500\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3330 - accuracy: 0.9348 - val_loss: 0.5550 - val_accuracy: 0.7500\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3324 - accuracy: 0.9348 - val_loss: 0.5546 - val_accuracy: 0.7500\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.3318 - accuracy: 0.9348 - val_loss: 0.5542 - val_accuracy: 0.7500\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.3311 - accuracy: 0.9348 - val_loss: 0.5539 - val_accuracy: 0.7500\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.3305 - accuracy: 0.9348 - val_loss: 0.5536 - val_accuracy: 0.7500\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.3299 - accuracy: 0.9348 - val_loss: 0.5535 - val_accuracy: 0.7500\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.3292 - accuracy: 0.9402 - val_loss: 0.5534 - val_accuracy: 0.7500\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 158us/step - loss: 0.3286 - accuracy: 0.9402 - val_loss: 0.5531 - val_accuracy: 0.7500\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.3280 - accuracy: 0.9402 - val_loss: 0.5527 - val_accuracy: 0.7500\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.3274 - accuracy: 0.9402 - val_loss: 0.5523 - val_accuracy: 0.7500\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.3268 - accuracy: 0.9402 - val_loss: 0.5521 - val_accuracy: 0.7500\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3262 - accuracy: 0.9402 - val_loss: 0.5519 - val_accuracy: 0.7500\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3256 - accuracy: 0.9402 - val_loss: 0.5517 - val_accuracy: 0.7500\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3250 - accuracy: 0.9457 - val_loss: 0.5515 - val_accuracy: 0.7500\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3244 - accuracy: 0.9457 - val_loss: 0.5514 - val_accuracy: 0.7500\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3238 - accuracy: 0.9457 - val_loss: 0.5511 - val_accuracy: 0.7500\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3232 - accuracy: 0.9457 - val_loss: 0.5507 - val_accuracy: 0.7500\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3226 - accuracy: 0.9511 - val_loss: 0.5503 - val_accuracy: 0.7500\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3221 - accuracy: 0.9511 - val_loss: 0.5502 - val_accuracy: 0.7500\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3215 - accuracy: 0.9511 - val_loss: 0.5500 - val_accuracy: 0.7500\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3209 - accuracy: 0.9511 - val_loss: 0.5499 - val_accuracy: 0.7500\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3203 - accuracy: 0.9511 - val_loss: 0.5496 - val_accuracy: 0.7500\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3197 - accuracy: 0.9511 - val_loss: 0.5492 - val_accuracy: 0.7500\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3192 - accuracy: 0.9511 - val_loss: 0.5490 - val_accuracy: 0.7500\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3186 - accuracy: 0.9511 - val_loss: 0.5487 - val_accuracy: 0.7500\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.3181 - accuracy: 0.9511 - val_loss: 0.5484 - val_accuracy: 0.7708\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3175 - accuracy: 0.9511 - val_loss: 0.5481 - val_accuracy: 0.7708\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3170 - accuracy: 0.9511 - val_loss: 0.5479 - val_accuracy: 0.7708\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3164 - accuracy: 0.9511 - val_loss: 0.5477 - val_accuracy: 0.7708\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3159 - accuracy: 0.9511 - val_loss: 0.5475 - val_accuracy: 0.7708\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3154 - accuracy: 0.9511 - val_loss: 0.5472 - val_accuracy: 0.7708\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3148 - accuracy: 0.9511 - val_loss: 0.5470 - val_accuracy: 0.7708\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3143 - accuracy: 0.9511 - val_loss: 0.5468 - val_accuracy: 0.7708\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3138 - accuracy: 0.9511 - val_loss: 0.5467 - val_accuracy: 0.7708\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3132 - accuracy: 0.9511 - val_loss: 0.5465 - val_accuracy: 0.7708\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.3127 - accuracy: 0.9511 - val_loss: 0.5461 - val_accuracy: 0.7708\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3122 - accuracy: 0.9511 - val_loss: 0.5459 - val_accuracy: 0.7708\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3117 - accuracy: 0.9511 - val_loss: 0.5457 - val_accuracy: 0.7708\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3112 - accuracy: 0.9511 - val_loss: 0.5455 - val_accuracy: 0.7917\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3107 - accuracy: 0.9511 - val_loss: 0.5453 - val_accuracy: 0.7917\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.3101 - accuracy: 0.9511 - val_loss: 0.5452 - val_accuracy: 0.7917\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3096 - accuracy: 0.9511 - val_loss: 0.5450 - val_accuracy: 0.7917\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3091 - accuracy: 0.9511 - val_loss: 0.5447 - val_accuracy: 0.7917\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3086 - accuracy: 0.9511 - val_loss: 0.5444 - val_accuracy: 0.7917\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3081 - accuracy: 0.9511 - val_loss: 0.5443 - val_accuracy: 0.7917\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3077 - accuracy: 0.9511 - val_loss: 0.5441 - val_accuracy: 0.7917\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3072 - accuracy: 0.9511 - val_loss: 0.5439 - val_accuracy: 0.7917\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3067 - accuracy: 0.9511 - val_loss: 0.5437 - val_accuracy: 0.7917\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3062 - accuracy: 0.9511 - val_loss: 0.5436 - val_accuracy: 0.7917\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3057 - accuracy: 0.9511 - val_loss: 0.5435 - val_accuracy: 0.7917\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.3052 - accuracy: 0.9511 - val_loss: 0.5432 - val_accuracy: 0.7917\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.3048 - accuracy: 0.9511 - val_loss: 0.5427 - val_accuracy: 0.7917\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3043 - accuracy: 0.9511 - val_loss: 0.5424 - val_accuracy: 0.7917\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3038 - accuracy: 0.9511 - val_loss: 0.5424 - val_accuracy: 0.7917\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3033 - accuracy: 0.9511 - val_loss: 0.5426 - val_accuracy: 0.7917\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3029 - accuracy: 0.9511 - val_loss: 0.5424 - val_accuracy: 0.7917\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3024 - accuracy: 0.9511 - val_loss: 0.5419 - val_accuracy: 0.7917\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.3020 - accuracy: 0.9511 - val_loss: 0.5414 - val_accuracy: 0.7917\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 143us/step - loss: 0.3015 - accuracy: 0.9565 - val_loss: 0.5413 - val_accuracy: 0.7917\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3010 - accuracy: 0.9565 - val_loss: 0.5415 - val_accuracy: 0.7917\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3006 - accuracy: 0.9565 - val_loss: 0.5415 - val_accuracy: 0.7917\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3002 - accuracy: 0.9565 - val_loss: 0.5411 - val_accuracy: 0.7917\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2997 - accuracy: 0.9565 - val_loss: 0.5406 - val_accuracy: 0.7917\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2993 - accuracy: 0.9565 - val_loss: 0.5404 - val_accuracy: 0.7917\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2988 - accuracy: 0.9620 - val_loss: 0.5405 - val_accuracy: 0.7917\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2984 - accuracy: 0.9620 - val_loss: 0.5407 - val_accuracy: 0.7917\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2980 - accuracy: 0.9565 - val_loss: 0.5407 - val_accuracy: 0.7917\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2975 - accuracy: 0.9620 - val_loss: 0.5402 - val_accuracy: 0.7917\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2971 - accuracy: 0.9620 - val_loss: 0.5398 - val_accuracy: 0.7917\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2967 - accuracy: 0.9620 - val_loss: 0.5396 - val_accuracy: 0.7917\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2962 - accuracy: 0.9620 - val_loss: 0.5398 - val_accuracy: 0.7917\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2958 - accuracy: 0.9620 - val_loss: 0.5400 - val_accuracy: 0.7917\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2954 - accuracy: 0.9620 - val_loss: 0.5398 - val_accuracy: 0.7917\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2950 - accuracy: 0.9620 - val_loss: 0.5395 - val_accuracy: 0.7917\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2945 - accuracy: 0.9620 - val_loss: 0.5392 - val_accuracy: 0.7917\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2941 - accuracy: 0.9674 - val_loss: 0.5391 - val_accuracy: 0.7917\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2937 - accuracy: 0.9674 - val_loss: 0.5392 - val_accuracy: 0.7917\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2933 - accuracy: 0.9674 - val_loss: 0.5391 - val_accuracy: 0.7917\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2929 - accuracy: 0.9674 - val_loss: 0.5388 - val_accuracy: 0.7917\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2925 - accuracy: 0.9674 - val_loss: 0.5385 - val_accuracy: 0.7917\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2921 - accuracy: 0.9674 - val_loss: 0.5387 - val_accuracy: 0.7917\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2917 - accuracy: 0.9674 - val_loss: 0.5389 - val_accuracy: 0.7917\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2913 - accuracy: 0.9674 - val_loss: 0.5388 - val_accuracy: 0.7917\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2909 - accuracy: 0.9674 - val_loss: 0.5384 - val_accuracy: 0.7917\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2905 - accuracy: 0.9674 - val_loss: 0.5380 - val_accuracy: 0.7917\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2901 - accuracy: 0.9674 - val_loss: 0.5380 - val_accuracy: 0.7917\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2897 - accuracy: 0.9674 - val_loss: 0.5382 - val_accuracy: 0.7917\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2893 - accuracy: 0.9674 - val_loss: 0.5380 - val_accuracy: 0.7917\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2890 - accuracy: 0.9674 - val_loss: 0.5376 - val_accuracy: 0.7917\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2885 - accuracy: 0.9674 - val_loss: 0.5372 - val_accuracy: 0.7917\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2882 - accuracy: 0.9674 - val_loss: 0.5373 - val_accuracy: 0.7917\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2878 - accuracy: 0.9674 - val_loss: 0.5375 - val_accuracy: 0.7917\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2874 - accuracy: 0.9674 - val_loss: 0.5374 - val_accuracy: 0.7917\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2870 - accuracy: 0.9674 - val_loss: 0.5371 - val_accuracy: 0.7917\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2866 - accuracy: 0.9674 - val_loss: 0.5371 - val_accuracy: 0.7917\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2862 - accuracy: 0.9674 - val_loss: 0.5371 - val_accuracy: 0.7917\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2858 - accuracy: 0.9674 - val_loss: 0.5370 - val_accuracy: 0.7917\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2855 - accuracy: 0.9674 - val_loss: 0.5368 - val_accuracy: 0.7917\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2851 - accuracy: 0.9674 - val_loss: 0.5367 - val_accuracy: 0.7917\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2847 - accuracy: 0.9674 - val_loss: 0.5369 - val_accuracy: 0.7917\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2843 - accuracy: 0.9674 - val_loss: 0.5370 - val_accuracy: 0.7917\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2840 - accuracy: 0.9674 - val_loss: 0.5368 - val_accuracy: 0.7917\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2836 - accuracy: 0.9674 - val_loss: 0.5365 - val_accuracy: 0.7917\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2832 - accuracy: 0.9674 - val_loss: 0.5366 - val_accuracy: 0.7917\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2829 - accuracy: 0.9674 - val_loss: 0.5366 - val_accuracy: 0.7917\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2825 - accuracy: 0.9674 - val_loss: 0.5365 - val_accuracy: 0.7917\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2821 - accuracy: 0.9674 - val_loss: 0.5364 - val_accuracy: 0.7917\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2818 - accuracy: 0.9674 - val_loss: 0.5365 - val_accuracy: 0.7917\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2814 - accuracy: 0.9674 - val_loss: 0.5366 - val_accuracy: 0.7917\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2811 - accuracy: 0.9674 - val_loss: 0.5364 - val_accuracy: 0.7917\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 86us/step - loss: 0.2807 - accuracy: 0.9674 - val_loss: 0.5362 - val_accuracy: 0.7917\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2804 - accuracy: 0.9674 - val_loss: 0.5362 - val_accuracy: 0.7917\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2800 - accuracy: 0.9674 - val_loss: 0.5362 - val_accuracy: 0.7917\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2797 - accuracy: 0.9674 - val_loss: 0.5360 - val_accuracy: 0.7917\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2793 - accuracy: 0.9728 - val_loss: 0.5361 - val_accuracy: 0.7917\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2790 - accuracy: 0.9728 - val_loss: 0.5361 - val_accuracy: 0.7917\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2786 - accuracy: 0.9728 - val_loss: 0.5361 - val_accuracy: 0.7917\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2783 - accuracy: 0.9728 - val_loss: 0.5361 - val_accuracy: 0.7917\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2779 - accuracy: 0.9783 - val_loss: 0.5357 - val_accuracy: 0.7917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "345cf18d-1d0b-4ec8-f661-075fe44406fe",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff8bfcc18d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 398
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7caae8c6-295d-4806-84a0-0eba3c8d0a4d",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff8c03284e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 399
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c785ae3d-a5d4-4051-9aaf-48eddbc4cb4d",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=92, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 978us/step - loss: 0.8604 - accuracy: 0.4348\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.8534 - accuracy: 0.4565\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.8466 - accuracy: 0.4891\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.8400 - accuracy: 0.5054\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.8337 - accuracy: 0.5326\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.8278 - accuracy: 0.5707\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.8221 - accuracy: 0.5924\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8165 - accuracy: 0.5978\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8112 - accuracy: 0.6250\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8060 - accuracy: 0.6359\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8010 - accuracy: 0.6902\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.7961 - accuracy: 0.6957\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7916 - accuracy: 0.6957\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.7873 - accuracy: 0.6902\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7832 - accuracy: 0.7065\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.7794 - accuracy: 0.7174\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.7757 - accuracy: 0.7065\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.7721 - accuracy: 0.7065\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7687 - accuracy: 0.7065\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.7656 - accuracy: 0.7120\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7626 - accuracy: 0.7065\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.7597 - accuracy: 0.7120\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.7570 - accuracy: 0.7120\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7545 - accuracy: 0.7120\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.7522 - accuracy: 0.7120\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.7499 - accuracy: 0.7065\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7478 - accuracy: 0.7065\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.7457 - accuracy: 0.7065\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7438 - accuracy: 0.7065\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.7419 - accuracy: 0.7065\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7401 - accuracy: 0.7065\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7384 - accuracy: 0.7065\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.7368 - accuracy: 0.7065\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.7352 - accuracy: 0.7065\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.7337 - accuracy: 0.7065\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7322 - accuracy: 0.7065\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7308 - accuracy: 0.7011\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.7293 - accuracy: 0.7011\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.7279 - accuracy: 0.7011\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7266 - accuracy: 0.7011\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7253 - accuracy: 0.7011\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7239 - accuracy: 0.6957\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7226 - accuracy: 0.6957\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7213 - accuracy: 0.7011\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.7200 - accuracy: 0.7011\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.7187 - accuracy: 0.7011\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7175 - accuracy: 0.7011\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7162 - accuracy: 0.7011\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.7149 - accuracy: 0.7011\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7136 - accuracy: 0.7011\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.7124 - accuracy: 0.7011\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7111 - accuracy: 0.7011\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.7099 - accuracy: 0.7011\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.7086 - accuracy: 0.7011\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.7073 - accuracy: 0.7011\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7061 - accuracy: 0.7011\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7048 - accuracy: 0.7011\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.7036 - accuracy: 0.7011\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7024 - accuracy: 0.7011\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7011 - accuracy: 0.7011\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6999 - accuracy: 0.7011\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6987 - accuracy: 0.7011\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6975 - accuracy: 0.7011\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6962 - accuracy: 0.7011\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6950 - accuracy: 0.7011\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6938 - accuracy: 0.7011\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6926 - accuracy: 0.7011\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6914 - accuracy: 0.7011\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6902 - accuracy: 0.7011\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6890 - accuracy: 0.7011\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6878 - accuracy: 0.7011\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6866 - accuracy: 0.7011\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.6855 - accuracy: 0.7011\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.6843 - accuracy: 0.7011\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6831 - accuracy: 0.7011\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6820 - accuracy: 0.7065\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6808 - accuracy: 0.7065\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6797 - accuracy: 0.7065\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6785 - accuracy: 0.7065\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 90us/step - loss: 0.6774 - accuracy: 0.7065\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6762 - accuracy: 0.7065\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6751 - accuracy: 0.7065\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6739 - accuracy: 0.7065\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6728 - accuracy: 0.7065\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6716 - accuracy: 0.7065\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6705 - accuracy: 0.7065\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6693 - accuracy: 0.7065\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.6682 - accuracy: 0.7065\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6671 - accuracy: 0.7065\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6659 - accuracy: 0.7065\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6648 - accuracy: 0.7065\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6636 - accuracy: 0.7065\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6625 - accuracy: 0.7065\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.6613 - accuracy: 0.7065\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.6602 - accuracy: 0.7065\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6591 - accuracy: 0.7120\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6579 - accuracy: 0.7120\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6568 - accuracy: 0.7120\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6557 - accuracy: 0.7120\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6546 - accuracy: 0.7120\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6534 - accuracy: 0.7120\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6523 - accuracy: 0.7120\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6512 - accuracy: 0.7120\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6501 - accuracy: 0.7120\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6490 - accuracy: 0.7120\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6478 - accuracy: 0.7120\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6467 - accuracy: 0.7120\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6456 - accuracy: 0.7120\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6445 - accuracy: 0.7120\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6433 - accuracy: 0.7120\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6422 - accuracy: 0.7120\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6410 - accuracy: 0.7120\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6398 - accuracy: 0.7120\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6385 - accuracy: 0.7120\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6372 - accuracy: 0.7120\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6360 - accuracy: 0.7120\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6348 - accuracy: 0.7120\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6337 - accuracy: 0.7120\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.6326 - accuracy: 0.7120\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.6316 - accuracy: 0.7120\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6304 - accuracy: 0.7120\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6292 - accuracy: 0.7120\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.6280 - accuracy: 0.7120\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6268 - accuracy: 0.7120\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6256 - accuracy: 0.7120\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.6244 - accuracy: 0.7120\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6233 - accuracy: 0.7120\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6222 - accuracy: 0.7120\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.6210 - accuracy: 0.7120\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6198 - accuracy: 0.7120\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6187 - accuracy: 0.7120\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6175 - accuracy: 0.7120\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6163 - accuracy: 0.7120\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.6151 - accuracy: 0.7120\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6139 - accuracy: 0.7120\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6128 - accuracy: 0.7120\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6116 - accuracy: 0.7120\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6104 - accuracy: 0.7120\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6093 - accuracy: 0.7120\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.6081 - accuracy: 0.7120\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6069 - accuracy: 0.7120\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6057 - accuracy: 0.7120\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.6046 - accuracy: 0.7120\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6034 - accuracy: 0.7120\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6022 - accuracy: 0.7120\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6010 - accuracy: 0.7120\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5998 - accuracy: 0.7120\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5986 - accuracy: 0.7120\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.5973 - accuracy: 0.7120\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.5961 - accuracy: 0.7120\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.5948 - accuracy: 0.7120\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5936 - accuracy: 0.7120\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5923 - accuracy: 0.7120\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5909 - accuracy: 0.7120\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.5896 - accuracy: 0.7120\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.5882 - accuracy: 0.7120\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5867 - accuracy: 0.7120\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5852 - accuracy: 0.7120\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.5836 - accuracy: 0.7120\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5821 - accuracy: 0.7120\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5807 - accuracy: 0.7120\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.5793 - accuracy: 0.7120\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.5779 - accuracy: 0.7120\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.5764 - accuracy: 0.7120\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5750 - accuracy: 0.7120\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5736 - accuracy: 0.7120\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5721 - accuracy: 0.7120\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5707 - accuracy: 0.7120\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.5693 - accuracy: 0.7120\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5679 - accuracy: 0.7120\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5665 - accuracy: 0.7120\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.5651 - accuracy: 0.7120\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.5637 - accuracy: 0.7120\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.5623 - accuracy: 0.7174\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5609 - accuracy: 0.7174\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5594 - accuracy: 0.7174\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.5580 - accuracy: 0.7174\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.5566 - accuracy: 0.7174\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.5552 - accuracy: 0.7174\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.5538 - accuracy: 0.7174\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.5524 - accuracy: 0.7174\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5509 - accuracy: 0.7174\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5495 - accuracy: 0.7174\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5481 - accuracy: 0.7174\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5467 - accuracy: 0.7174\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5453 - accuracy: 0.7174\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.5439 - accuracy: 0.7174\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5425 - accuracy: 0.7283\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5411 - accuracy: 0.7283\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.5397 - accuracy: 0.7337\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.5383 - accuracy: 0.7337\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5368 - accuracy: 0.7337\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5354 - accuracy: 0.7337\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5340 - accuracy: 0.7337\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5326 - accuracy: 0.7337\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5312 - accuracy: 0.7337\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5297 - accuracy: 0.7337\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.5283 - accuracy: 0.7337\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.5269 - accuracy: 0.7337\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.5255 - accuracy: 0.7337\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.5241 - accuracy: 0.7337\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5227 - accuracy: 0.7391\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.5213 - accuracy: 0.7391\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.5199 - accuracy: 0.7446\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5185 - accuracy: 0.7446\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5171 - accuracy: 0.7554\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.5157 - accuracy: 0.7500\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5143 - accuracy: 0.7500\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5129 - accuracy: 0.7500\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5115 - accuracy: 0.7554\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5101 - accuracy: 0.7717\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.5087 - accuracy: 0.7717\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.5073 - accuracy: 0.7717\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5059 - accuracy: 0.7717\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5045 - accuracy: 0.7717\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.5031 - accuracy: 0.7717\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5017 - accuracy: 0.7717\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5004 - accuracy: 0.7717\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4990 - accuracy: 0.7717\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.4976 - accuracy: 0.7717\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4962 - accuracy: 0.7717\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.4949 - accuracy: 0.7772\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4935 - accuracy: 0.7826\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4921 - accuracy: 0.7826\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4908 - accuracy: 0.7826\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4894 - accuracy: 0.7826\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4881 - accuracy: 0.7826\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4867 - accuracy: 0.7826\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4854 - accuracy: 0.7826\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4840 - accuracy: 0.7826\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4827 - accuracy: 0.7826\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.4813 - accuracy: 0.7880\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4799 - accuracy: 0.7989\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4785 - accuracy: 0.7989\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4770 - accuracy: 0.8043\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4755 - accuracy: 0.8098\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4740 - accuracy: 0.8152\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4726 - accuracy: 0.8152\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.4712 - accuracy: 0.8152\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4698 - accuracy: 0.8152\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4685 - accuracy: 0.8152\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4671 - accuracy: 0.8261\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.4658 - accuracy: 0.8261\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.4645 - accuracy: 0.8315\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4632 - accuracy: 0.8315\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.4619 - accuracy: 0.8315\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.4606 - accuracy: 0.8315\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.4593 - accuracy: 0.8315\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4580 - accuracy: 0.8315\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4567 - accuracy: 0.8315\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4554 - accuracy: 0.8315\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4541 - accuracy: 0.8370\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.4529 - accuracy: 0.8370\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4516 - accuracy: 0.8424\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4503 - accuracy: 0.8478\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4491 - accuracy: 0.8533\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4478 - accuracy: 0.8533\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4466 - accuracy: 0.8533\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4453 - accuracy: 0.8533\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4441 - accuracy: 0.8587\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.4428 - accuracy: 0.8587\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4416 - accuracy: 0.8587\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4403 - accuracy: 0.8641\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.4391 - accuracy: 0.8641\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4379 - accuracy: 0.8641\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4367 - accuracy: 0.8641\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4355 - accuracy: 0.8641\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.4343 - accuracy: 0.8641\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4331 - accuracy: 0.8641\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4319 - accuracy: 0.8641\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4307 - accuracy: 0.8696\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4295 - accuracy: 0.8696\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4283 - accuracy: 0.8696\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4271 - accuracy: 0.8750\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.4260 - accuracy: 0.8750\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4248 - accuracy: 0.8804\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4236 - accuracy: 0.8804\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.4225 - accuracy: 0.8804\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4213 - accuracy: 0.8804\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.4202 - accuracy: 0.8804\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.4191 - accuracy: 0.8804\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4179 - accuracy: 0.8804\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.4168 - accuracy: 0.8804\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.4157 - accuracy: 0.8859\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4146 - accuracy: 0.8859\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4135 - accuracy: 0.8859\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4124 - accuracy: 0.8859\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4113 - accuracy: 0.8859\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4102 - accuracy: 0.8859\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4091 - accuracy: 0.8859\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4081 - accuracy: 0.8859\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4070 - accuracy: 0.8859\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.4060 - accuracy: 0.8859\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4049 - accuracy: 0.8859\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.4039 - accuracy: 0.8859\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.4028 - accuracy: 0.8859\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.4018 - accuracy: 0.8859\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4008 - accuracy: 0.8859\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3997 - accuracy: 0.8859\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3987 - accuracy: 0.8913\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3977 - accuracy: 0.8967\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3967 - accuracy: 0.9022\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3957 - accuracy: 0.9022\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3947 - accuracy: 0.9022\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3937 - accuracy: 0.9022\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3928 - accuracy: 0.9022\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3918 - accuracy: 0.9022\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3908 - accuracy: 0.9022\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3899 - accuracy: 0.9022\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3889 - accuracy: 0.9022\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3880 - accuracy: 0.9022\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3870 - accuracy: 0.9022\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3861 - accuracy: 0.9022\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3852 - accuracy: 0.9022\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3843 - accuracy: 0.9022\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3833 - accuracy: 0.9022\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3824 - accuracy: 0.9022\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3815 - accuracy: 0.9022\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3806 - accuracy: 0.9022\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3797 - accuracy: 0.9022\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3788 - accuracy: 0.9130\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3779 - accuracy: 0.9130\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3771 - accuracy: 0.9130\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3762 - accuracy: 0.9130\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3753 - accuracy: 0.9130\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3745 - accuracy: 0.9130\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3736 - accuracy: 0.9130\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3728 - accuracy: 0.9130\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3719 - accuracy: 0.9185\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3711 - accuracy: 0.9185\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.3702 - accuracy: 0.9185\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3694 - accuracy: 0.9185\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3686 - accuracy: 0.9185\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3678 - accuracy: 0.9185\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3670 - accuracy: 0.9185\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3662 - accuracy: 0.9239\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3654 - accuracy: 0.9239\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3646 - accuracy: 0.9239\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3638 - accuracy: 0.9239\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3630 - accuracy: 0.9239\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3622 - accuracy: 0.9293\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3614 - accuracy: 0.9293\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3606 - accuracy: 0.9293\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3598 - accuracy: 0.9293\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3591 - accuracy: 0.9293\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3583 - accuracy: 0.9293\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3575 - accuracy: 0.9293\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3568 - accuracy: 0.9293\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3560 - accuracy: 0.9293\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3553 - accuracy: 0.9293\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3545 - accuracy: 0.9293\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3538 - accuracy: 0.9293\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3531 - accuracy: 0.9293\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3523 - accuracy: 0.9293\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3516 - accuracy: 0.9293\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3509 - accuracy: 0.9293\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3502 - accuracy: 0.9293\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3495 - accuracy: 0.9293\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3487 - accuracy: 0.9293\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3480 - accuracy: 0.9348\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3473 - accuracy: 0.9348\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3467 - accuracy: 0.9348\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3460 - accuracy: 0.9348\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3453 - accuracy: 0.9348\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3446 - accuracy: 0.9348\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3439 - accuracy: 0.9348\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3432 - accuracy: 0.9348\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3426 - accuracy: 0.9348\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3419 - accuracy: 0.9348\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3412 - accuracy: 0.9348\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3406 - accuracy: 0.9348\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3399 - accuracy: 0.9348\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3393 - accuracy: 0.9348\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3386 - accuracy: 0.9348\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3380 - accuracy: 0.9348\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3373 - accuracy: 0.9348\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3367 - accuracy: 0.9348\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3361 - accuracy: 0.9348\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3354 - accuracy: 0.9402\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3348 - accuracy: 0.9402\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3342 - accuracy: 0.9402\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3335 - accuracy: 0.9402\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3329 - accuracy: 0.9402\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3323 - accuracy: 0.9402\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3317 - accuracy: 0.9402\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3311 - accuracy: 0.9402\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3305 - accuracy: 0.9402\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3299 - accuracy: 0.9402\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3293 - accuracy: 0.9402\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3287 - accuracy: 0.9402\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3281 - accuracy: 0.9402\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3275 - accuracy: 0.9402\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3269 - accuracy: 0.9402\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3263 - accuracy: 0.9402\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3257 - accuracy: 0.9402\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3251 - accuracy: 0.9402\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3246 - accuracy: 0.9402\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3240 - accuracy: 0.9402\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3234 - accuracy: 0.9402\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3229 - accuracy: 0.9402\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3223 - accuracy: 0.9402\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3217 - accuracy: 0.9402\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3212 - accuracy: 0.9402\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3206 - accuracy: 0.9402\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.3201 - accuracy: 0.9402\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3196 - accuracy: 0.9402\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3190 - accuracy: 0.9402\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3185 - accuracy: 0.9457\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3179 - accuracy: 0.9457\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3174 - accuracy: 0.9457\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3169 - accuracy: 0.9457\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3164 - accuracy: 0.9457\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3159 - accuracy: 0.9457\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3153 - accuracy: 0.9511\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3148 - accuracy: 0.9511\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3143 - accuracy: 0.9511\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3138 - accuracy: 0.9511\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3133 - accuracy: 0.9565\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3128 - accuracy: 0.9565\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3123 - accuracy: 0.9565\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3118 - accuracy: 0.9565\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3113 - accuracy: 0.9565\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3108 - accuracy: 0.9565\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3104 - accuracy: 0.9565\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3099 - accuracy: 0.9565\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3094 - accuracy: 0.9565\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3089 - accuracy: 0.9565\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3084 - accuracy: 0.9565\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3080 - accuracy: 0.9565\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3075 - accuracy: 0.9565\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3070 - accuracy: 0.9565\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3066 - accuracy: 0.9565\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3061 - accuracy: 0.9620\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3057 - accuracy: 0.9620\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3052 - accuracy: 0.9620\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3047 - accuracy: 0.9620\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3043 - accuracy: 0.9620\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3039 - accuracy: 0.9620\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.3034 - accuracy: 0.9620\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3030 - accuracy: 0.9620\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3025 - accuracy: 0.9620\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3021 - accuracy: 0.9620\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3017 - accuracy: 0.9620\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3012 - accuracy: 0.9620\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3008 - accuracy: 0.9620\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3004 - accuracy: 0.9620\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3000 - accuracy: 0.9620\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2995 - accuracy: 0.9620\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2991 - accuracy: 0.9620\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2987 - accuracy: 0.9620\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2983 - accuracy: 0.9620\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2979 - accuracy: 0.9620\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2975 - accuracy: 0.9620\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2970 - accuracy: 0.9620\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2966 - accuracy: 0.9620\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2962 - accuracy: 0.9620\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2958 - accuracy: 0.9620\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2954 - accuracy: 0.9620\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2950 - accuracy: 0.9620\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2946 - accuracy: 0.9620\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2942 - accuracy: 0.9620\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2939 - accuracy: 0.9674\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2935 - accuracy: 0.9674\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2931 - accuracy: 0.9674\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2927 - accuracy: 0.9674\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2923 - accuracy: 0.9674\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2919 - accuracy: 0.9674\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2915 - accuracy: 0.9674\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2912 - accuracy: 0.9674\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2908 - accuracy: 0.9674\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2904 - accuracy: 0.9674\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2900 - accuracy: 0.9674\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2897 - accuracy: 0.9674\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2893 - accuracy: 0.9674\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2889 - accuracy: 0.9674\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2886 - accuracy: 0.9674\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2882 - accuracy: 0.9783\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.2879 - accuracy: 0.9783\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2875 - accuracy: 0.9783\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2871 - accuracy: 0.9783\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2868 - accuracy: 0.9783\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2864 - accuracy: 0.9728\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.2861 - accuracy: 0.9728\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2857 - accuracy: 0.9783\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2854 - accuracy: 0.9783\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2850 - accuracy: 0.9783\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2847 - accuracy: 0.9783\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2843 - accuracy: 0.9783\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2840 - accuracy: 0.9783\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2837 - accuracy: 0.9783\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2833 - accuracy: 0.9783\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.2830 - accuracy: 0.9783\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2827 - accuracy: 0.9783\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2823 - accuracy: 0.9783\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.2820 - accuracy: 0.9783\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2817 - accuracy: 0.9783\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2813 - accuracy: 0.9783\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2810 - accuracy: 0.9783\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2807 - accuracy: 0.9783\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2804 - accuracy: 0.9783\n",
            "30/30 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "623d2b2d-abaf-499a-b4be-53f03ca23089",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "800be5b5-20f0-494b-9934-748686e483bf",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333134651184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 402
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzVeFpPUFgv8",
        "colab_type": "text"
      },
      "source": [
        "#SENZA PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yjOvqGHhFl1T"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJgxb6a5Fl1U",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdKgkMePFl1e",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eHCd1eHGFl1n",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKb84eAAFl1w",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "469qraA8Fl15",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F93d4NAtFl2C",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak0h5b48sUsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "train_labels_enc = encoder.fit_transform(y_train)\n",
        "val_labels_enc = encoder.transform(y_val)\n",
        "test_labels_enc = encoder.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QEgnmw_EFl2L"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztWE5G71Fl2O",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJ7QoIiyFl2Z",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mfSvItJFl2i",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lFKekeMCFl2p",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ko6DZPbBFl2y",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpe4ugPVnX3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPIrjKmgnvdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2ii7-J9Fl26",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(584,), kernel_regularizer=regularizers.l2(l=0.05)))\n",
        "  #model.add(layers.Dropout(0.01))\n",
        "  #model.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(l=0.05)))\n",
        "  #model.add(layers.Dropout(0.01))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.05, momentum=0.9)\n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  rmsprop = RMSprop(lr=0.001)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lBjwz4bMFl3G",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, factor=0.1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1f4fafe6-5cf0-4778-f466-98324d098c8d",
        "id": "7ZqWt-mxFl3P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "59d6511a-454b-4f72-d27d-f0a90f5d1316",
        "id": "_CdS5EKbFl3W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, train_labels_enc, validation_data=(val_data_stand, val_labels_enc), \n",
        "                      epochs= num_epochs, batch_size=92, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 2.7233 - accuracy: 0.2826 - val_loss: 2.5479 - val_accuracy: 0.7083\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 2.6221 - accuracy: 0.6957 - val_loss: 2.0510 - val_accuracy: 0.7083\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 83us/step - loss: 2.0740 - accuracy: 0.6957 - val_loss: 1.7524 - val_accuracy: 0.8333\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 1.7453 - accuracy: 0.8696 - val_loss: 1.5144 - val_accuracy: 0.8333\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 1.4941 - accuracy: 0.8587 - val_loss: 1.3304 - val_accuracy: 0.7917\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 1.2886 - accuracy: 0.7717 - val_loss: 1.2119 - val_accuracy: 0.7917\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 1.1510 - accuracy: 0.7609 - val_loss: 1.0954 - val_accuracy: 0.8333\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 1.0220 - accuracy: 0.8043 - val_loss: 1.0004 - val_accuracy: 0.8333\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.9227 - accuracy: 0.8370 - val_loss: 0.9380 - val_accuracy: 0.8333\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.8581 - accuracy: 0.8913 - val_loss: 0.8934 - val_accuracy: 0.8333\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.7979 - accuracy: 0.9022 - val_loss: 0.8701 - val_accuracy: 0.8333\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.7493 - accuracy: 0.8696 - val_loss: 0.8651 - val_accuracy: 0.8333\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.7199 - accuracy: 0.8478 - val_loss: 0.8424 - val_accuracy: 0.8333\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6903 - accuracy: 0.8696 - val_loss: 0.8119 - val_accuracy: 0.8333\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6639 - accuracy: 0.9022 - val_loss: 0.7896 - val_accuracy: 0.8333\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6436 - accuracy: 0.8913 - val_loss: 0.7788 - val_accuracy: 0.8333\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6207 - accuracy: 0.9022 - val_loss: 0.7755 - val_accuracy: 0.8333\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5996 - accuracy: 0.8804 - val_loss: 0.7611 - val_accuracy: 0.8333\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.5786 - accuracy: 0.8804 - val_loss: 0.7307 - val_accuracy: 0.8333\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5555 - accuracy: 0.9239 - val_loss: 0.7029 - val_accuracy: 0.8333\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5349 - accuracy: 0.9239 - val_loss: 0.6884 - val_accuracy: 0.8333\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5126 - accuracy: 0.9239 - val_loss: 0.6830 - val_accuracy: 0.8333\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4926 - accuracy: 0.9130 - val_loss: 0.6645 - val_accuracy: 0.8333\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4732 - accuracy: 0.9130 - val_loss: 0.6353 - val_accuracy: 0.8333\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.4556 - accuracy: 0.9457 - val_loss: 0.6232 - val_accuracy: 0.8333\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.4384 - accuracy: 0.9348 - val_loss: 0.6265 - val_accuracy: 0.8333\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.4241 - accuracy: 0.9022 - val_loss: 0.6056 - val_accuracy: 0.8333\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.4089 - accuracy: 0.9348 - val_loss: 0.5817 - val_accuracy: 0.8333\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3987 - accuracy: 0.9565 - val_loss: 0.5929 - val_accuracy: 0.8333\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3866 - accuracy: 0.9348 - val_loss: 0.5896 - val_accuracy: 0.8333\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3778 - accuracy: 0.9348 - val_loss: 0.5657 - val_accuracy: 0.8333\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3710 - accuracy: 0.9674 - val_loss: 0.5836 - val_accuracy: 0.8333\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.3620 - accuracy: 0.9457 - val_loss: 0.5779 - val_accuracy: 0.8333\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3545 - accuracy: 0.9457 - val_loss: 0.5599 - val_accuracy: 0.8333\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3495 - accuracy: 0.9674 - val_loss: 0.5885 - val_accuracy: 0.8333\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3437 - accuracy: 0.9457 - val_loss: 0.5609 - val_accuracy: 0.8333\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.3361 - accuracy: 0.9674 - val_loss: 0.5703 - val_accuracy: 0.8333\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3292 - accuracy: 0.9565 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3236 - accuracy: 0.9457 - val_loss: 0.5491 - val_accuracy: 0.8333\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.3194 - accuracy: 0.9674 - val_loss: 0.5959 - val_accuracy: 0.8333\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.3179 - accuracy: 0.9239 - val_loss: 0.5306 - val_accuracy: 0.8333\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.3220 - accuracy: 0.9674 - val_loss: 0.6588 - val_accuracy: 0.8333\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.3402 - accuracy: 0.8587 - val_loss: 0.5241 - val_accuracy: 0.8333\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3306 - accuracy: 0.9674 - val_loss: 0.6272 - val_accuracy: 0.8333\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3082 - accuracy: 0.8913 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2869 - accuracy: 0.9674 - val_loss: 0.5275 - val_accuracy: 0.8333\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3045 - accuracy: 0.9674 - val_loss: 0.6766 - val_accuracy: 0.8333\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3168 - accuracy: 0.8696 - val_loss: 0.5518 - val_accuracy: 0.8333\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2793 - accuracy: 0.9891 - val_loss: 0.5332 - val_accuracy: 0.8333\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2884 - accuracy: 0.9783 - val_loss: 0.6786 - val_accuracy: 0.8333\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3081 - accuracy: 0.8696 - val_loss: 0.5483 - val_accuracy: 0.8333\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2727 - accuracy: 0.9783 - val_loss: 0.5389 - val_accuracy: 0.8333\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2757 - accuracy: 0.9783 - val_loss: 0.6815 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2982 - accuracy: 0.8696 - val_loss: 0.6651 - val_accuracy: 0.8333\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2887 - accuracy: 0.8913 - val_loss: 0.6242 - val_accuracy: 0.8333\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2712 - accuracy: 0.9348 - val_loss: 0.5775 - val_accuracy: 0.8333\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2623 - accuracy: 0.9674 - val_loss: 0.5445 - val_accuracy: 0.8333\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2683 - accuracy: 0.9783 - val_loss: 0.5306 - val_accuracy: 0.8333\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 89us/step - loss: 0.2790 - accuracy: 0.9783 - val_loss: 0.5295 - val_accuracy: 0.8333\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 125us/step - loss: 0.2799 - accuracy: 0.9783 - val_loss: 0.5386 - val_accuracy: 0.8333\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2704 - accuracy: 0.9783 - val_loss: 0.5604 - val_accuracy: 0.8333\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2616 - accuracy: 0.9891 - val_loss: 0.5911 - val_accuracy: 0.8333\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2613 - accuracy: 0.9674 - val_loss: 0.6182 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.2669 - accuracy: 0.9348 - val_loss: 0.6194 - val_accuracy: 0.8333\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2672 - accuracy: 0.9348 - val_loss: 0.6190 - val_accuracy: 0.8333\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2670 - accuracy: 0.9348 - val_loss: 0.6171 - val_accuracy: 0.8333\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2665 - accuracy: 0.9348 - val_loss: 0.6141 - val_accuracy: 0.8333\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2656 - accuracy: 0.9457 - val_loss: 0.6101 - val_accuracy: 0.8333\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2645 - accuracy: 0.9565 - val_loss: 0.6054 - val_accuracy: 0.8333\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2634 - accuracy: 0.9565 - val_loss: 0.6002 - val_accuracy: 0.8333\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2622 - accuracy: 0.9674 - val_loss: 0.5948 - val_accuracy: 0.8333\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2612 - accuracy: 0.9674 - val_loss: 0.5893 - val_accuracy: 0.8333\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2604 - accuracy: 0.9674 - val_loss: 0.5839 - val_accuracy: 0.8333\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2597 - accuracy: 0.9674 - val_loss: 0.5789 - val_accuracy: 0.8333\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2594 - accuracy: 0.9674 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2592 - accuracy: 0.9674 - val_loss: 0.5703 - val_accuracy: 0.8333\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2592 - accuracy: 0.9674 - val_loss: 0.5668 - val_accuracy: 0.8333\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2593 - accuracy: 0.9891 - val_loss: 0.5639 - val_accuracy: 0.8333\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2595 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 88us/step - loss: 0.2597 - accuracy: 0.9891 - val_loss: 0.5599 - val_accuracy: 0.8333\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2598 - accuracy: 0.9891 - val_loss: 0.5588 - val_accuracy: 0.8333\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2599 - accuracy: 0.9891 - val_loss: 0.5582 - val_accuracy: 0.8333\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2600 - accuracy: 0.9891 - val_loss: 0.5582 - val_accuracy: 0.8333\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 84us/step - loss: 0.2599 - accuracy: 0.9891 - val_loss: 0.5586 - val_accuracy: 0.8333\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2598 - accuracy: 0.9891 - val_loss: 0.5594 - val_accuracy: 0.8333\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2596 - accuracy: 0.9891 - val_loss: 0.5605 - val_accuracy: 0.8333\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 88us/step - loss: 0.2594 - accuracy: 0.9891 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 99us/step - loss: 0.2591 - accuracy: 0.9891 - val_loss: 0.5636 - val_accuracy: 0.8333\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2589 - accuracy: 0.9891 - val_loss: 0.5655 - val_accuracy: 0.8333\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2587 - accuracy: 0.9891 - val_loss: 0.5674 - val_accuracy: 0.8333\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 92us/step - loss: 0.2585 - accuracy: 0.9783 - val_loss: 0.5693 - val_accuracy: 0.8333\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2584 - accuracy: 0.9674 - val_loss: 0.5711 - val_accuracy: 0.8333\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 86us/step - loss: 0.2583 - accuracy: 0.9674 - val_loss: 0.5729 - val_accuracy: 0.8333\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2582 - accuracy: 0.9674 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2582 - accuracy: 0.9674 - val_loss: 0.5757 - val_accuracy: 0.8333\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2582 - accuracy: 0.9674 - val_loss: 0.5768 - val_accuracy: 0.8333\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2582 - accuracy: 0.9674 - val_loss: 0.5776 - val_accuracy: 0.8333\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2582 - accuracy: 0.9674 - val_loss: 0.5781 - val_accuracy: 0.8333\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 86us/step - loss: 0.2582 - accuracy: 0.9674 - val_loss: 0.5783 - val_accuracy: 0.8333\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2581 - accuracy: 0.9674 - val_loss: 0.5782 - val_accuracy: 0.8333\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2581 - accuracy: 0.9674 - val_loss: 0.5779 - val_accuracy: 0.8333\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2580 - accuracy: 0.9674 - val_loss: 0.5773 - val_accuracy: 0.8333\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2579 - accuracy: 0.9674 - val_loss: 0.5766 - val_accuracy: 0.8333\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2579 - accuracy: 0.9674 - val_loss: 0.5758 - val_accuracy: 0.8333\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2578 - accuracy: 0.9674 - val_loss: 0.5748 - val_accuracy: 0.8333\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2577 - accuracy: 0.9674 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2576 - accuracy: 0.9674 - val_loss: 0.5728 - val_accuracy: 0.8333\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2575 - accuracy: 0.9674 - val_loss: 0.5719 - val_accuracy: 0.8333\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.2575 - accuracy: 0.9674 - val_loss: 0.5710 - val_accuracy: 0.8333\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2574 - accuracy: 0.9674 - val_loss: 0.5701 - val_accuracy: 0.8333\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2574 - accuracy: 0.9674 - val_loss: 0.5694 - val_accuracy: 0.8333\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2573 - accuracy: 0.9674 - val_loss: 0.5688 - val_accuracy: 0.8333\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2573 - accuracy: 0.9674 - val_loss: 0.5683 - val_accuracy: 0.8333\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2572 - accuracy: 0.9674 - val_loss: 0.5679 - val_accuracy: 0.8333\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2572 - accuracy: 0.9783 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2571 - accuracy: 0.9783 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2571 - accuracy: 0.9783 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2570 - accuracy: 0.9783 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2570 - accuracy: 0.9783 - val_loss: 0.5679 - val_accuracy: 0.8333\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 92us/step - loss: 0.2569 - accuracy: 0.9674 - val_loss: 0.5682 - val_accuracy: 0.8333\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2569 - accuracy: 0.9674 - val_loss: 0.5685 - val_accuracy: 0.8333\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2568 - accuracy: 0.9674 - val_loss: 0.5688 - val_accuracy: 0.8333\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2568 - accuracy: 0.9674 - val_loss: 0.5691 - val_accuracy: 0.8333\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2567 - accuracy: 0.9674 - val_loss: 0.5695 - val_accuracy: 0.8333\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2567 - accuracy: 0.9674 - val_loss: 0.5698 - val_accuracy: 0.8333\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2566 - accuracy: 0.9674 - val_loss: 0.5701 - val_accuracy: 0.8333\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2566 - accuracy: 0.9674 - val_loss: 0.5703 - val_accuracy: 0.8333\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2565 - accuracy: 0.9674 - val_loss: 0.5704 - val_accuracy: 0.8333\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2565 - accuracy: 0.9674 - val_loss: 0.5705 - val_accuracy: 0.8333\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2564 - accuracy: 0.9674 - val_loss: 0.5706 - val_accuracy: 0.8333\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2564 - accuracy: 0.9674 - val_loss: 0.5706 - val_accuracy: 0.8333\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2563 - accuracy: 0.9674 - val_loss: 0.5705 - val_accuracy: 0.8333\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2563 - accuracy: 0.9674 - val_loss: 0.5703 - val_accuracy: 0.8333\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2562 - accuracy: 0.9674 - val_loss: 0.5701 - val_accuracy: 0.8333\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2562 - accuracy: 0.9674 - val_loss: 0.5699 - val_accuracy: 0.8333\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2561 - accuracy: 0.9674 - val_loss: 0.5697 - val_accuracy: 0.8333\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2561 - accuracy: 0.9674 - val_loss: 0.5694 - val_accuracy: 0.8333\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2560 - accuracy: 0.9674 - val_loss: 0.5692 - val_accuracy: 0.8333\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2560 - accuracy: 0.9674 - val_loss: 0.5689 - val_accuracy: 0.8333\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2559 - accuracy: 0.9674 - val_loss: 0.5687 - val_accuracy: 0.8333\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2559 - accuracy: 0.9674 - val_loss: 0.5684 - val_accuracy: 0.8333\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2558 - accuracy: 0.9674 - val_loss: 0.5682 - val_accuracy: 0.8333\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2558 - accuracy: 0.9674 - val_loss: 0.5681 - val_accuracy: 0.8333\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2557 - accuracy: 0.9674 - val_loss: 0.5679 - val_accuracy: 0.8333\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2557 - accuracy: 0.9674 - val_loss: 0.5678 - val_accuracy: 0.8333\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2556 - accuracy: 0.9674 - val_loss: 0.5678 - val_accuracy: 0.8333\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2556 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2555 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 83us/step - loss: 0.2555 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2554 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2554 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2553 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2553 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2552 - accuracy: 0.9674 - val_loss: 0.5678 - val_accuracy: 0.8333\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2552 - accuracy: 0.9674 - val_loss: 0.5678 - val_accuracy: 0.8333\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2551 - accuracy: 0.9674 - val_loss: 0.5678 - val_accuracy: 0.8333\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2551 - accuracy: 0.9674 - val_loss: 0.5678 - val_accuracy: 0.8333\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2551 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.2550 - accuracy: 0.9674 - val_loss: 0.5677 - val_accuracy: 0.8333\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2550 - accuracy: 0.9674 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2549 - accuracy: 0.9674 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2549 - accuracy: 0.9674 - val_loss: 0.5675 - val_accuracy: 0.8333\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2548 - accuracy: 0.9674 - val_loss: 0.5674 - val_accuracy: 0.8333\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2548 - accuracy: 0.9674 - val_loss: 0.5673 - val_accuracy: 0.8333\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2547 - accuracy: 0.9674 - val_loss: 0.5672 - val_accuracy: 0.8333\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2547 - accuracy: 0.9674 - val_loss: 0.5671 - val_accuracy: 0.8333\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2546 - accuracy: 0.9674 - val_loss: 0.5670 - val_accuracy: 0.8333\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2546 - accuracy: 0.9674 - val_loss: 0.5669 - val_accuracy: 0.8333\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2545 - accuracy: 0.9674 - val_loss: 0.5669 - val_accuracy: 0.8333\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2545 - accuracy: 0.9674 - val_loss: 0.5668 - val_accuracy: 0.8333\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2545 - accuracy: 0.9674 - val_loss: 0.5667 - val_accuracy: 0.8333\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2544 - accuracy: 0.9674 - val_loss: 0.5666 - val_accuracy: 0.8333\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2544 - accuracy: 0.9674 - val_loss: 0.5666 - val_accuracy: 0.8333\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 97us/step - loss: 0.2543 - accuracy: 0.9674 - val_loss: 0.5665 - val_accuracy: 0.8333\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2543 - accuracy: 0.9674 - val_loss: 0.5665 - val_accuracy: 0.8333\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2542 - accuracy: 0.9674 - val_loss: 0.5664 - val_accuracy: 0.8333\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2542 - accuracy: 0.9674 - val_loss: 0.5664 - val_accuracy: 0.8333\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 81us/step - loss: 0.2541 - accuracy: 0.9674 - val_loss: 0.5663 - val_accuracy: 0.8333\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2541 - accuracy: 0.9674 - val_loss: 0.5663 - val_accuracy: 0.8333\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2540 - accuracy: 0.9674 - val_loss: 0.5662 - val_accuracy: 0.8333\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2540 - accuracy: 0.9674 - val_loss: 0.5662 - val_accuracy: 0.8333\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2540 - accuracy: 0.9674 - val_loss: 0.5662 - val_accuracy: 0.8333\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2539 - accuracy: 0.9674 - val_loss: 0.5661 - val_accuracy: 0.8333\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2539 - accuracy: 0.9674 - val_loss: 0.5661 - val_accuracy: 0.8333\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2538 - accuracy: 0.9674 - val_loss: 0.5661 - val_accuracy: 0.8333\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2538 - accuracy: 0.9674 - val_loss: 0.5661 - val_accuracy: 0.8333\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2537 - accuracy: 0.9674 - val_loss: 0.5660 - val_accuracy: 0.8333\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2537 - accuracy: 0.9674 - val_loss: 0.5660 - val_accuracy: 0.8333\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 101us/step - loss: 0.2536 - accuracy: 0.9674 - val_loss: 0.5659 - val_accuracy: 0.8333\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2536 - accuracy: 0.9674 - val_loss: 0.5659 - val_accuracy: 0.8333\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2536 - accuracy: 0.9674 - val_loss: 0.5658 - val_accuracy: 0.8333\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2535 - accuracy: 0.9674 - val_loss: 0.5658 - val_accuracy: 0.8333\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2535 - accuracy: 0.9674 - val_loss: 0.5657 - val_accuracy: 0.8333\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2534 - accuracy: 0.9674 - val_loss: 0.5657 - val_accuracy: 0.8333\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2534 - accuracy: 0.9674 - val_loss: 0.5656 - val_accuracy: 0.8333\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2533 - accuracy: 0.9674 - val_loss: 0.5655 - val_accuracy: 0.8333\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2533 - accuracy: 0.9674 - val_loss: 0.5655 - val_accuracy: 0.8333\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2533 - accuracy: 0.9674 - val_loss: 0.5654 - val_accuracy: 0.8333\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2532 - accuracy: 0.9674 - val_loss: 0.5654 - val_accuracy: 0.8333\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2532 - accuracy: 0.9674 - val_loss: 0.5653 - val_accuracy: 0.8333\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2531 - accuracy: 0.9674 - val_loss: 0.5653 - val_accuracy: 0.8333\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2531 - accuracy: 0.9674 - val_loss: 0.5652 - val_accuracy: 0.8333\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2530 - accuracy: 0.9674 - val_loss: 0.5652 - val_accuracy: 0.8333\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2530 - accuracy: 0.9674 - val_loss: 0.5651 - val_accuracy: 0.8333\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2530 - accuracy: 0.9674 - val_loss: 0.5651 - val_accuracy: 0.8333\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.2529 - accuracy: 0.9674 - val_loss: 0.5650 - val_accuracy: 0.8333\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2529 - accuracy: 0.9674 - val_loss: 0.5650 - val_accuracy: 0.8333\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2528 - accuracy: 0.9674 - val_loss: 0.5650 - val_accuracy: 0.8333\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2528 - accuracy: 0.9674 - val_loss: 0.5649 - val_accuracy: 0.8333\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.2527 - accuracy: 0.9674 - val_loss: 0.5649 - val_accuracy: 0.8333\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2527 - accuracy: 0.9674 - val_loss: 0.5649 - val_accuracy: 0.8333\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2527 - accuracy: 0.9674 - val_loss: 0.5648 - val_accuracy: 0.8333\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2526 - accuracy: 0.9674 - val_loss: 0.5648 - val_accuracy: 0.8333\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2526 - accuracy: 0.9674 - val_loss: 0.5647 - val_accuracy: 0.8333\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2525 - accuracy: 0.9674 - val_loss: 0.5647 - val_accuracy: 0.8333\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2525 - accuracy: 0.9674 - val_loss: 0.5646 - val_accuracy: 0.8333\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2525 - accuracy: 0.9674 - val_loss: 0.5646 - val_accuracy: 0.8333\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2524 - accuracy: 0.9674 - val_loss: 0.5645 - val_accuracy: 0.8333\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2524 - accuracy: 0.9674 - val_loss: 0.5645 - val_accuracy: 0.8333\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2523 - accuracy: 0.9674 - val_loss: 0.5645 - val_accuracy: 0.8333\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2523 - accuracy: 0.9674 - val_loss: 0.5644 - val_accuracy: 0.8333\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2522 - accuracy: 0.9674 - val_loss: 0.5644 - val_accuracy: 0.8333\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2522 - accuracy: 0.9674 - val_loss: 0.5644 - val_accuracy: 0.8333\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.2522 - accuracy: 0.9674 - val_loss: 0.5643 - val_accuracy: 0.8333\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2521 - accuracy: 0.9674 - val_loss: 0.5643 - val_accuracy: 0.8333\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2521 - accuracy: 0.9674 - val_loss: 0.5643 - val_accuracy: 0.8333\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2520 - accuracy: 0.9674 - val_loss: 0.5642 - val_accuracy: 0.8333\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 101us/step - loss: 0.2520 - accuracy: 0.9674 - val_loss: 0.5642 - val_accuracy: 0.8333\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2520 - accuracy: 0.9674 - val_loss: 0.5642 - val_accuracy: 0.8333\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2519 - accuracy: 0.9674 - val_loss: 0.5641 - val_accuracy: 0.8333\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2519 - accuracy: 0.9674 - val_loss: 0.5641 - val_accuracy: 0.8333\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2518 - accuracy: 0.9674 - val_loss: 0.5641 - val_accuracy: 0.8333\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2518 - accuracy: 0.9674 - val_loss: 0.5640 - val_accuracy: 0.8333\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 106us/step - loss: 0.2518 - accuracy: 0.9674 - val_loss: 0.5640 - val_accuracy: 0.8333\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.2517 - accuracy: 0.9674 - val_loss: 0.5640 - val_accuracy: 0.8333\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2517 - accuracy: 0.9674 - val_loss: 0.5640 - val_accuracy: 0.8333\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2516 - accuracy: 0.9674 - val_loss: 0.5639 - val_accuracy: 0.8333\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2516 - accuracy: 0.9674 - val_loss: 0.5639 - val_accuracy: 0.8333\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2516 - accuracy: 0.9674 - val_loss: 0.5639 - val_accuracy: 0.8333\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2515 - accuracy: 0.9674 - val_loss: 0.5638 - val_accuracy: 0.8333\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2515 - accuracy: 0.9674 - val_loss: 0.5638 - val_accuracy: 0.8333\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2514 - accuracy: 0.9674 - val_loss: 0.5638 - val_accuracy: 0.8333\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 86us/step - loss: 0.2514 - accuracy: 0.9674 - val_loss: 0.5637 - val_accuracy: 0.8333\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.2514 - accuracy: 0.9674 - val_loss: 0.5637 - val_accuracy: 0.8333\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2513 - accuracy: 0.9674 - val_loss: 0.5637 - val_accuracy: 0.8333\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2513 - accuracy: 0.9674 - val_loss: 0.5637 - val_accuracy: 0.8333\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2512 - accuracy: 0.9674 - val_loss: 0.5636 - val_accuracy: 0.8333\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2512 - accuracy: 0.9674 - val_loss: 0.5636 - val_accuracy: 0.8333\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2512 - accuracy: 0.9674 - val_loss: 0.5636 - val_accuracy: 0.8333\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2511 - accuracy: 0.9674 - val_loss: 0.5635 - val_accuracy: 0.8333\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2511 - accuracy: 0.9674 - val_loss: 0.5635 - val_accuracy: 0.8333\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2510 - accuracy: 0.9674 - val_loss: 0.5635 - val_accuracy: 0.8333\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2510 - accuracy: 0.9674 - val_loss: 0.5634 - val_accuracy: 0.8333\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2510 - accuracy: 0.9674 - val_loss: 0.5634 - val_accuracy: 0.8333\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2509 - accuracy: 0.9674 - val_loss: 0.5634 - val_accuracy: 0.8333\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2509 - accuracy: 0.9674 - val_loss: 0.5633 - val_accuracy: 0.8333\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2508 - accuracy: 0.9674 - val_loss: 0.5633 - val_accuracy: 0.8333\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2508 - accuracy: 0.9674 - val_loss: 0.5633 - val_accuracy: 0.8333\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2508 - accuracy: 0.9674 - val_loss: 0.5633 - val_accuracy: 0.8333\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2507 - accuracy: 0.9674 - val_loss: 0.5632 - val_accuracy: 0.8333\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2507 - accuracy: 0.9674 - val_loss: 0.5632 - val_accuracy: 0.8333\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2507 - accuracy: 0.9674 - val_loss: 0.5632 - val_accuracy: 0.8333\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2506 - accuracy: 0.9674 - val_loss: 0.5632 - val_accuracy: 0.8333\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2506 - accuracy: 0.9674 - val_loss: 0.5631 - val_accuracy: 0.8333\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2505 - accuracy: 0.9674 - val_loss: 0.5631 - val_accuracy: 0.8333\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2505 - accuracy: 0.9674 - val_loss: 0.5631 - val_accuracy: 0.8333\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2505 - accuracy: 0.9674 - val_loss: 0.5630 - val_accuracy: 0.8333\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2504 - accuracy: 0.9674 - val_loss: 0.5630 - val_accuracy: 0.8333\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 90us/step - loss: 0.2504 - accuracy: 0.9674 - val_loss: 0.5630 - val_accuracy: 0.8333\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2503 - accuracy: 0.9674 - val_loss: 0.5630 - val_accuracy: 0.8333\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2503 - accuracy: 0.9674 - val_loss: 0.5630 - val_accuracy: 0.8333\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2503 - accuracy: 0.9674 - val_loss: 0.5630 - val_accuracy: 0.8333\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2502 - accuracy: 0.9674 - val_loss: 0.5629 - val_accuracy: 0.8333\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2502 - accuracy: 0.9674 - val_loss: 0.5629 - val_accuracy: 0.8333\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2502 - accuracy: 0.9674 - val_loss: 0.5629 - val_accuracy: 0.8333\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2501 - accuracy: 0.9674 - val_loss: 0.5629 - val_accuracy: 0.8333\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2501 - accuracy: 0.9674 - val_loss: 0.5628 - val_accuracy: 0.8333\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 87us/step - loss: 0.2500 - accuracy: 0.9674 - val_loss: 0.5628 - val_accuracy: 0.8333\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2500 - accuracy: 0.9674 - val_loss: 0.5628 - val_accuracy: 0.8333\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2500 - accuracy: 0.9674 - val_loss: 0.5628 - val_accuracy: 0.8333\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2499 - accuracy: 0.9674 - val_loss: 0.5628 - val_accuracy: 0.8333\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2499 - accuracy: 0.9674 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2499 - accuracy: 0.9674 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2498 - accuracy: 0.9674 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2498 - accuracy: 0.9674 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2497 - accuracy: 0.9674 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2497 - accuracy: 0.9674 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2497 - accuracy: 0.9674 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2496 - accuracy: 0.9674 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2496 - accuracy: 0.9674 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2496 - accuracy: 0.9674 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2495 - accuracy: 0.9674 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.2495 - accuracy: 0.9674 - val_loss: 0.5625 - val_accuracy: 0.8333\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2494 - accuracy: 0.9674 - val_loss: 0.5625 - val_accuracy: 0.8333\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2494 - accuracy: 0.9674 - val_loss: 0.5625 - val_accuracy: 0.8333\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2494 - accuracy: 0.9674 - val_loss: 0.5625 - val_accuracy: 0.8333\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2493 - accuracy: 0.9674 - val_loss: 0.5624 - val_accuracy: 0.8333\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2493 - accuracy: 0.9674 - val_loss: 0.5624 - val_accuracy: 0.8333\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2493 - accuracy: 0.9674 - val_loss: 0.5624 - val_accuracy: 0.8333\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2492 - accuracy: 0.9674 - val_loss: 0.5624 - val_accuracy: 0.8333\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2492 - accuracy: 0.9674 - val_loss: 0.5624 - val_accuracy: 0.8333\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2491 - accuracy: 0.9674 - val_loss: 0.5624 - val_accuracy: 0.8333\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2491 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2491 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 90us/step - loss: 0.2490 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2490 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2490 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 94us/step - loss: 0.2489 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2489 - accuracy: 0.9674 - val_loss: 0.5623 - val_accuracy: 0.8333\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2489 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2488 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2488 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2487 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2487 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2487 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2486 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2486 - accuracy: 0.9674 - val_loss: 0.5622 - val_accuracy: 0.8333\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2486 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2485 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2485 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2485 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.2484 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2484 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2484 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2483 - accuracy: 0.9674 - val_loss: 0.5621 - val_accuracy: 0.8333\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2483 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2482 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2482 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2482 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2481 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2481 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2481 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2480 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2480 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2480 - accuracy: 0.9674 - val_loss: 0.5620 - val_accuracy: 0.8333\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2479 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2479 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2479 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2478 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2478 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2477 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2477 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2477 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2476 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2476 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2476 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 84us/step - loss: 0.2475 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 101us/step - loss: 0.2475 - accuracy: 0.9783 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 98us/step - loss: 0.2475 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 81us/step - loss: 0.2474 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 129us/step - loss: 0.2474 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2474 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2473 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2473 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2473 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2472 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2472 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2471 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2471 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2471 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2470 - accuracy: 0.9783 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2470 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2470 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2469 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2469 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2469 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2468 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2468 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2468 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2467 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2467 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2467 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2466 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2466 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2466 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2465 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2465 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2465 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2464 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2464 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2464 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2463 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2463 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2463 - accuracy: 0.9783 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2462 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2462 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2462 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2461 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2461 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2461 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2460 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2460 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 91us/step - loss: 0.2459 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2459 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 96us/step - loss: 0.2459 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2458 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2458 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2458 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2457 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2457 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2457 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2456 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2456 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2456 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2455 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2455 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2455 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2454 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2454 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2454 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2453 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2453 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2453 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2452 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2452 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2452 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2451 - accuracy: 0.9783 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2451 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2451 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2450 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2450 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2450 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2449 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2449 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2449 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2448 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2448 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2448 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2447 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2447 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2447 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2446 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2446 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2446 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2445 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2445 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2445 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2444 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2444 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2444 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2444 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2443 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2443 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2443 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2442 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2442 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2442 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2441 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2441 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2441 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2440 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2440 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2440 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 103us/step - loss: 0.2439 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2439 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2439 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2438 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2438 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2438 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2437 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2437 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2437 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2436 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 87us/step - loss: 0.2436 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 87us/step - loss: 0.2436 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2435 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2435 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2435 - accuracy: 0.9891 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2434 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2434 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2434 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2433 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2433 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2433 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2433 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2432 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2432 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2432 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2431 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2431 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2431 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2430 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2430 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2430 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2429 - accuracy: 0.9891 - val_loss: 0.5617 - val_accuracy: 0.8333\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2429 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2429 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2428 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2428 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2428 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 89us/step - loss: 0.2427 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2427 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.2427 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2426 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2426 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2426 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2426 - accuracy: 0.9891 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 111us/step - loss: 0.2425 - accuracy: 0.9891 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2425 - accuracy: 0.9891 - val_loss: 0.5619 - val_accuracy: 0.8333\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2425 - accuracy: 0.9891 - val_loss: 0.5619 - val_accuracy: 0.8333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8nZswCbNFl3d"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6g1W5VoYFl3d",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TDnMuIJMFl3k",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a3a65485-ff72-4828-edde-42bf20aae5cf",
        "id": "QNntaA6rFl3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff8be265550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 416
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Pcr8dYkFl3x"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ca158c55-4c86-4132-f0a6-1ebb99eda923",
        "id": "xF6aLHBgFl3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff8be1cfa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 417
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXwV9b3/8deHAAYE2VUkKKiI4kW2iBZsxaotdIEfboBdRG1VKlVs69Zay7W391erv7pUtBevikstLq2UWtS60VqxmsgmgihQlKDQiKwikMDn98fMSSbJOclJyHBI5v18PM7jzHbmfObkZD7n+5mZ75i7IyIiydUi1wGIiEhuKRGIiCScEoGISMIpEYiIJJwSgYhIwikRiIgknBJBApjZM2Z2QWMvm0tmttrMzohhvW5mR4fDvzWzn2azbAPe5xtm9teGxpl0ZtYr/PxbZpg/1cwe2ddxNVVpP0TJPTPbFhltC+wEdofjl7r777Jdl7uPimPZ5s7dL2uM9ZhZL+BfQCt3Lw/X/Tsg67+hSJyUCPZT7t4uNWxmq4HvuPsL1Zczs5apnYtIrun72DSpNNTEmNkIMysxs2vNbB3wgJl1MrOnzazUzDaGwwWR18w1s++EwxPN7B9mdmu47L/MbFQDl+1tZn83s61m9oKZTcvUHM8yxp+b2avh+v5qZl0j879lZu+b2QYz+0ktn89JZrbOzPIi08aa2eJweKiZvWZmm8zsIzO7y8xaZ1jXDDP7r8j41eFrPjSzi6ot+1UzW2BmW8xsjZlNjcz+e/i8ycy2mdnnUp9t5PXDzKzIzDaHz8Oy/Wzq+Tl3NrMHwm3YaGazIvPGmNnCcBtWmtnIcHqVMly07BIp0VxsZh8AL4XTnwj/DpvD78jxkde3MbP/F/49N4ffsTZm9hcz+3617VlsZmPTbWu15Xqb2d/Cz+d5oGu1+RnjESWCpupQoDNwBHAJwd/xgXD8cOAz4K5aXn8SsJzgn+VXwH1mZg1Y9lHgDaALMBX4Vi3vmU2M5wMXAgcDrYEfAZhZP+CecP2Hhe9XQBru/jrwKfDFaut9NBzeDVwVbs/ngNOB79USN2EMI8N4zgT6ANWPT3wKfBvoCHwVmGRm/yec94XwuaO7t3P316qtuzPwF+DOcNt+DfzFzLpU24Yan00adX3ODxOUGo8P13VbGMNQ4CHg6nAbvgCszvR5pHEqcBzw5XD8GYLP6WBgPlXLYLcCQ4BhBN/ja4A9wIPAN1MLmdkAoAfBZ1OXR4E3Cf6uPweqH+eqLR5xdz328wfBP+QZ4fAIYBeQX8vyA4GNkfG5BKUlgInAisi8toADh9ZnWYKdTDnQNjL/EeCRLLcpXYw3RMa/BzwbDt8IzIzMOzD8DM7IsO7/Au4Ph9sT7KSPyLDsFOCpyLgDR4fDM4D/CofvB34ZWe6Y6LJp1ns7cFs43CtctmVk/kTgH+Hwt4A3qr3+NWBiXZ9NfT5noDvBDrdTmuX+JxVvbd+/cHxq6u8c2bYja4mhY7hMB4JE9RkwIM1y+cBGoE84fitwd4Z1Vnymke/igZH5j2b6Lkbj2dv/zebyUIugaSp19x2pETNra2b/Eza1txCUIjpGyyPVrEsNuPv2cLBdPZc9DPgkMg1gTaaAs4xxXWR4eySmw6LrdvdPgQ2Z3otgJ3CWmR0AnAXMd/f3wziOCcsl68I4/ptqZYQMqsQAvF9t+04ys5fDksxm4LIs15ta9/vVpr1P8Gs4JdNnU0Udn3NPgr/ZxjQv7QmszDLedCo+GzPLM7NfhuWlLVS2LLqGj/x07xV+px8DvmlmLYAJBC2YuhxGkOw+jUyr+DzriEdQaaipqt5l7A+BvsBJ7n4QlaWITOWexvAR0NnM2kam9axl+b2J8aPousP37JJpYXdfSrAjGEXVshAEJaZ3CH51HgT8uCExEPwKjXoUmA30dPcOwG8j662ri98PCUo5UYcDa7OIq7raPuc1BH+zjmletwY4KsM6PyVoDaYcmmaZ6DaeD4whKJ91IPj1norhY2BHLe/1IPANgpLddq9WRsvgI6CTmR0YmRb9+9QWj6BE0Fy0J2hubwrrzT+L+w3DX9jFwFQza21mnwO+HlOMTwJfM7NTLDiwexN1f3cfBa4k2BE+US2OLcA2MzsWmJRlDI8DE82sX5iIqsffnuDX9o6w3n5+ZF4pQUnmyAzrngMcY2bnm1lLMxsH9AOezjK26nGk/Zzd/SOCWvnd4UHlVmaWShT3ARea2elm1sLMeoSfD8BCYHy4fCFwThYx7CRotbUlaHWlYthDUGb7tZkdFv5a/1zYeiPc8e8B/h/ZtQai38X/DL+Lp1D1u5gxHgkoETQPtwNtCH5t/RN4dh+97zcIDrhuIKjLP0bwD5dOg2N097eBywl27h8R1JFL6njZ7wkOYL7k7h9Hpv+IYCe9Fbg3jDmbGJ4Jt+ElYEX4HPU94CYz20pwTOPxyGu3A78AXrXgbKWTq617A/A1gl/zGwgOnn6tWtzZqutz/hZQRtAq+jfBMRLc/Q2Cg9G3AZuBv1HZSvkpwS/4jcB/UrWFlc5DBC2ytcDSMI6oHwFvAUXAJ8DNVN0XPQT0JzjmlK3zCU5s+IQg+T1Uj3gSz8KDJyJ7zcweA95x99hbJNJ8mdm3gUvc/ZRcx5IUahFIg5nZiWZ2VFhKGElQh51V1+tEMgnLbt8Dpuc6liRRIpC9cSjBqY3bCM6Bn+TuC3IakTRZZvZlguMp66m7/CSNSKUhEZGEU4tARCThmlync127dvVevXrlOgwRkSblzTff/Njdu6Wb1+QSQa9evSguLs51GCIiTYqZVb96vYJKQyIiCadEICKScLElAjO738z+bWZLMsw3M7vTzFaEfY4PjisWERHJLM4WwQxgZC3zRxH0D96HoE/9e2KMRUREMogtEbj73wn6/chkDPCQB/5J0FVu97jiERGR9HJ5jKAHVft3L6Fq/+sVzOwSMys2s+LS0tJ9EpyISFI0iYPF7j7d3QvdvbBbt7SnwYqISAPl8jqCtVS90UcBDbsRx37LHR54AM4/H/Lza84vK4MHH4SJE6FlLX+JNWvg/vth9+708/PzYfJkOOigRglbpNlwh7vvhvXrcx1J4/j61+HEExt/vblMBLOByWY2k6Af8c3hjTOajX/8Ay6+GObNg//935rzb7sNrr0W8vLgwgszr+e3v4X//m9Id3v5VFdR3brBd7/bOHGLNBdLlgQ/kiD9/09Tc9hhTSwRmNnvCW603tXMSghuFtEKwN1/S3BXpq8Q3ORjO8FNMZqVAw4Inp9/Pv38hQuD5x070s9PWbsWCgqClkF17tC5M7z5phKBSHWpTgjeeQf69s1tLPuz2BKBu0+oY74T3HWq2UqVcj74AO64A4YNC7L5okXw7LNQVBTM35juVuIRH34Y/BJIxwwGDw4SQUPMmQNvvRUMH3kknHtuw9YjzUuqrNnUz8145hlo3x769Ml1JPu3JtfXUFNSVlY5PGUKHHssLFsWDM+dWznvww9rX89HH8HRR2eeX1gIt98Ou3ZB69b1i++cc+CzzyqnlZZC167Zr0Oap8WLg7Jmc3DOOdCiSZwWkztKBDGKJgKATz6BPXuCX++XXhocIzjxxLoTwYcfwuc/n3n+kCFBEliyJGgdZGvp0iAJPPggHHwwjBoVxPblL2e/DmmeUiWVt96Co47KbSx7K92JGlKVEkEDrF8PK1cGv5zXrQua0YcfDiUlwc7/00+DXyCz0ty08b33YOtWOOkkaNMmKPl8VO0Q+cKF8MorwfCePUECyVQagiARQNAqqM+BpFQ56eSTg0QAMH06vPtu9uuQ5mnWrOAstH799Gs6CZQIGuCUU2DFisrxFi2CHXY2li0Lnvv3D54PPTRIDlEXXlh5IDnlhBMyr/PII6F3b3j44eBRH4cfHpSdWrQIksgf/xg8RMaOVRJICiWCBogmAcg+CUDw6x4q6/CdOlU9WPzZZ0Fz/Ac/gB//OJjWsiV06JB5nWbBWRFbt2YfR0q7dpX/7K++Clu21H8d0jx17JjrCGRfUSKoh5074fXXG/56s8qdfufOwXOnTrB5c3CGUV4e3HVXMHzKKdClS/brbt26fsun06rV3q9DRJoeNfzq4eqr4dRT924dGzcGO/z27YPxTp2C502bYPt2uOaaYPzkk/fufUREsqUWQT009Fz9FPegNNSxY+VVjqlEsHEjLF8eDM+YAd3VD6uI7CNKBFlYtSo4w2dvlZcHO/zUzh8qS0QbN1YmmjPO2Pv3EhHJlkpDWTjqqNpP3xw3Lrv1lJXVTATRFkFxMRxySO3vJSLS2JQI6iHaJ9A11wR9AG3eDL//fVDOqUtdieDNN4OrhJtD51gi0nQoEdTD/PmVw717B7/cDzoo2HH3SHtLnap27cqcCObODa4xSF0cJiKyr+gYQR0y9Qx6+OFVx1NnAdUmdZVw9PzsLl2gbdugq2mA4cMbFqeISEMpEdShes+gU6cGnVj161d1ejaJAIJEEL2BTOvWwdlC69cHB6SPO26vwhURqTclggzWrAm6figvrzr9iCPg+ONrLp/tWUXuNZNGQUHwEBHJBSWCDEaODHrnrC5a34+qz+X42bYeRET2BR0sziDTPU4zJYJOneDpp7NbtxKBiOxPlAgy2Lo1OJWzukyJAKBXr+zWrUQgIvsTJYI0du0KHiNG1JyXuhI4nVatslt/u3YNCktEJBaxJgIzG2lmy81shZldl2b+EWb2opktNrO5ZrZfHDJNdefcs2dwf+EFCyrn1dYiyDYRqEUgIvuT2BKBmeUB04BRQD9ggplVO+mSW4GH3P0E4Cbg/8YVTyapjuAgSABbtlQeH2jfPrghzMCBlcvXdnaQEoGINEVxtgiGAivcfZW77wJmAmOqLdMPeCkcfjnN/NjdeWdwUdf77wfn93foUHl6aHSHnar/19b9gxKBiDRFcSaCHsCayHhJOC1qEXBWODwWaG9mNW6NYmaXmFmxmRWXlpY2apC/+U3wnO4G8tFa/vz5Ne9MVp0SgYg0Rbk+WPwj4FQzWwCcCqwFdldfyN2nu3uhuxd269atUQNYuTJ4rn4Deai6w+7UKeiFtDZKBCLSFMV5QdlaoGdkvCCcVsHdPyRsEZhZO+Bsd98UY0wZrVpVc1p9d9itW2e3XH5+/dYrIhKnOFsERUAfM+ttZq2B8cDs6AJm1tXMUjFcD9wfYzw1RG86n0oE0WsB6psIoi2CZcsyL6dupkVkfxJbInD3cmAy8BywDHjc3d82s5vMbHS42AhguZm9CxwC/CKueNL57LPK4VQimDSpclp9E0GLyKd57LENj0tEZF+Kta8hd58DzKk27cbI8JPAk3HGUJtPP60cTiWCE06onKZavogkQaI7ndu+vXI4ddD4uOPgT3+CAw4IHo2ta9fGX6eIyN5QIgiljhd06gSjR6dfvjEoEYjI/ibXp4/uc5deWnnWTrQ0BJCXF3856KST4l2/iEh9Ja5FMH168Lx5c2WLoFOn4E5khxzSuGf0LFoUlJfWr4djjoE33oAvfanx1i8i0hgSlwg6doRNm4KO5FJnDR19NBQVBTejb0ypA899+wbPcZacREQaKnGloQEDgucFCypLQ0cfHTzX5y5jIiLNReJaBKn7CWzaFHQ2B5WJoDG8+mrt9ywQEdnfJC4RpM4O2rWr8hjBEUc03vqHDWu8dYmI7AuJKw2VlwfPu3ZVloa6dw+ee/ZM/xoRkeYscS2C3WHfpjt3VrYIzjwT7r0Xxo3LXVwiIrmS2ESQKg21ahU8vvOd3MYlIpIriSsNRVsEn34KbdvmNh4RkVxLVougpIRHXj2Fv3EynV7rxuc+mMnPyoDGvdeNiEg8brkFJk5s9NUmKxG89x7dd77PeN5nzbrj2NayI6+0+xLjzst1YCIiWajrNokNlKxEkDplCGhTtoXXD/oSv+03jXHTchiTiEiOJesYQVlZxeCBZZv4pPwgOnXKYTwiIvuBZCWCaItg96d8vKu9EoGIJF6yEkGkRQDw8Q4lAhGRZCWCSIsAYEP5QeoXSEQSL9ZEYGYjzWy5ma0ws+vSzD/czF42swVmttjMvhJnPNVbBFtRi0BEJLZEYGZ5wDRgFNAPmGBm/aotdgPwuLsPAsYDd8cVD1CjRaBEICISb4tgKLDC3Ve5+y5gJjCm2jIOHBQOdwA+jDGeGolgCwfRoUOs7ygist+LMxH0ANZExkvCaVFTgW+aWQkwB/h+uhWZ2SVmVmxmxaWlpQ2PKE1pqF27hq9ORKQ5yPXB4gnADHcvAL4CPGxmNWJy9+nuXujuhd267UV/EGlKQ3HfrF5EZH8XZyJYC0R7+C8Ip0VdDDwO4O6vAflA19giqtYiKKWbEoGIJF6ciaAI6GNmvc2sNcHB4NnVlvkAOB3AzI4jSAR7UfupQ9gi6M9ijmUZm+ikRCAiiRdbX0PuXm5mk4HngDzgfnd/28xuAordfTbwQ+BeM7uK4MDxRHf3uGJKtQiW05cyWgMoEYhI4sXa6Zy7zyE4CByddmNkeCkwPM4YqghbBOWRzc7P32fvLiKyX8r1weJ9q6yM3bSA8Hh0ixZgluOYRERyLFmJoLycMlrRpk0w2rp1bsMREdkfJCsRlJVRTksOOywY3bEjt+GIiOwPkpUIyssppyVDh+Y6EBGR/UeiEoHvKqOMVvTpk+tIRET2H4m6VaWXBS2Cli3h6qvh8MNzHZGISO4lKhHsKQsOFuflwa9+letoRET2D4kqDbErOFicl5frQERE9h+JSgQeaRGIiEggWYkgbBG0TFRBTESkdslKBOVqEYiIVJesRKBjBCIiNSQqEaAWgYhIDclKBGoRiIjUkKxEELYIdLBYRKRSohKBl6lFICJSXaISQarTOSUCEZFKyUoEZWU6WCwiUk2iEoGVl+uCMhGRamJNBGY20syWm9kKM7suzfzbzGxh+HjXzDbFFswdd5C/8m120VotAhGRiDoTgZl93czqnTDMLA+YBowC+gETzKxfdBl3v8rdB7r7QOA3wB/r+z5ZO+EE1k24itu4SolARCQimx38OOA9M/uVmR1bj3UPBVa4+yp33wXMBMbUsvwE4Pf1WH/9nHYaq6/4NUUMVSIQEYmoMxG4+zeBQcBKYIaZvWZml5hZ+zpe2gNYExkvCafVYGZHAL2BlzLMv8TMis2suLS0tK6QMyoqCp6VCEREKmVV8nH3LcCTBL/quwNjgflm9v1GimM88KS7787w/tPdvdDdC7t169bgN7n11uC5e/cGr0JEpNnJ5hjBaDN7CpgLtAKGuvsoYADww1peuhboGRkvCKelM544y0Khli1h5Ejo3z/udxIRaTqyOZHybOA2d/97dKK7bzezi2t5XRHQx8x6EySA8cD51RcKjzt0Al7LOuq90KXLvngXEZGmI5vS0FTgjdSImbUxs14A7v5iphe5ezkwGXgOWAY87u5vm9lNZjY6suh4YKa7e72jryd3MIv7XUREmpZsWgRPAMMi47vDaSfW9UJ3nwPMqTbtxmrjU7OIoVG4Q4tEXUInIlK3bHaLLcPTPwEIh1vHF1J89uxRi0BEpLpsEkFptJRjZmOAj+MLKT4qDYmI1JRNaegy4HdmdhdgBNcGfDvWqGKi0pCISE11JgJ3XwmcbGbtwvFtsUcVE5WGRERqyqofTjP7KnA8kG/hntTdb4oxrlioRSAiUlM2F5T9lqC/oe8TlIbOBY6IOa5YqEUgIlJTNr+Ph7n7t4GN7v6fwOeAY+INKx46WCwiUlM2iWBH+LzdzA4Dygj6G2pyVBoSEakpm2MEfzazjsAtwHzAgXtjjSomKg2JiNRUayIIb0jzortvAv5gZk8D+e6+eZ9E18hUGhIRqanWQom77yG4y1hqfGdTTQKg0pCISDrZ7BZfNLOzzZr+b2mVhkREasomEVxK0MncTjPbYmZbzWxLzHHFQqUhEZGasrmyuK5bUjYZKg2JiNRUZyIwsy+km179RjVNgUpDIiI1ZXP66NWR4XxgKPAm8MVYIoqRSkMiIjVlUxr6enTczHoCt8cWUYxUGhIRqakhu8US4LjGDmRfUGlIRKSmbI4R/IbgamIIEsdAgiuMmxyVhkREasqmRVBMcEzgTeA14Fp3/2Y2KzezkWa23MxWmNl1GZY5z8yWmtnbZvZo1pE3gEpDIiI1ZXOw+Elgh7vvBjCzPDNr6+7ba3uRmeURXJV8JkE5qcjMZrv70sgyfYDrgeHuvtHMDm7ohmRDpSERkZqyurIYaBMZbwO8kMXrhgIr3H1VeMP7mcCYast8F5jm7hsB3P3fWay3wVQaEhGpKZtEkB+9PWU43DaL1/UguL9xSkk4LeoY4Bgze9XM/mlmI9OtyMwuMbNiMysuLS3N4q3TU2lIRKSmbHaLn5rZ4NSImQ0BPmuk928J9AFGABOAe8Mur6tw9+nuXujuhd26dWvwm6lFICJSUzbHCKYAT5jZhwS3qjyU4NaVdVkL9IyMF4TTokqA1929DPiXmb1LkBiKslh/vXh43pNaBCIiVWVzQVmRmR0L9A0nLQ933HUpAvqYWW+CBDAeOL/aMrMIWgIPmFlXglLRqmyDr489e4JntQhERKrK5ub1lwMHuvsSd18CtDOz79X1OncvByYDzwHLgMfd/W0zu8nMRoeLPQdsMLOlwMvA1e6+oaEbU3s8wbMSgYhIVdmUhr7r7tGb02w0s+8Cd9f1QnefA8ypNu3GyLADPwgfsVJpSEQkvWx2i3nRm9KE1we0ji+keKg0JCKSXjYtgmeBx8zsf8LxS4Fn4gspHioNiYikl00iuBa4BLgsHF9McOZQk6LSkIhIenXuFsMb2L8OrCa4WviLBAd/mxSVhkRE0svYIjCzYwhO7ZwAfAw8BuDup+2b0BqXSkMiIunVVhp6B3gF+Jq7rwAws6v2SVQxUGlIRCS92naLZwEfAS+b2b1mdjrBlcVNkkpDIiLpZUwE7j7L3ccDxxJc7DUFONjM7jGzL+2rABuLSkMiIullc7D4U3d/NLx3cQGwgOBMoiZFpSERkfTqtVt0941hT6CnxxVQXFQaEhFJLzG/j1UaEhFJL3GJQKUhEZGqErNbVGlIRCS9xCQClYZERNJLXCJQaUhEpKrE7BZVGhIRSS8xiUAtAhGR9BKzW1SLQEQkvcQkAh0sFhFJL9ZEYGYjzWy5ma0ws+vSzJ9oZqVmtjB8fCeuWFQaEhFJL5s7lDVIeG/jacCZQAlQZGaz3X1ptUUfc/fJccWRotKQiEh6cf4+HgqscPdV7r4LmAmMifH9aqXSkIhIenEmgh7Amsh4STiturPNbLGZPWlmPdOtyMwuMbNiMysuLS1tUDAqDYmIpJfr3eKfgV7ufgLwPPBguoXCHk8L3b2wW7duDXojlYZERNKLMxGsBaK/8AvCaRXcfYO77wxH/xcYElcwKg2JiKQXZyIoAvqYWW8zaw2MB2ZHFzCz7pHR0cCyuIJRaUhEJL3Yzhpy93Izmww8B+QB97v722Z2E1Ds7rOBK8xsNFAOfAJMjCselYZERNKLLREAuPscYE61aTdGhq8Hro8zhsr3Cp6VCEREqkpMoUSlIRGR9BKzW1RpSEQkvcQkApWGRETSS1wiUGlIRKSqxOwWVRoSEUkvMYlApSERkfQSlwhUGhIRqSoxu0WVhkRE0ktMIlCLQEQkvcTsFtUiEBFJLzGJQAeLRUTSS1wiUGlIRKSqxOwWVRoSEUkvMYlApSERkfQSlwhUGhIRqSoxu0WVhkRE0ktMIlBpSEQkvcQlApWGRESqSsxuUaUhEZH0Yk0EZjbSzJab2Qozu66W5c42MzezwrhiUWlIRCS92BKBmeUB04BRQD9ggpn1S7Nce+BK4PW4YgGVhkREMolztzgUWOHuq9x9FzATGJNmuZ8DNwM7YoxFpSERkQziTAQ9gDWR8ZJwWgUzGwz0dPe/1LYiM7vEzIrNrLi0tLRBwag0JCKSXs4KJWbWAvg18MO6lnX36e5e6O6F3bp1a9D7qTQkIpJenLvFtUDPyHhBOC2lPfAfwFwzWw2cDMyO64CxSkMiIunFmQiKgD5m1tvMWgPjgdmpme6+2d27unsvd+8F/BMY7e7FcQSj0pCISHqxJQJ3LwcmA88By4DH3f1tM7vJzEbH9b6Z4wmeVRoSEamqZZwrd/c5wJxq027MsOyIOGNRaUiao7KyMkpKStixI9aT7qQJyc/Pp6CggFatWmX9mlgTwf5EpSFpjkpKSmjfvj29evXC9OVOPHdnw4YNlJSU0Lt376xfl5hCSapFoNKQNCc7duygS5cuSgICgJnRpUuXercQE7NbVItAmislAYlqyPchcYlALQIRkaoSs1vUwWKRxrdhwwYGDhzIwIEDOfTQQ+nRo0fF+K5du2p9bXFxMVdccUWd7zFs2LDGClcy0MFiEWmwLl26sHDhQgCmTp1Ku3bt+NGPflQxv7y8nJYt0+9mCgsLKSys+/rRefPmNU6w+9Du3bvJy8vLdRhZS1wiUGlImqspUyDcJzeagQPh9tvr95qJEyeSn5/PggULGD58OOPHj+fKK69kx44dtGnThgceeIC+ffsyd+5cbr31Vp5++mmmTp3KBx98wKpVq/jggw+YMmVKRWuhXbt2bNu2jblz5zJ16lS6du3KkiVLGDJkCI888ghmxpw5c/jBD37AgQceyPDhw1m1ahVPP/10lbhWr17Nt771LT799FMA7rrrrorWxs0338wjjzxCixYtGDVqFL/85S9ZsWIFl112GaWlpeTl5fHEE0+wZs2aipgBJk+eTGFhIRMnTqRXr16MGzeO559/nmuuuYatW7cyffp0du3axdFHH83DDz9M27ZtWb9+PZdddhmrVq0C4J577uHZZ5+lc+fOTJkyBYCf/OQnHHzwwVx55ZUN/tvVR2ISgUpDIvtOSUkJ8+bNIy8vjy1btvDKK6/QsmVLXnjhBX784x/zhz/8ocZr3nnnHV5++WW2bt1K3759mTRpUo1z4RcsWMDbb7/NYYcdxvDhw3n11VcpLCzk0ksv5e9//zu9e/dmwoQJaWM6+OCDef7558nPz+e9995jwoQJFBcX88wzz/CnP/2J119/nbZt2/LJJ58A8D2wmI8AAA8KSURBVI1vfIPrrruOsWPHsmPHDvbs2cOaNWvSrjulS5cuzJ8/HwjKZt/97ncBuOGGG7jvvvv4/ve/zxVXXMGpp57KU089xe7du9m2bRuHHXYYZ511FlOmTGHPnj3MnDmTN954o96fe0MlJhGoNCTNXX1/ucfp3HPPrSiNbN68mQsuuID33nsPM6OsrCzta7761a9ywAEHcMABB3DwwQezfv16CgoKqiwzdOjQimkDBw5k9erVtGvXjiOPPLLivPkJEyYwffr0GusvKytj8uTJLFy4kLy8PN59910AXnjhBS688ELatm0LQOfOndm6dStr165l7NixQHCRVjbGjRtXMbxkyRJuuOEGNm3axLZt2/jyl78MwEsvvcRDDz0EQF5eHh06dKBDhw506dKFBQsWsH79egYNGkSXLl2yes/GkLhEoNKQSPwOPPDAiuGf/vSnnHbaaTz11FOsXr2aESNGpH3NAQccUDGcl5dHeXl5g5bJ5LbbbuOQQw5h0aJF7NmzJ+ude1TLli3ZkyovQI3z9aPbPXHiRGbNmsWAAQOYMWMGc+fOrXXd3/nOd5gxYwbr1q3joosuqndseyMxu0WVhkRyY/PmzfToEdyKZMaMGY2+/r59+7Jq1SpWr14NwGOPPZYxju7du9OiRQsefvhhdu/eDcCZZ57JAw88wPbt2wH45JNPaN++PQUFBcyaNQuAnTt3sn37do444giWLl3Kzp072bRpEy+++GLGuLZu3Ur37t0pKyvjd7/7XcX0008/nXvuuQcIDipv3rwZgLFjx/Lss89SVFRU0XrYVxKTCFQaEsmNa665huuvv55BgwbV6xd8ttq0acPdd9/NyJEjGTJkCO3bt6dDhw41lvve977Hgw8+yIABA3jnnXcqfr2PHDmS0aNHU1hYyMCBA7n11lsBePjhh7nzzjs54YQTGDZsGOvWraNnz56cd955/Md//AfnnXcegwYNyhjXz3/+c0466SSGDx/OscceWzH9jjvu4OWXX6Z///4MGTKEpUuXAtC6dWtOO+00zjvvvH1+xpF5ag/ZRBQWFnpxcf17qr7/frj4Ynj/fTj88BgCE8mBZcuWcdxxx+U6jJzbtm0b7dq1w925/PLL6dOnD1dddVWuw6qXPXv2MHjwYJ544gn69OmzV+tK970wszfdPe35uolpEag0JNJ83XvvvQwcOJDjjz+ezZs3c+mll+Y6pHpZunQpRx99NKeffvpeJ4GGSNzBYiUCkebnqquuanItgKh+/fpVXFeQC4lpEeisIRGR9BKzW1RpSEQkvcQkApWGRETSS1wiUGlIRKSqWHeLZjbSzJab2Qozuy7N/MvM7C0zW2hm/zCzfnHFotKQSOM77bTTeO6556pMu/3225k0aVLG14wYMYLUKeBf+cpX2LRpU41lpk6dWnE+fyazZs2qOAcf4MYbb+SFF16oT/gSii0RmFkeMA0YBfQDJqTZ0T/q7v3dfSDwK+DXccWj0pBI45swYQIzZ86sMm3mzJkZO36rbs6cOXTs2LFB7109Edx0002cccYZDVpXrqSubs61OFsEQ4EV7r7K3XcBM4Ex0QXcfUtk9EAgtqvbdM9iafamTIERIxr3EXaLnMk555zDX/7yl4qb0KxevZoPP/yQz3/+80yaNInCwkKOP/54fvazn6V9fa9evfj4448B+MUvfsExxxzDKaecwvLlyyuWuffeeznxxBMZMGAAZ599Ntu3b2fevHnMnj2bq6++moEDB7Jy5UomTpzIk08+CcCLL77IoEGD6N+/PxdddBE7d+6seL+f/exnDB48mP79+/POO+/UiGn16tV8/vOfZ/DgwQwePLjK/RBuvvlm+vfvz4ABA7juuqDIsWLFCs444wwGDBjA4MGDWblyJXPnzuVrX/taxesmT55c0b1Gr169uPbaaysuHku3fQDr169n7NixDBgwgAEDBjBv3jxuvPFGbo/0LviTn/yEO+64o9a/UTbi3C32AKJ9tpaE06ows8vNbCVBiyDt7YrM7BIzKzaz4tLS0gYFoxaBSOPr3LkzQ4cO5ZlnngGC1sB5552HmfGLX/yC4uJiFi9ezN/+9jcWL16ccT1vvvkmM2fOZOHChcyZM4eioqKKeWeddRZFRUUsWrSI4447jvvuu49hw4YxevRobrnlFhYuXMhRRx1VsfyOHTuYOHEijz32GG+99Rbl5eUVffsAdO3alfnz5zNp0qS05adUd9Xz58/nscceq7gvQrS76kWLFnHNNdcAQXfVl19+OYsWLWLevHl07969zs8t1V31+PHj024fUNFd9aJFi5g/fz7HH388F110UUXPpanuqr/5zW/W+X51yfkFZe4+DZhmZucDNwAXpFlmOjAdgi4mGvY+wbNaBNJs5agf6lR5aMyYMcycObNiR/b4448zffp0ysvL+eijj1i6dCknnHBC2nW88sorjB07tqIr6NGjR1fMy9SdcybLly+nd+/eHHPMMQBccMEFTJs2reKmL2eddRYAQ4YM4Y9//GON1yexu+o4E8FaoGdkvCCclslM4J5a5u8VHSwWiceYMWO46qqrmD9/Ptu3b2fIkCH861//4tZbb6WoqIhOnToxceLEGl02Z6u+3TnXJdWVdaZurJPYXXWcv4+LgD5m1tvMWgPjgdnRBcws2qnGV4H34gpGpSGReLRr147TTjuNiy66qOIg8ZYtWzjwwAPp0KED69evrygdZfKFL3yBWbNm8dlnn7F161b+/Oc/V8zL1J1z+/bt2bp1a4119e3bl9WrV7NixQog6EX01FNPzXp7kthddWyJwN3LgcnAc8Ay4HF3f9vMbjKzVLtvspm9bWYLgR+QpizUePEEzyoNiTS+CRMmsGjRoopEMGDAAAYNGsSxxx7L+eefz/Dhw2t9/eDBgxk3bhwDBgxg1KhRnHjiiRXzMnXnPH78eG655RYGDRrEypUrK6bn5+fzwAMPcO6559K/f39atGjBZZddlvW2JLG76sR0Q/2nP8EjjwSPyE2ORJo0dUOdPNl0V61uqDMYMwaeeEJJQESarri6q875WUMiIpKduLqrTkyLQKS5amrlXYlXQ74PSgQiTVh+fj4bNmxQMhAgSAIbNmyo9ymvKg2JNGEFBQWUlJTQ0CvupfnJz8+noKCgXq9RIhBpwlq1akXv3r1zHYY0cSoNiYgknBKBiEjCKRGIiCRck7uy2MxKgfcb+PKuwMeNGE5ToG1OBm1zMuzNNh/h7t3SzWhyiWBvmFlxpkusmyttczJom5Mhrm1WaUhEJOGUCEREEi5piWB6rgPIAW1zMmibkyGWbU7UMQIREakpaS0CERGpRolARCThEpEIzGykmS03sxVmdl2u42ksZna/mf3bzJZEpnU2s+fN7L3wuVM43czszvAzWGxmg3MXecOZWU8ze9nMloa3Ob0ynN5st9vM8s3sDTNbFG7zf4bTe5vZ6+G2PRbeGxwzOyAcXxHO75XL+PeGmeWZ2QIzezocb9bbbGarzewtM1toZsXhtNi/280+EZhZHjANGAX0AyaYWb/cRtVoZgAjq027DnjR3fsAL4bjEGx/n/BxCXDPPoqxsZUDP3T3fsDJwOXh37M5b/dO4IvuPgAYCIw0s5OBm4Hb3P1oYCNwcbj8xcDGcPpt4XJN1ZUE9zxPScI2n+buAyPXC8T/3Xb3Zv0APgc8Fxm/Hrg+13E14vb1ApZExpcD3cPh7sDycPh/gAnplmvKD+BPwJlJ2W6gLTAfOIngCtOW4fSK7znwHPC5cLhluJzlOvYGbGtBuOP7IvA0YAnY5tVA12rTYv9uN/sWAdADWBMZLwmnNVeHuPtH4fA64JBwuNl9DmHzfxDwOs18u8MSyULg38DzwEpgk7uXh4tEt6tim8P5m4Eu+zbiRnE7cA2wJxzvQvPfZgf+amZvmtkl4bTYv9u6H0Ez5u5uZs3y/GAzawf8AZji7lvMrGJec9xud98NDDSzjsBTwLE5DilWZvY14N/u/qaZjch1PPvQKe6+1swOBp43s3eiM+P6biehRbAW6BkZLwinNVfrzaw7QPj873B6s/kczKwVQRL4nbv/MZzc7LcbwN03AS8TlEU6mlnqx1x0uyq2OZzfAdiwj0PdW8OB0Wa2GphJUB66g+a9zbj72vD53wQJfyj74LudhERQBPQJzzZoDYwHZuc4pjjNBi4Ihy8gqKGnpn87PNPgZGBzpLnZZFjw0/8+YJm7/zoyq9lut5l1C1sCmFkbgmMiywgSwjnhYtW3OfVZnAO85GERualw9+vdvcDdexH8z77k7t+gGW+zmR1oZu1Tw8CXgCXsi+92rg+O7KMDMF8B3iWoq/4k1/E04nb9HvgIKCOoD15MUBd9EXgPeAHoHC5rBGdPrQTeAgpzHX8Dt/kUgjrqYmBh+PhKc95u4ARgQbjNS4Abw+lHAm8AK4AngAPC6fnh+Ipw/pG53oa93P4RwNPNfZvDbVsUPt5O7av2xXdbXUyIiCRcEkpDIiJSCyUCEZGEUyIQEUk4JQIRkYRTIhARSTglApGQme0Oe31MPRqtp1oz62WRXmJF9ifqYkKk0mfuPjDXQYjsa2oRiNQh7CP+V2E/8W+Y2dHh9F5m9lLYF/yLZnZ4OP0QM3sqvH/AIjMbFq4qz8zuDe8p8NfwKmHM7AoL7q+w2Mxm5mgzJcGUCEQqtalWGhoXmbfZ3fsDdxH0ignwG+BBdz8B+B1wZzj9TuBvHtw/YDDBVaIQ9Bs/zd2PBzYBZ4fTrwMGheu5LK6NE8lEVxaLhMxsm7u3SzN9NcGNYVaFHd6tc/cuZvYxQf/vZeH0j9y9q5mVAgXuvjOyjl7A8x7cXAQzuxZo5e7/ZWbPAtuAWcAsd98W86aKVKEWgUh2PMNwfeyMDO+m8hjdVwn6jBkMFEV61xTZJ5QIRLIzLvL8Wjg8j6BnTIBvAK+Ewy8Ck6DihjIdMq3UzFoAPd39ZeBagu6Ta7RKROKkXx4ildqEdwFLedbdU6eQdjKzxQS/6ieE074PPGBmVwOlwIXh9CuB6WZ2McEv/0kEvcSmkwc8EiYLA+704J4DIvuMjhGI1CE8RlDo7h/nOhaROKg0JCKScGoRiIgknFoEIiIJp0QgIpJwSgQiIgmnRCAiknBKBCIiCff/Aca4uy0C/1s/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0f7Lx6xFl36"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8cee8148-bab1-4d55-a961-a40257911f64",
        "id": "5H6LrO6jFl3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, train_labels_dec, epochs= num_epochs, batch_size=92, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, test_labels_dec)\n",
        "  "
      ],
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 891us/step - loss: 2.5823 - accuracy: 0.6522\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 2.5669 - accuracy: 0.6957\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 1.9312 - accuracy: 0.8152\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 1.9614 - accuracy: 0.4457\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 1.4493 - accuracy: 0.8370\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 1.4102 - accuracy: 0.7609\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 1.2656 - accuracy: 0.7609\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 1.0231 - accuracy: 0.8152\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.9557 - accuracy: 0.9130\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.9590 - accuracy: 0.8478\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.8310 - accuracy: 0.9130\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.7735 - accuracy: 0.8696\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.7947 - accuracy: 0.8152\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.7783 - accuracy: 0.8152\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7168 - accuracy: 0.8696\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6826 - accuracy: 0.9130\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6953 - accuracy: 0.9348\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6782 - accuracy: 0.9348\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6243 - accuracy: 0.9348\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6042 - accuracy: 0.8913\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.5989 - accuracy: 0.8587\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5595 - accuracy: 0.8913\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5316 - accuracy: 0.9348\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5256 - accuracy: 0.9457\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4916 - accuracy: 0.9457\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.4712 - accuracy: 0.9239\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.4614 - accuracy: 0.8913\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4405 - accuracy: 0.9130\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4253 - accuracy: 0.9674\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.4194 - accuracy: 0.9565\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4037 - accuracy: 0.9674\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3947 - accuracy: 0.9239\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3903 - accuracy: 0.9130\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3784 - accuracy: 0.9348\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3727 - accuracy: 0.9674\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3681 - accuracy: 0.9674\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3584 - accuracy: 0.9565\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3548 - accuracy: 0.9348\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3459 - accuracy: 0.9565\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3437 - accuracy: 0.9783\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3373 - accuracy: 0.9348\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3251 - accuracy: 0.9674\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3193 - accuracy: 0.9674\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3142 - accuracy: 0.9457\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3077 - accuracy: 0.9674\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3048 - accuracy: 0.9891\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2981 - accuracy: 0.9565\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2936 - accuracy: 0.9565\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2923 - accuracy: 0.9891\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2864 - accuracy: 0.9674\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2826 - accuracy: 0.9674\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2815 - accuracy: 0.9891\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2771 - accuracy: 0.9674\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2727 - accuracy: 0.9674\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2713 - accuracy: 0.9891\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2703 - accuracy: 0.9565\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2676 - accuracy: 0.9891\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2639 - accuracy: 0.9674\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2601 - accuracy: 0.9891\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2566 - accuracy: 0.9674\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2536 - accuracy: 0.9891\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2510 - accuracy: 0.9674\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2485 - accuracy: 0.9891\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2461 - accuracy: 0.9891\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2439 - accuracy: 0.9891\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2416 - accuracy: 0.9891\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2395 - accuracy: 0.9783\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2385 - accuracy: 0.9891\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2488 - accuracy: 0.9457\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3666 - accuracy: 0.9130\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.6470 - accuracy: 0.7500\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3976 - accuracy: 0.8152\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5596 - accuracy: 0.7609\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3137 - accuracy: 0.9783\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3989 - accuracy: 0.8261\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.4318 - accuracy: 0.8152\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3126 - accuracy: 0.9457\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.4178 - accuracy: 0.9239\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3463 - accuracy: 0.9674\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3348 - accuracy: 0.8804\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3865 - accuracy: 0.8478\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3231 - accuracy: 0.9022\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3228 - accuracy: 0.9674\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3516 - accuracy: 0.9674\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2914 - accuracy: 0.9565\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3038 - accuracy: 0.9022\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2903 - accuracy: 0.9457\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2750 - accuracy: 0.9674\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2938 - accuracy: 0.9674\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2672 - accuracy: 0.9674\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2800 - accuracy: 0.9348\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2684 - accuracy: 0.9457\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2665 - accuracy: 0.9783\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2668 - accuracy: 0.9783\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2587 - accuracy: 0.9565\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2630 - accuracy: 0.9457\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2589 - accuracy: 0.9891\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2538 - accuracy: 0.9891\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2591 - accuracy: 0.9457\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2462 - accuracy: 0.9891\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2536 - accuracy: 0.9891\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2452 - accuracy: 0.9783\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2446 - accuracy: 0.9674\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2446 - accuracy: 0.9891\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2384 - accuracy: 0.9891\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2418 - accuracy: 0.9674\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2353 - accuracy: 0.9891\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2375 - accuracy: 0.9891\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2342 - accuracy: 0.9674\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2321 - accuracy: 0.9674\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2327 - accuracy: 0.9891\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2280 - accuracy: 0.9891\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2288 - accuracy: 0.9674\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2263 - accuracy: 0.9891\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2237 - accuracy: 0.9891\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2243 - accuracy: 0.9783\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2216 - accuracy: 0.9891\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2196 - accuracy: 0.9891\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2199 - accuracy: 0.9891\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2182 - accuracy: 0.9891\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2162 - accuracy: 0.9891\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2157 - accuracy: 0.9891\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2150 - accuracy: 0.9891\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2136 - accuracy: 0.9891\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2126 - accuracy: 0.9891\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2121 - accuracy: 0.9891\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2117 - accuracy: 0.9891\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2124 - accuracy: 0.9891\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2147 - accuracy: 0.9674\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2186 - accuracy: 0.9891\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2280 - accuracy: 0.9457\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2319 - accuracy: 0.9783\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2335 - accuracy: 0.9348\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2173 - accuracy: 0.9891\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2045 - accuracy: 0.9891\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2046 - accuracy: 0.9783\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2131 - accuracy: 0.9891\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2177 - accuracy: 0.9565\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2093 - accuracy: 0.9891\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2006 - accuracy: 0.9891\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1997 - accuracy: 0.9891\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2052 - accuracy: 0.9891\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2109 - accuracy: 0.9674\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2102 - accuracy: 0.9891\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2073 - accuracy: 0.9783\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2010 - accuracy: 0.9891\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1960 - accuracy: 0.9891\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1932 - accuracy: 0.9891\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1931 - accuracy: 0.9891\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1952 - accuracy: 0.9891\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2004 - accuracy: 0.9891\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2193 - accuracy: 0.9457\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2634 - accuracy: 0.9674\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3853 - accuracy: 0.8152\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2779 - accuracy: 0.9674\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2033 - accuracy: 0.9674\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2218 - accuracy: 0.9565\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2435 - accuracy: 0.9783\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2100 - accuracy: 0.9674\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2136 - accuracy: 0.9674\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2284 - accuracy: 0.9783\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2021 - accuracy: 0.9891\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2072 - accuracy: 0.9783\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2143 - accuracy: 0.9783\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1945 - accuracy: 0.9891\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1961 - accuracy: 0.9891\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2058 - accuracy: 0.9891\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1960 - accuracy: 0.9891\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1856 - accuracy: 0.9891\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1941 - accuracy: 0.9891\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2023 - accuracy: 0.9674\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1959 - accuracy: 0.9891\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1879 - accuracy: 0.9891\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1817 - accuracy: 0.9891\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1821 - accuracy: 0.9891\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1859 - accuracy: 0.9891\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1868 - accuracy: 0.9891\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1858 - accuracy: 0.9783\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1816 - accuracy: 0.9891\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1782 - accuracy: 0.9891\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1779 - accuracy: 0.9891\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1799 - accuracy: 0.9891\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1828 - accuracy: 0.9891\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1850 - accuracy: 0.9891\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1884 - accuracy: 0.9783\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1901 - accuracy: 0.9891\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1941 - accuracy: 0.9783\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1936 - accuracy: 0.9891\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1952 - accuracy: 0.9565\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1896 - accuracy: 0.9891\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1831 - accuracy: 0.9783\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1750 - accuracy: 0.9891\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1713 - accuracy: 0.9891\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1736 - accuracy: 0.9891\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1783 - accuracy: 0.9891\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1838 - accuracy: 0.9783\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1871 - accuracy: 0.9891\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1934 - accuracy: 0.9674\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1967 - accuracy: 0.9783\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2082 - accuracy: 0.9565\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2118 - accuracy: 0.9783\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2245 - accuracy: 0.9348\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2072 - accuracy: 0.9783\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1847 - accuracy: 0.9783\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1685 - accuracy: 0.9891\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1806 - accuracy: 0.9891\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1940 - accuracy: 0.9674\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1818 - accuracy: 0.9891\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1676 - accuracy: 0.9891\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1709 - accuracy: 0.9891\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1815 - accuracy: 0.9891\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1854 - accuracy: 0.9783\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1764 - accuracy: 0.9891\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1668 - accuracy: 0.9891\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1625 - accuracy: 0.9891\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 140us/step - loss: 0.1653 - accuracy: 0.9891\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1725 - accuracy: 1.0000\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1810 - accuracy: 0.9891\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1988 - accuracy: 0.9565\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2223 - accuracy: 0.9783\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3046 - accuracy: 0.8370\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3496 - accuracy: 0.9022\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5550 - accuracy: 0.7935\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1851 - accuracy: 0.9783\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.4124 - accuracy: 0.8804\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.6056 - accuracy: 0.7935\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3142 - accuracy: 0.8696\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.5864 - accuracy: 0.7609\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2492 - accuracy: 0.9674\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.4897 - accuracy: 0.8370\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2697 - accuracy: 0.9674\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.4078 - accuracy: 0.9239\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3006 - accuracy: 0.9783\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2999 - accuracy: 0.9457\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3437 - accuracy: 0.9022\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2593 - accuracy: 0.9891\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3264 - accuracy: 0.9674\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2644 - accuracy: 0.9783\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2697 - accuracy: 0.9565\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2791 - accuracy: 0.9565\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2357 - accuracy: 0.9891\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2762 - accuracy: 0.9674\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2191 - accuracy: 0.9891\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2599 - accuracy: 0.9783\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2090 - accuracy: 0.9891\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2430 - accuracy: 0.9783\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1988 - accuracy: 0.9891\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2285 - accuracy: 0.9783\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1911 - accuracy: 0.9891\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2177 - accuracy: 0.9891\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1901 - accuracy: 0.9891\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2019 - accuracy: 0.9783\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1916 - accuracy: 0.9891\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1858 - accuracy: 0.9891\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1932 - accuracy: 0.9783\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1762 - accuracy: 0.9891\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1909 - accuracy: 0.9891\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1735 - accuracy: 0.9891\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1809 - accuracy: 0.9891\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1754 - accuracy: 0.9891\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1707 - accuracy: 0.9891\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1762 - accuracy: 0.9783\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1659 - accuracy: 0.9891\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1729 - accuracy: 0.9891\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1669 - accuracy: 0.9891\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1660 - accuracy: 0.9891\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1685 - accuracy: 0.9891\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1619 - accuracy: 0.9891\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1659 - accuracy: 0.9891\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1624 - accuracy: 0.9891\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1614 - accuracy: 0.9891\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1632 - accuracy: 0.9891\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1592 - accuracy: 0.9891\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1612 - accuracy: 0.9891\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1600 - accuracy: 0.9891\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1579 - accuracy: 0.9891\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1598 - accuracy: 0.9891\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1573 - accuracy: 0.9891\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1567 - accuracy: 0.9891\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1575 - accuracy: 0.9891\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1558 - accuracy: 0.9891\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1555 - accuracy: 0.9891\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1556 - accuracy: 0.9891\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1545 - accuracy: 0.9891\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1542 - accuracy: 0.9891\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1538 - accuracy: 0.9891\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1526 - accuracy: 0.9891\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1525 - accuracy: 0.9891\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1522 - accuracy: 0.9891\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1508 - accuracy: 0.9891\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1505 - accuracy: 0.9891\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1503 - accuracy: 0.9891\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1498 - accuracy: 0.9891\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1489 - accuracy: 0.9891\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1483 - accuracy: 0.9891\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1480 - accuracy: 0.9891\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1477 - accuracy: 0.9891\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1471 - accuracy: 0.9891\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1465 - accuracy: 0.9891\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1459 - accuracy: 0.9891\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1456 - accuracy: 0.9891\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1452 - accuracy: 0.9891\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1448 - accuracy: 0.9891\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1445 - accuracy: 0.9891\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1441 - accuracy: 0.9891\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1437 - accuracy: 0.9891\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1432 - accuracy: 0.9891\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1429 - accuracy: 0.9891\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1426 - accuracy: 0.9891\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1423 - accuracy: 0.9891\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1419 - accuracy: 0.9891\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1415 - accuracy: 0.9891\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1411 - accuracy: 0.9891\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1408 - accuracy: 0.9891\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1405 - accuracy: 0.9891\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1401 - accuracy: 0.9891\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 88us/step - loss: 0.1398 - accuracy: 0.9891\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1394 - accuracy: 0.9891\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1391 - accuracy: 0.9891\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1388 - accuracy: 0.9891\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1386 - accuracy: 0.9891\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1383 - accuracy: 0.9891\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1380 - accuracy: 0.9891\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1379 - accuracy: 0.9891\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1376 - accuracy: 0.9891\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1376 - accuracy: 0.9891\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1375 - accuracy: 0.9891\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1376 - accuracy: 0.9891\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1376 - accuracy: 0.9891\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1379 - accuracy: 1.0000\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1385 - accuracy: 0.9891\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1399 - accuracy: 1.0000\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1426 - accuracy: 0.9891\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1485 - accuracy: 1.0000\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1597 - accuracy: 0.9891\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1913 - accuracy: 0.9565\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2630 - accuracy: 0.9348\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6305 - accuracy: 0.7717\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4176 - accuracy: 0.8478\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3164 - accuracy: 0.8478\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1839 - accuracy: 0.9674\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3160 - accuracy: 0.9239\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1941 - accuracy: 0.9674\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2916 - accuracy: 0.8804\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2034 - accuracy: 0.9783\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2890 - accuracy: 0.9674\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1973 - accuracy: 0.9783\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2942 - accuracy: 0.8804\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1923 - accuracy: 0.9891\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2859 - accuracy: 0.9674\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1825 - accuracy: 0.9891\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2923 - accuracy: 0.8804\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1822 - accuracy: 0.9891\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2924 - accuracy: 0.9348\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1984 - accuracy: 0.9891\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2829 - accuracy: 0.8804\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2166 - accuracy: 0.9783\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2573 - accuracy: 0.9674\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2435 - accuracy: 0.9130\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2151 - accuracy: 0.9674\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2282 - accuracy: 0.9783\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1938 - accuracy: 0.9783\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2022 - accuracy: 0.9674\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1874 - accuracy: 0.9891\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1868 - accuracy: 0.9891\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1861 - accuracy: 0.9891\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1750 - accuracy: 0.9891\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1801 - accuracy: 0.9891\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1701 - accuracy: 0.9891\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1715 - accuracy: 0.9891\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1683 - accuracy: 1.0000\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1586 - accuracy: 1.0000\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1686 - accuracy: 0.9891\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1455 - accuracy: 0.9891\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1672 - accuracy: 1.0000\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1403 - accuracy: 0.9891\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1605 - accuracy: 0.9891\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1484 - accuracy: 1.0000\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1441 - accuracy: 1.0000\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1565 - accuracy: 0.9891\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1377 - accuracy: 1.0000\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1473 - accuracy: 1.0000\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1465 - accuracy: 0.9891\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1356 - accuracy: 0.9891\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1457 - accuracy: 1.0000\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1393 - accuracy: 0.9891\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1359 - accuracy: 0.9891\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1427 - accuracy: 1.0000\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1357 - accuracy: 0.9891\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1358 - accuracy: 0.9891\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1397 - accuracy: 1.0000\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1338 - accuracy: 0.9891\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1350 - accuracy: 0.9891\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1375 - accuracy: 1.0000\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1328 - accuracy: 0.9891\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1337 - accuracy: 0.9891\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1357 - accuracy: 1.0000\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1321 - accuracy: 0.9891\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1323 - accuracy: 0.9891\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1341 - accuracy: 1.0000\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1315 - accuracy: 0.9891\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1310 - accuracy: 0.9891\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1325 - accuracy: 1.0000\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1309 - accuracy: 0.9891\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1299 - accuracy: 0.9891\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1310 - accuracy: 1.0000\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1303 - accuracy: 0.9891\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1292 - accuracy: 0.9891\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1296 - accuracy: 1.0000\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1296 - accuracy: 0.9891\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1286 - accuracy: 0.9891\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1285 - accuracy: 0.9891\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1288 - accuracy: 0.9891\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1282 - accuracy: 0.9891\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1277 - accuracy: 0.9891\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1278 - accuracy: 0.9891\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1277 - accuracy: 1.0000\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1271 - accuracy: 0.9891\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1269 - accuracy: 0.9891\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1270 - accuracy: 1.0000\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1267 - accuracy: 0.9891\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1263 - accuracy: 0.9891\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1261 - accuracy: 0.9891\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1261 - accuracy: 0.9891\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1258 - accuracy: 0.9891\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1255 - accuracy: 0.9891\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1253 - accuracy: 0.9891\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1252 - accuracy: 0.9891\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1249 - accuracy: 0.9891\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1247 - accuracy: 0.9891\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1245 - accuracy: 0.9891\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1244 - accuracy: 0.9891\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1241 - accuracy: 0.9891\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1239 - accuracy: 0.9891\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1237 - accuracy: 0.9891\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1236 - accuracy: 0.9891\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1234 - accuracy: 0.9891\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1232 - accuracy: 0.9891\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1230 - accuracy: 0.9891\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1228 - accuracy: 0.9891\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1226 - accuracy: 1.0000\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1224 - accuracy: 0.9891\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1222 - accuracy: 0.9891\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1220 - accuracy: 0.9891\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1219 - accuracy: 0.9891\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1217 - accuracy: 1.0000\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1215 - accuracy: 0.9891\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1213 - accuracy: 0.9891\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1211 - accuracy: 0.9891\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1209 - accuracy: 0.9891\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1208 - accuracy: 1.0000\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1206 - accuracy: 0.9891\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1204 - accuracy: 0.9891\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1202 - accuracy: 1.0000\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1200 - accuracy: 0.9891\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1199 - accuracy: 1.0000\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1197 - accuracy: 0.9891\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1195 - accuracy: 1.0000\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1193 - accuracy: 1.0000\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1191 - accuracy: 1.0000\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1190 - accuracy: 1.0000\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1188 - accuracy: 0.9891\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1186 - accuracy: 1.0000\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1185 - accuracy: 1.0000\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1183 - accuracy: 1.0000\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1181 - accuracy: 1.0000\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1179 - accuracy: 1.0000\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1178 - accuracy: 1.0000\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1176 - accuracy: 1.0000\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1174 - accuracy: 1.0000\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1173 - accuracy: 1.0000\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1171 - accuracy: 1.0000\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1169 - accuracy: 1.0000\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1168 - accuracy: 1.0000\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1166 - accuracy: 1.0000\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1165 - accuracy: 1.0000\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1163 - accuracy: 1.0000\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1161 - accuracy: 1.0000\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1160 - accuracy: 1.0000\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1158 - accuracy: 1.0000\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1157 - accuracy: 1.0000\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1155 - accuracy: 1.0000\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1153 - accuracy: 1.0000\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1152 - accuracy: 1.0000\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1150 - accuracy: 1.0000\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1149 - accuracy: 1.0000\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1147 - accuracy: 1.0000\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1146 - accuracy: 1.0000\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1144 - accuracy: 1.0000\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1143 - accuracy: 1.0000\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1142 - accuracy: 1.0000\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1140 - accuracy: 0.9891\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1142 - accuracy: 0.9891\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1145 - accuracy: 1.0000\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1152 - accuracy: 0.9891\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1166 - accuracy: 1.0000\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1194 - accuracy: 0.9891\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1255 - accuracy: 1.0000\n",
            "30/30 [==============================] - 0s 939us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "10c504e8-d914-46fb-e2a5-1dab5056fe5f",
        "id": "3ZA9uHPGFl4F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e82ddca6-9f36-4bbd-d484-36d405307d5d",
        "id": "NqEhhZesFl4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 420,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333373069763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 420
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dzxDs6lEFl4U"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqDsTgsM1Dou",
        "colab_type": "text"
      },
      "source": [
        "#SELEKT FEATURES PEARSON CORRELATION, Ãˆ UNSUPERVISED QUINDI NON INTRODUCO BIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QkSb5hJ1Krt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create correlation matrix\n",
        "corr_matrix = data.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaQsViq03byP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c771e02b-2335-4d31-90e3-17c1eefe6f8e"
      },
      "source": [
        "len(to_drop)\n"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 327
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la9sruYz3c3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop features \n",
        "data_reduced=data.drop(data[to_drop], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVTm6r6r4zFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "4a1762f4-c125-4a40-e4e8-b743d5a43c3d"
      },
      "source": [
        "data_reduced"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>INTENSITY_Mean_NET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_NET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_ED_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ED_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ED_T1</th>\n",
              "      <th>INTENSITY_STD_ED_T1</th>\n",
              "      <th>INTENSITY_Mean_ED_T2</th>\n",
              "      <th>INTENSITY_STD_ED_T2</th>\n",
              "      <th>INTENSITY_Mean_ED_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ED_FLAIR</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ET_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>227.7510</td>\n",
              "      <td>23.9509</td>\n",
              "      <td>131.8402</td>\n",
              "      <td>8.0213</td>\n",
              "      <td>188.7259</td>\n",
              "      <td>15.9249</td>\n",
              "      <td>112.7128</td>\n",
              "      <td>18.5509</td>\n",
              "      <td>171.7623</td>\n",
              "      <td>28.2802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.36966</td>\n",
              "      <td>0.066484</td>\n",
              "      <td>13.51700</td>\n",
              "      <td>0.61118</td>\n",
              "      <td>0.095961</td>\n",
              "      <td>4668.4405</td>\n",
              "      <td>11.62680</td>\n",
              "      <td>0.49151</td>\n",
              "      <td>0.083098</td>\n",
              "      <td>3640.7448</td>\n",
              "      <td>13.7035</td>\n",
              "      <td>0.000849</td>\n",
              "      <td>1.33550</td>\n",
              "      <td>0.65599</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>209.7901</td>\n",
              "      <td>16.7943</td>\n",
              "      <td>139.2815</td>\n",
              "      <td>11.7893</td>\n",
              "      <td>189.7841</td>\n",
              "      <td>22.8683</td>\n",
              "      <td>109.9477</td>\n",
              "      <td>18.6668</td>\n",
              "      <td>190.0843</td>\n",
              "      <td>19.1663</td>\n",
              "      <td>...</td>\n",
              "      <td>0.18462</td>\n",
              "      <td>0.154740</td>\n",
              "      <td>6.90060</td>\n",
              "      <td>0.47885</td>\n",
              "      <td>0.179650</td>\n",
              "      <td>3378.9729</td>\n",
              "      <td>4.70500</td>\n",
              "      <td>0.24995</td>\n",
              "      <td>0.205790</td>\n",
              "      <td>1306.2095</td>\n",
              "      <td>3.7181</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.54351</td>\n",
              "      <td>1.99800</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>165.1014</td>\n",
              "      <td>25.2120</td>\n",
              "      <td>161.5790</td>\n",
              "      <td>14.7441</td>\n",
              "      <td>184.1934</td>\n",
              "      <td>20.4879</td>\n",
              "      <td>126.6645</td>\n",
              "      <td>35.5877</td>\n",
              "      <td>116.7452</td>\n",
              "      <td>18.9931</td>\n",
              "      <td>...</td>\n",
              "      <td>0.26313</td>\n",
              "      <td>0.992810</td>\n",
              "      <td>1.05600</td>\n",
              "      <td>0.46056</td>\n",
              "      <td>1.149400</td>\n",
              "      <td>3627.2679</td>\n",
              "      <td>0.83550</td>\n",
              "      <td>0.31288</td>\n",
              "      <td>0.897080</td>\n",
              "      <td>2402.4765</td>\n",
              "      <td>1.0795</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>2.40570</td>\n",
              "      <td>0.33032</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>207.5531</td>\n",
              "      <td>32.7868</td>\n",
              "      <td>129.2927</td>\n",
              "      <td>15.8702</td>\n",
              "      <td>192.1728</td>\n",
              "      <td>19.1468</td>\n",
              "      <td>127.4586</td>\n",
              "      <td>21.7466</td>\n",
              "      <td>166.1232</td>\n",
              "      <td>19.0174</td>\n",
              "      <td>...</td>\n",
              "      <td>0.37522</td>\n",
              "      <td>0.317880</td>\n",
              "      <td>3.08780</td>\n",
              "      <td>0.99191</td>\n",
              "      <td>0.796790</td>\n",
              "      <td>8568.8327</td>\n",
              "      <td>1.23890</td>\n",
              "      <td>0.73033</td>\n",
              "      <td>0.887200</td>\n",
              "      <td>5540.7900</td>\n",
              "      <td>1.1435</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>1.61720</td>\n",
              "      <td>0.60797</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>139.3234</td>\n",
              "      <td>12.9900</td>\n",
              "      <td>120.7850</td>\n",
              "      <td>9.5358</td>\n",
              "      <td>127.2605</td>\n",
              "      <td>11.5588</td>\n",
              "      <td>141.4811</td>\n",
              "      <td>30.3721</td>\n",
              "      <td>174.9837</td>\n",
              "      <td>39.2676</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21169</td>\n",
              "      <td>1.618800</td>\n",
              "      <td>0.56564</td>\n",
              "      <td>0.53075</td>\n",
              "      <td>1.497700</td>\n",
              "      <td>4101.2628</td>\n",
              "      <td>0.66943</td>\n",
              "      <td>0.37437</td>\n",
              "      <td>0.930440</td>\n",
              "      <td>3094.3823</td>\n",
              "      <td>1.0690</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>2.08790</td>\n",
              "      <td>0.44848</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>195.2111</td>\n",
              "      <td>26.9315</td>\n",
              "      <td>93.2602</td>\n",
              "      <td>17.0673</td>\n",
              "      <td>141.6237</td>\n",
              "      <td>12.0477</td>\n",
              "      <td>167.1742</td>\n",
              "      <td>33.3711</td>\n",
              "      <td>198.2391</td>\n",
              "      <td>27.7802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.14318</td>\n",
              "      <td>0.119840</td>\n",
              "      <td>4.80430</td>\n",
              "      <td>0.43837</td>\n",
              "      <td>0.069513</td>\n",
              "      <td>2781.0021</td>\n",
              "      <td>14.19380</td>\n",
              "      <td>0.30470</td>\n",
              "      <td>0.064140</td>\n",
              "      <td>1695.5136</td>\n",
              "      <td>16.2000</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>3.30530</td>\n",
              "      <td>0.30539</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>176.3511</td>\n",
              "      <td>15.6089</td>\n",
              "      <td>92.3474</td>\n",
              "      <td>13.9088</td>\n",
              "      <td>95.5196</td>\n",
              "      <td>10.0495</td>\n",
              "      <td>157.7984</td>\n",
              "      <td>19.1808</td>\n",
              "      <td>156.2050</td>\n",
              "      <td>17.6038</td>\n",
              "      <td>...</td>\n",
              "      <td>0.24254</td>\n",
              "      <td>0.056586</td>\n",
              "      <td>14.78380</td>\n",
              "      <td>0.30014</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>1541.0463</td>\n",
              "      <td>5.24340</td>\n",
              "      <td>0.18788</td>\n",
              "      <td>0.123600</td>\n",
              "      <td>786.3592</td>\n",
              "      <td>5.0610</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>1.01150</td>\n",
              "      <td>0.98669</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>152.2227</td>\n",
              "      <td>8.7866</td>\n",
              "      <td>77.4138</td>\n",
              "      <td>12.3593</td>\n",
              "      <td>95.6459</td>\n",
              "      <td>10.5155</td>\n",
              "      <td>130.4020</td>\n",
              "      <td>22.9069</td>\n",
              "      <td>162.0746</td>\n",
              "      <td>21.0190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21899</td>\n",
              "      <td>0.090350</td>\n",
              "      <td>13.62950</td>\n",
              "      <td>0.40069</td>\n",
              "      <td>0.138990</td>\n",
              "      <td>2441.1777</td>\n",
              "      <td>7.72570</td>\n",
              "      <td>0.18868</td>\n",
              "      <td>0.117270</td>\n",
              "      <td>783.3983</td>\n",
              "      <td>6.8371</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>1.60230</td>\n",
              "      <td>0.63381</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>161.5295</td>\n",
              "      <td>24.1303</td>\n",
              "      <td>116.2060</td>\n",
              "      <td>19.2959</td>\n",
              "      <td>116.9104</td>\n",
              "      <td>8.9413</td>\n",
              "      <td>74.9602</td>\n",
              "      <td>14.7574</td>\n",
              "      <td>118.0905</td>\n",
              "      <td>10.9597</td>\n",
              "      <td>...</td>\n",
              "      <td>0.15495</td>\n",
              "      <td>0.515090</td>\n",
              "      <td>2.16740</td>\n",
              "      <td>0.40647</td>\n",
              "      <td>0.337070</td>\n",
              "      <td>2946.9071</td>\n",
              "      <td>2.78790</td>\n",
              "      <td>0.19372</td>\n",
              "      <td>0.269660</td>\n",
              "      <td>930.0437</td>\n",
              "      <td>2.7257</td>\n",
              "      <td>0.002336</td>\n",
              "      <td>0.33724</td>\n",
              "      <td>2.71990</td>\n",
              "      <td>0.003182</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>163.4821</td>\n",
              "      <td>10.5344</td>\n",
              "      <td>80.2180</td>\n",
              "      <td>11.7079</td>\n",
              "      <td>101.9670</td>\n",
              "      <td>6.8936</td>\n",
              "      <td>113.1424</td>\n",
              "      <td>19.9577</td>\n",
              "      <td>161.5958</td>\n",
              "      <td>27.0185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.14813</td>\n",
              "      <td>0.661010</td>\n",
              "      <td>1.29430</td>\n",
              "      <td>0.37790</td>\n",
              "      <td>0.411020</td>\n",
              "      <td>2774.2228</td>\n",
              "      <td>2.54050</td>\n",
              "      <td>0.20349</td>\n",
              "      <td>0.255830</td>\n",
              "      <td>1320.7903</td>\n",
              "      <td>3.8278</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>3.59770</td>\n",
              "      <td>0.27053</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows Ã— 378 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     VOLUME_ET  VOLUME_NET  VOLUME_ED  ...  TGM_Cog_Y_1  TGM_Cog_Z_1    TGM_T_1\n",
              "0         1662         384      36268  ...     0.096256     0.052741    2.00000\n",
              "1         4362        4349      15723  ...     0.122900     0.094336   91.47360\n",
              "2        33404       48612      45798  ...     0.116006     0.096035  272.42900\n",
              "3        12114        7587      34086  ...     0.088249     0.096470  128.46800\n",
              "4        34538        7137      65653  ...     0.131952     0.096894  240.77800\n",
              "..         ...         ...        ...  ...          ...          ...        ...\n",
              "141       1036      189152     171595  ...     0.070503     0.090456  719.23800\n",
              "142       2093        8685      39142  ...     0.120586     0.054307    2.00000\n",
              "143       1929         437      54079  ...     0.073453     0.126712    7.06744\n",
              "144       8755      168606      11325  ...     0.167317     0.107433   15.52240\n",
              "145      11757        1012     138755  ...     0.144989     0.069101    7.62280\n",
              "\n",
              "[146 rows x 378 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 330
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cWgjvLGi5FJ9"
      },
      "source": [
        "##Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ifZA9ha-5FKH",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6sP_wBH45FKe",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(data_reduced, labels, test_size=0.2, stratify=labels, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oJ6cCEac5LKn"
      },
      "source": [
        "##Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phG88UV75LK0",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xb6Kipew5LLM",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=0.2, stratify=y_train_big, random_state=3)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e71d173-6013-442d-8942-88e8f501901c",
        "id": "pPT5k8Am5LLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 378)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DR8meyk5da-"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1G8qIrFp5dbH",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y64hCefH453w"
      },
      "source": [
        "##SENZA PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E7rv8oRg4534"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k29a5RWu453-",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y5xCOoop454T",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n9RBB_jW454i",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T2vCpbRy454s",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LvHFfzDN4542",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-YIHI8P455B",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0gcB_vTI455K",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0kB5pF455U",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "train_labels_enc = encoder.fit_transform(y_train)\n",
        "val_labels_enc = encoder.transform(y_val)\n",
        "test_labels_enc = encoder.transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I53-QF9O455g"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vSCBmaZV455i",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vtbtofer455q",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sZ0UR1_E455y",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GRh6JVrb4555",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SISmEQ86456G",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uawvixOb456P",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3KTR9A3P456b",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i_NmROtC456i",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(378,), kernel_regularizer=regularizers.l2(l=0.05)))\n",
        "  #model.add(layers.Dropout(0.01))\n",
        "  #model.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(l=0.05)))\n",
        "  #model.add(layers.Dropout(0.01))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.05, momentum=0.9)\n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  rmsprop = RMSprop(lr=0.001)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MVkqlnxl456q",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, factor=0.1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b671911d-0553-4707-fe44-5bf924f222b1",
        "id": "Jk2BuIPu456v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "999a5775-5dfd-41aa-cc37-9066531838b3",
        "id": "9qPpRDWg4563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, train_labels_enc, validation_data=(val_data_stand, val_labels_enc), \n",
        "                      epochs= num_epochs, batch_size=92, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 2.6359 - accuracy: 0.3587 - val_loss: 2.3241 - val_accuracy: 0.7083\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 2.3220 - accuracy: 0.6957 - val_loss: 2.0399 - val_accuracy: 0.7083\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 2.0297 - accuracy: 0.6957 - val_loss: 1.7484 - val_accuracy: 0.7500\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 1.7278 - accuracy: 0.7935 - val_loss: 1.5541 - val_accuracy: 0.8750\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 1.5324 - accuracy: 0.8913 - val_loss: 1.3190 - val_accuracy: 0.8333\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 1.2812 - accuracy: 0.8261 - val_loss: 1.1688 - val_accuracy: 0.7917\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 1.1082 - accuracy: 0.7935 - val_loss: 1.0534 - val_accuracy: 0.7917\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.9740 - accuracy: 0.7935 - val_loss: 0.9256 - val_accuracy: 0.8333\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.8415 - accuracy: 0.8043 - val_loss: 0.8275 - val_accuracy: 0.8333\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.7511 - accuracy: 0.8587 - val_loss: 0.7657 - val_accuracy: 0.8333\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.6891 - accuracy: 0.9022 - val_loss: 0.7202 - val_accuracy: 0.8333\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.6252 - accuracy: 0.8804 - val_loss: 0.7082 - val_accuracy: 0.8333\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.5896 - accuracy: 0.8587 - val_loss: 0.6969 - val_accuracy: 0.8333\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.5636 - accuracy: 0.8587 - val_loss: 0.6634 - val_accuracy: 0.8333\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5369 - accuracy: 0.9022 - val_loss: 0.6464 - val_accuracy: 0.8333\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5282 - accuracy: 0.9239 - val_loss: 0.6462 - val_accuracy: 0.8333\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5125 - accuracy: 0.9130 - val_loss: 0.6643 - val_accuracy: 0.8333\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5023 - accuracy: 0.8913 - val_loss: 0.6735 - val_accuracy: 0.8333\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4963 - accuracy: 0.8587 - val_loss: 0.6535 - val_accuracy: 0.8333\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.4827 - accuracy: 0.9130 - val_loss: 0.6325 - val_accuracy: 0.8333\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4756 - accuracy: 0.9348 - val_loss: 0.6270 - val_accuracy: 0.8333\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.4642 - accuracy: 0.9457 - val_loss: 0.6396 - val_accuracy: 0.8333\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.4509 - accuracy: 0.9348 - val_loss: 0.6460 - val_accuracy: 0.8333\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.4416 - accuracy: 0.9022 - val_loss: 0.6206 - val_accuracy: 0.8333\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4258 - accuracy: 0.9348 - val_loss: 0.5963 - val_accuracy: 0.8333\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4158 - accuracy: 0.9565 - val_loss: 0.5954 - val_accuracy: 0.8333\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4012 - accuracy: 0.9674 - val_loss: 0.6076 - val_accuracy: 0.8333\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3912 - accuracy: 0.9348 - val_loss: 0.5919 - val_accuracy: 0.8333\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3781 - accuracy: 0.9348 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3693 - accuracy: 0.9674 - val_loss: 0.5702 - val_accuracy: 0.8333\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3579 - accuracy: 0.9674 - val_loss: 0.5834 - val_accuracy: 0.8333\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.3502 - accuracy: 0.9348 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.3404 - accuracy: 0.9674 - val_loss: 0.5552 - val_accuracy: 0.8333\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3346 - accuracy: 0.9674 - val_loss: 0.5714 - val_accuracy: 0.8333\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3272 - accuracy: 0.9674 - val_loss: 0.5702 - val_accuracy: 0.8333\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3213 - accuracy: 0.9674 - val_loss: 0.5537 - val_accuracy: 0.8333\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.3168 - accuracy: 0.9674 - val_loss: 0.5710 - val_accuracy: 0.8333\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3107 - accuracy: 0.9674 - val_loss: 0.5715 - val_accuracy: 0.8333\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.3059 - accuracy: 0.9674 - val_loss: 0.5538 - val_accuracy: 0.8333\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3023 - accuracy: 0.9783 - val_loss: 0.5787 - val_accuracy: 0.8333\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2978 - accuracy: 0.9674 - val_loss: 0.5587 - val_accuracy: 0.8333\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2925 - accuracy: 0.9674 - val_loss: 0.5607 - val_accuracy: 0.8333\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2882 - accuracy: 0.9674 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2853 - accuracy: 0.9674 - val_loss: 0.5438 - val_accuracy: 0.8333\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2832 - accuracy: 0.9891 - val_loss: 0.5919 - val_accuracy: 0.8333\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2819 - accuracy: 0.9674 - val_loss: 0.5328 - val_accuracy: 0.8333\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2808 - accuracy: 0.9783 - val_loss: 0.6127 - val_accuracy: 0.8333\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2815 - accuracy: 0.9348 - val_loss: 0.5273 - val_accuracy: 0.8333\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2782 - accuracy: 0.9783 - val_loss: 0.6118 - val_accuracy: 0.8333\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2736 - accuracy: 0.9457 - val_loss: 0.5395 - val_accuracy: 0.8333\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2639 - accuracy: 0.9891 - val_loss: 0.5696 - val_accuracy: 0.8333\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2571 - accuracy: 0.9783 - val_loss: 0.5789 - val_accuracy: 0.8333\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2551 - accuracy: 0.9674 - val_loss: 0.5346 - val_accuracy: 0.8333\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2569 - accuracy: 0.9891 - val_loss: 0.6253 - val_accuracy: 0.8333\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2632 - accuracy: 0.9348 - val_loss: 0.5153 - val_accuracy: 0.8333\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2696 - accuracy: 0.9783 - val_loss: 0.6670 - val_accuracy: 0.8333\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2785 - accuracy: 0.8696 - val_loss: 0.5214 - val_accuracy: 0.8333\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2577 - accuracy: 0.9783 - val_loss: 0.5849 - val_accuracy: 0.8333\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2410 - accuracy: 0.9783 - val_loss: 0.6067 - val_accuracy: 0.8333\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2425 - accuracy: 0.9674 - val_loss: 0.5240 - val_accuracy: 0.8333\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2526 - accuracy: 0.9783 - val_loss: 0.6593 - val_accuracy: 0.8333\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2564 - accuracy: 0.9239 - val_loss: 0.5397 - val_accuracy: 0.8333\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2393 - accuracy: 0.9783 - val_loss: 0.5723 - val_accuracy: 0.8333\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 91us/step - loss: 0.2307 - accuracy: 0.9891 - val_loss: 0.6287 - val_accuracy: 0.8333\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2369 - accuracy: 0.9674 - val_loss: 0.5271 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2431 - accuracy: 0.9783 - val_loss: 0.5350 - val_accuracy: 0.8333\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2369 - accuracy: 0.9783 - val_loss: 0.5580 - val_accuracy: 0.8333\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2284 - accuracy: 0.9891 - val_loss: 0.5919 - val_accuracy: 0.8333\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2272 - accuracy: 0.9891 - val_loss: 0.6217 - val_accuracy: 0.8333\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2323 - accuracy: 0.9674 - val_loss: 0.6333 - val_accuracy: 0.8333\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2356 - accuracy: 0.9674 - val_loss: 0.6249 - val_accuracy: 0.8333\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2328 - accuracy: 0.9674 - val_loss: 0.6009 - val_accuracy: 0.8333\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2275 - accuracy: 0.9783 - val_loss: 0.5725 - val_accuracy: 0.8333\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2257 - accuracy: 0.9891 - val_loss: 0.5520 - val_accuracy: 0.8333\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2282 - accuracy: 0.9891 - val_loss: 0.5436 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2307 - accuracy: 0.9891 - val_loss: 0.5439 - val_accuracy: 0.8333\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2305 - accuracy: 0.9891 - val_loss: 0.5452 - val_accuracy: 0.8333\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2300 - accuracy: 0.9891 - val_loss: 0.5474 - val_accuracy: 0.8333\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2293 - accuracy: 0.9891 - val_loss: 0.5503 - val_accuracy: 0.8333\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2283 - accuracy: 0.9891 - val_loss: 0.5540 - val_accuracy: 0.8333\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2274 - accuracy: 0.9891 - val_loss: 0.5582 - val_accuracy: 0.8333\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2266 - accuracy: 0.9891 - val_loss: 0.5628 - val_accuracy: 0.8333\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2259 - accuracy: 0.9891 - val_loss: 0.5676 - val_accuracy: 0.8333\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2254 - accuracy: 0.9891 - val_loss: 0.5724 - val_accuracy: 0.8333\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2251 - accuracy: 0.9891 - val_loss: 0.5771 - val_accuracy: 0.8333\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2250 - accuracy: 0.9891 - val_loss: 0.5814 - val_accuracy: 0.8333\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2250 - accuracy: 0.9891 - val_loss: 0.5853 - val_accuracy: 0.8333\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2252 - accuracy: 0.9891 - val_loss: 0.5885 - val_accuracy: 0.8333\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2254 - accuracy: 0.9891 - val_loss: 0.5910 - val_accuracy: 0.8333\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2256 - accuracy: 0.9891 - val_loss: 0.5928 - val_accuracy: 0.8333\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2257 - accuracy: 0.9891 - val_loss: 0.5938 - val_accuracy: 0.8333\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2258 - accuracy: 0.9891 - val_loss: 0.5940 - val_accuracy: 0.8333\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2258 - accuracy: 0.9891 - val_loss: 0.5935 - val_accuracy: 0.8333\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2257 - accuracy: 0.9891 - val_loss: 0.5925 - val_accuracy: 0.8333\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2256 - accuracy: 0.9891 - val_loss: 0.5909 - val_accuracy: 0.8333\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2254 - accuracy: 0.9891 - val_loss: 0.5889 - val_accuracy: 0.8333\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2253 - accuracy: 0.9891 - val_loss: 0.5866 - val_accuracy: 0.8333\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2251 - accuracy: 0.9891 - val_loss: 0.5842 - val_accuracy: 0.8333\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2249 - accuracy: 0.9891 - val_loss: 0.5818 - val_accuracy: 0.8333\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2248 - accuracy: 0.9891 - val_loss: 0.5794 - val_accuracy: 0.8333\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2247 - accuracy: 0.9891 - val_loss: 0.5771 - val_accuracy: 0.8333\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2246 - accuracy: 0.9891 - val_loss: 0.5751 - val_accuracy: 0.8333\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2246 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 86us/step - loss: 0.2246 - accuracy: 0.9891 - val_loss: 0.5718 - val_accuracy: 0.8333\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2246 - accuracy: 0.9891 - val_loss: 0.5707 - val_accuracy: 0.8333\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2247 - accuracy: 0.9891 - val_loss: 0.5699 - val_accuracy: 0.8333\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2247 - accuracy: 0.9891 - val_loss: 0.5694 - val_accuracy: 0.8333\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2247 - accuracy: 0.9891 - val_loss: 0.5692 - val_accuracy: 0.8333\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2247 - accuracy: 0.9891 - val_loss: 0.5694 - val_accuracy: 0.8333\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2246 - accuracy: 0.9891 - val_loss: 0.5698 - val_accuracy: 0.8333\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2246 - accuracy: 0.9891 - val_loss: 0.5704 - val_accuracy: 0.8333\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2245 - accuracy: 0.9891 - val_loss: 0.5712 - val_accuracy: 0.8333\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2245 - accuracy: 0.9891 - val_loss: 0.5721 - val_accuracy: 0.8333\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2244 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2244 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2243 - accuracy: 0.9891 - val_loss: 0.5751 - val_accuracy: 0.8333\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2243 - accuracy: 0.9891 - val_loss: 0.5761 - val_accuracy: 0.8333\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 104us/step - loss: 0.2243 - accuracy: 0.9891 - val_loss: 0.5770 - val_accuracy: 0.8333\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2242 - accuracy: 0.9891 - val_loss: 0.5777 - val_accuracy: 0.8333\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2242 - accuracy: 0.9891 - val_loss: 0.5783 - val_accuracy: 0.8333\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2242 - accuracy: 0.9891 - val_loss: 0.5787 - val_accuracy: 0.8333\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2242 - accuracy: 0.9891 - val_loss: 0.5789 - val_accuracy: 0.8333\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2242 - accuracy: 0.9891 - val_loss: 0.5790 - val_accuracy: 0.8333\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2242 - accuracy: 0.9891 - val_loss: 0.5789 - val_accuracy: 0.8333\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2241 - accuracy: 0.9891 - val_loss: 0.5787 - val_accuracy: 0.8333\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 84us/step - loss: 0.2241 - accuracy: 0.9891 - val_loss: 0.5783 - val_accuracy: 0.8333\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2241 - accuracy: 0.9891 - val_loss: 0.5779 - val_accuracy: 0.8333\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2240 - accuracy: 0.9891 - val_loss: 0.5774 - val_accuracy: 0.8333\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2240 - accuracy: 0.9891 - val_loss: 0.5768 - val_accuracy: 0.8333\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2240 - accuracy: 0.9891 - val_loss: 0.5763 - val_accuracy: 0.8333\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2240 - accuracy: 0.9891 - val_loss: 0.5757 - val_accuracy: 0.8333\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2239 - accuracy: 0.9891 - val_loss: 0.5752 - val_accuracy: 0.8333\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2239 - accuracy: 0.9891 - val_loss: 0.5748 - val_accuracy: 0.8333\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2239 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2239 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2238 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2238 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2238 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2238 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2238 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2237 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2237 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2237 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2237 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2236 - accuracy: 0.9891 - val_loss: 0.5748 - val_accuracy: 0.8333\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2236 - accuracy: 0.9891 - val_loss: 0.5750 - val_accuracy: 0.8333\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2236 - accuracy: 0.9891 - val_loss: 0.5752 - val_accuracy: 0.8333\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2236 - accuracy: 0.9891 - val_loss: 0.5753 - val_accuracy: 0.8333\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2235 - accuracy: 0.9891 - val_loss: 0.5754 - val_accuracy: 0.8333\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2235 - accuracy: 0.9891 - val_loss: 0.5754 - val_accuracy: 0.8333\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2235 - accuracy: 0.9891 - val_loss: 0.5754 - val_accuracy: 0.8333\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2235 - accuracy: 0.9891 - val_loss: 0.5753 - val_accuracy: 0.8333\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2234 - accuracy: 0.9891 - val_loss: 0.5753 - val_accuracy: 0.8333\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2234 - accuracy: 0.9891 - val_loss: 0.5752 - val_accuracy: 0.8333\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2234 - accuracy: 0.9891 - val_loss: 0.5750 - val_accuracy: 0.8333\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2234 - accuracy: 0.9891 - val_loss: 0.5749 - val_accuracy: 0.8333\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2234 - accuracy: 0.9891 - val_loss: 0.5748 - val_accuracy: 0.8333\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2233 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2233 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2233 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2233 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2232 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2232 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2232 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 127us/step - loss: 0.2232 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2232 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2231 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2231 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2231 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2231 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2230 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2230 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2230 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2230 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2229 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2229 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2229 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2229 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2229 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2228 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2228 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2228 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2228 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2227 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2227 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2227 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2227 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2227 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2226 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2226 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 105us/step - loss: 0.2226 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.2226 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2225 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.2225 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2225 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2225 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2225 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2224 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2224 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2224 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2224 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2223 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2223 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2223 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2223 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2223 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2222 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2222 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2222 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2222 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2221 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2221 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2221 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2221 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2220 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2220 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2220 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2220 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2220 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2219 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2219 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2219 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2219 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2219 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2218 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2218 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2218 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2218 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2217 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2217 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2217 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2217 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2217 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2216 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2216 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2216 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2216 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2215 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2215 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2215 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2215 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2215 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2214 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2214 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2214 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2214 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2213 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2213 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2213 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2213 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2213 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2212 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2212 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2212 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2212 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2211 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2211 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2211 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2211 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2211 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 101us/step - loss: 0.2210 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2210 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2210 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2210 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2209 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 96us/step - loss: 0.2209 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2209 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2209 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2209 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2208 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2208 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2208 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2208 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2208 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2207 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2207 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2207 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2207 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2206 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2206 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2206 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2206 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2206 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2205 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2205 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2205 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2205 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2204 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2204 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2204 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2204 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2204 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2203 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2203 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2203 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2203 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2203 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2202 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2202 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2202 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2202 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2201 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2201 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2201 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2201 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2201 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 94us/step - loss: 0.2200 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 83us/step - loss: 0.2200 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2200 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2200 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2200 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2199 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2199 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2199 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2199 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2198 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2198 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2198 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2198 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 102us/step - loss: 0.2198 - accuracy: 0.9891 - val_loss: 0.5730 - val_accuracy: 0.8333\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2197 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2197 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2197 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2197 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2196 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2196 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2196 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2196 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2196 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2195 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2195 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2195 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2195 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2195 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2194 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2194 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2194 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2194 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2193 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2193 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2193 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2193 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2193 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2192 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2192 - accuracy: 0.9891 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2192 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2192 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2192 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2191 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2191 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2191 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2191 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2190 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2190 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2190 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2190 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2190 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2189 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2189 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2189 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2189 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2189 - accuracy: 0.9891 - val_loss: 0.5732 - val_accuracy: 0.8333\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2188 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2188 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2188 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.2188 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2187 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2187 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2187 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2187 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.2187 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2186 - accuracy: 0.9891 - val_loss: 0.5733 - val_accuracy: 0.8333\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2186 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2186 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2186 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2186 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2185 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2185 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2185 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 100us/step - loss: 0.2185 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2185 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2184 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 105us/step - loss: 0.2184 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2184 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2184 - accuracy: 0.9891 - val_loss: 0.5734 - val_accuracy: 0.8333\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2183 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2183 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2183 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 74us/step - loss: 0.2183 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2183 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2182 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2182 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2182 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.2182 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2182 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2181 - accuracy: 0.9891 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2181 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2181 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2181 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2180 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2180 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2180 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2180 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2180 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2179 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2179 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2179 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2179 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.2179 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2178 - accuracy: 0.9891 - val_loss: 0.5736 - val_accuracy: 0.8333\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2178 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2178 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2178 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2177 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2177 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2177 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2177 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2177 - accuracy: 0.9891 - val_loss: 0.5737 - val_accuracy: 0.8333\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 95us/step - loss: 0.2176 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2176 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2176 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2176 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2176 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2175 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2175 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2175 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2175 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2175 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2174 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2174 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2174 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2174 - accuracy: 0.9891 - val_loss: 0.5738 - val_accuracy: 0.8333\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2173 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2173 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2173 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2173 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2173 - accuracy: 0.9891 - val_loss: 0.5739 - val_accuracy: 0.8333\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2172 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2172 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2172 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2172 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2172 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2171 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2171 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2171 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2171 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2170 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2170 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2170 - accuracy: 0.9891 - val_loss: 0.5740 - val_accuracy: 0.8333\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2170 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2170 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 0.5741 - val_accuracy: 0.8333\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2168 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2168 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2168 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2168 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2168 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2167 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2167 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2167 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2167 - accuracy: 0.9891 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2166 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2166 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2166 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2166 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2166 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2165 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2165 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2165 - accuracy: 0.9891 - val_loss: 0.5743 - val_accuracy: 0.8333\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2165 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2165 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2164 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2164 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2164 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2164 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2164 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2163 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2163 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2163 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2163 - accuracy: 0.9891 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2162 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2162 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2162 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2162 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2162 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2161 - accuracy: 0.9891 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.2161 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2161 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2161 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2161 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2160 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2160 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 150us/step - loss: 0.2160 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2160 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2160 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2159 - accuracy: 0.9891 - val_loss: 0.5746 - val_accuracy: 0.8333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6mRlQSPm456-"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_F7vGZs2457A",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gF_t17C5457F",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d41ddaa6-f081-48b0-e8a4-2b48638becf0",
        "id": "PfOZ2qJY457M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff8bfe30e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 368
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vSuv_hzy457V"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f72ac55b-0cba-4043-bfc1-35d8cb6e3afb",
        "id": "MUSdTQZr457W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff8c0ed7668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 369
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1o2u3SqX457d"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b4cb9bf3-55df-4a41-9bda-c293514f63eb",
        "id": "1AMu50DR457e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, train_labels_dec, epochs= num_epochs, batch_size=92, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, test_labels_enc)\n",
        "  "
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 2.7087 - accuracy: 0.2065\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 2.2928 - accuracy: 0.6957\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 1.9988 - accuracy: 0.6957\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 1.6924 - accuracy: 0.7065\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 1.4962 - accuracy: 0.8152\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 1.2766 - accuracy: 0.7717\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 1.1119 - accuracy: 0.7500\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.9851 - accuracy: 0.7717\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.8688 - accuracy: 0.8152\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7849 - accuracy: 0.8587\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7193 - accuracy: 0.8804\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.6642 - accuracy: 0.8696\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6250 - accuracy: 0.8587\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.5936 - accuracy: 0.8804\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.5690 - accuracy: 0.9022\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5490 - accuracy: 0.8804\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.5287 - accuracy: 0.9130\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5129 - accuracy: 0.9239\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4986 - accuracy: 0.9130\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4824 - accuracy: 0.9348\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4694 - accuracy: 0.9457\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.4539 - accuracy: 0.9348\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.4370 - accuracy: 0.9565\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.4227 - accuracy: 0.9674\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4070 - accuracy: 0.9565\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3906 - accuracy: 0.9674\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3770 - accuracy: 0.9783\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3647 - accuracy: 0.9674\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3520 - accuracy: 0.9891\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3399 - accuracy: 0.9783\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3292 - accuracy: 0.9783\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.3199 - accuracy: 0.9891\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3129 - accuracy: 0.9783\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3092 - accuracy: 0.9891\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3185 - accuracy: 0.9022\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3308 - accuracy: 0.9783\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3193 - accuracy: 0.8587\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2838 - accuracy: 0.9674\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3158 - accuracy: 0.9783\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3128 - accuracy: 0.8587\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2750 - accuracy: 0.9674\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3181 - accuracy: 0.9674\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2834 - accuracy: 0.9239\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2818 - accuracy: 0.9239\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2885 - accuracy: 0.9783\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2583 - accuracy: 0.9891\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2807 - accuracy: 0.8913\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2521 - accuracy: 0.9891\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2710 - accuracy: 0.9783\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2566 - accuracy: 0.9674\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2529 - accuracy: 0.9674\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2602 - accuracy: 0.9783\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2397 - accuracy: 0.9891\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2521 - accuracy: 0.9565\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2401 - accuracy: 0.9891\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2377 - accuracy: 0.9891\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2437 - accuracy: 0.9674\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2303 - accuracy: 0.9891\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2361 - accuracy: 0.9891\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2361 - accuracy: 0.9674\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2249 - accuracy: 0.9891\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2317 - accuracy: 0.9891\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2309 - accuracy: 0.9674\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2203 - accuracy: 0.9891\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2255 - accuracy: 0.9891\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2271 - accuracy: 0.9674\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2169 - accuracy: 0.9891\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2177 - accuracy: 0.9891\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2224 - accuracy: 0.9674\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2157 - accuracy: 0.9891\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2107 - accuracy: 0.9891\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2136 - accuracy: 0.9783\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2143 - accuracy: 0.9891\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 108us/step - loss: 0.2101 - accuracy: 0.9783\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2057 - accuracy: 0.9891\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2062 - accuracy: 0.9891\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2086 - accuracy: 0.9783\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2082 - accuracy: 0.9891\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2054 - accuracy: 0.9783\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2013 - accuracy: 0.9891\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1988 - accuracy: 0.9891\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1982 - accuracy: 0.9891\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1990 - accuracy: 0.9891\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2010 - accuracy: 0.9783\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2040 - accuracy: 0.9891\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2110 - accuracy: 0.9674\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2187 - accuracy: 0.9783\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2357 - accuracy: 0.9239\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2318 - accuracy: 0.9783\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2218 - accuracy: 0.9457\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1941 - accuracy: 0.9891\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1945 - accuracy: 0.9891\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2128 - accuracy: 0.9674\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2061 - accuracy: 0.9783\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1899 - accuracy: 0.9891\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1895 - accuracy: 0.9891\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2004 - accuracy: 0.9783\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2018 - accuracy: 0.9783\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1892 - accuracy: 0.9891\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1825 - accuracy: 0.9891\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1876 - accuracy: 0.9891\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1951 - accuracy: 0.9891\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1997 - accuracy: 0.9783\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1945 - accuracy: 0.9891\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1874 - accuracy: 0.9891\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1798 - accuracy: 0.9891\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1769 - accuracy: 0.9891\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1788 - accuracy: 0.9891\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1828 - accuracy: 0.9891\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1880 - accuracy: 0.9783\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1923 - accuracy: 0.9783\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1994 - accuracy: 0.9674\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1994 - accuracy: 0.9783\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1997 - accuracy: 0.9783\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1875 - accuracy: 0.9891\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1757 - accuracy: 0.9891\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1710 - accuracy: 0.9891\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1759 - accuracy: 0.9891\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1826 - accuracy: 0.9783\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1827 - accuracy: 0.9891\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1782 - accuracy: 0.9891\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1712 - accuracy: 0.9891\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1671 - accuracy: 0.9891\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1673 - accuracy: 0.9891\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1707 - accuracy: 0.9891\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1766 - accuracy: 0.9891\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1857 - accuracy: 0.9783\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2079 - accuracy: 0.9565\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2369 - accuracy: 0.9783\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3189 - accuracy: 0.8370\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2720 - accuracy: 0.9457\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2014 - accuracy: 0.9674\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1806 - accuracy: 0.9783\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2289 - accuracy: 0.9783\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1831 - accuracy: 0.9783\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1960 - accuracy: 0.9783\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2021 - accuracy: 0.9783\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1719 - accuracy: 0.9891\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2008 - accuracy: 0.9674\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1716 - accuracy: 0.9891\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1813 - accuracy: 0.9891\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1877 - accuracy: 0.9891\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1622 - accuracy: 0.9891\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1809 - accuracy: 0.9891\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1831 - accuracy: 0.9891\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1598 - accuracy: 0.9891\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1696 - accuracy: 0.9891\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1833 - accuracy: 0.9783\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1667 - accuracy: 0.9891\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1563 - accuracy: 0.9891\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1679 - accuracy: 1.0000\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1721 - accuracy: 0.9891\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1620 - accuracy: 1.0000\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1540 - accuracy: 0.9891\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1603 - accuracy: 0.9891\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1661 - accuracy: 0.9891\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1597 - accuracy: 0.9891\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1523 - accuracy: 0.9891\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1539 - accuracy: 0.9891\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1590 - accuracy: 0.9891\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1589 - accuracy: 0.9891\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1532 - accuracy: 0.9891\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1496 - accuracy: 0.9891\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1515 - accuracy: 0.9891\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1545 - accuracy: 0.9891\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1547 - accuracy: 0.9891\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1516 - accuracy: 0.9891\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1482 - accuracy: 0.9891\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1470 - accuracy: 0.9891\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1481 - accuracy: 0.9891\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1497 - accuracy: 0.9891\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1506 - accuracy: 0.9891\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1503 - accuracy: 1.0000\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1490 - accuracy: 0.9891\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1473 - accuracy: 0.9891\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1456 - accuracy: 0.9891\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1442 - accuracy: 0.9891\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1432 - accuracy: 0.9891\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1426 - accuracy: 0.9891\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1423 - accuracy: 0.9891\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1422 - accuracy: 0.9891\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1425 - accuracy: 0.9891\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1436 - accuracy: 0.9891\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1465 - accuracy: 1.0000\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1541 - accuracy: 0.9891\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1761 - accuracy: 0.9783\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2345 - accuracy: 0.9565\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5021 - accuracy: 0.7717\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5398 - accuracy: 0.7283\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.7090 - accuracy: 0.7717\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2910 - accuracy: 0.8804\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6391 - accuracy: 0.7500\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2241 - accuracy: 0.9674\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5277 - accuracy: 0.8370\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3035 - accuracy: 0.9239\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3523 - accuracy: 0.9457\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4293 - accuracy: 0.9130\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2721 - accuracy: 0.9783\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4134 - accuracy: 0.8478\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3456 - accuracy: 0.9022\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2768 - accuracy: 0.9783\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3765 - accuracy: 0.9565\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3000 - accuracy: 0.9783\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2593 - accuracy: 0.9891\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3326 - accuracy: 0.8804\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2605 - accuracy: 0.9783\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2456 - accuracy: 0.9891\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2918 - accuracy: 0.9565\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2287 - accuracy: 0.9891\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2315 - accuracy: 0.9783\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2532 - accuracy: 0.9565\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1997 - accuracy: 0.9891\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2298 - accuracy: 0.9783\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2079 - accuracy: 0.9891\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1913 - accuracy: 0.9891\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2127 - accuracy: 0.9783\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1756 - accuracy: 0.9891\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2035 - accuracy: 0.9783\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1726 - accuracy: 0.9891\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1892 - accuracy: 0.9891\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1735 - accuracy: 1.0000\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1785 - accuracy: 0.9891\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1723 - accuracy: 0.9891\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1711 - accuracy: 1.0000\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1698 - accuracy: 1.0000\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1657 - accuracy: 0.9891\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1661 - accuracy: 0.9891\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1624 - accuracy: 1.0000\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1628 - accuracy: 1.0000\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1593 - accuracy: 0.9891\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1595 - accuracy: 0.9891\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1570 - accuracy: 0.9891\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1566 - accuracy: 0.9891\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1548 - accuracy: 0.9891\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1541 - accuracy: 0.9891\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1532 - accuracy: 0.9891\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1520 - accuracy: 0.9891\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1516 - accuracy: 0.9891\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1502 - accuracy: 0.9891\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1502 - accuracy: 0.9891\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1485 - accuracy: 0.9891\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1489 - accuracy: 0.9891\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1469 - accuracy: 0.9891\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1477 - accuracy: 0.9891\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1454 - accuracy: 0.9891\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1468 - accuracy: 0.9891\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1441 - accuracy: 0.9891\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1450 - accuracy: 0.9891\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1432 - accuracy: 0.9891\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1426 - accuracy: 0.9891\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1423 - accuracy: 0.9891\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1406 - accuracy: 0.9891\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1408 - accuracy: 0.9891\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1393 - accuracy: 0.9891\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1390 - accuracy: 0.9891\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1386 - accuracy: 0.9891\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1376 - accuracy: 0.9891\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1376 - accuracy: 0.9891\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1367 - accuracy: 0.9891\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1362 - accuracy: 0.9891\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1359 - accuracy: 0.9891\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1350 - accuracy: 0.9891\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1344 - accuracy: 0.9891\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1341 - accuracy: 0.9891\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1334 - accuracy: 0.9891\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1328 - accuracy: 0.9891\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1324 - accuracy: 0.9891\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1317 - accuracy: 0.9891\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1312 - accuracy: 0.9891\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1307 - accuracy: 0.9891\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1302 - accuracy: 0.9891\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1298 - accuracy: 0.9891\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1295 - accuracy: 0.9891\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1290 - accuracy: 0.9891\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1287 - accuracy: 0.9891\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1283 - accuracy: 0.9891\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1280 - accuracy: 0.9891\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1277 - accuracy: 0.9891\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1273 - accuracy: 0.9891\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1270 - accuracy: 0.9891\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1267 - accuracy: 0.9891\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1264 - accuracy: 0.9891\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1261 - accuracy: 0.9891\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1258 - accuracy: 0.9891\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1255 - accuracy: 0.9891\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1252 - accuracy: 0.9891\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1250 - accuracy: 0.9891\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1247 - accuracy: 0.9891\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1244 - accuracy: 0.9891\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1242 - accuracy: 0.9891\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1239 - accuracy: 0.9891\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1236 - accuracy: 0.9891\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1234 - accuracy: 0.9891\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1231 - accuracy: 0.9891\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 147us/step - loss: 0.1228 - accuracy: 0.9891\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1226 - accuracy: 0.9891\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1223 - accuracy: 0.9891\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1221 - accuracy: 0.9891\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1218 - accuracy: 0.9891\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1216 - accuracy: 0.9891\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1213 - accuracy: 0.9891\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1211 - accuracy: 0.9891\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1209 - accuracy: 0.9891\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1206 - accuracy: 0.9891\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1204 - accuracy: 0.9891\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1201 - accuracy: 0.9891\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1199 - accuracy: 0.9891\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1197 - accuracy: 0.9891\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1195 - accuracy: 0.9891\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1193 - accuracy: 0.9891\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1190 - accuracy: 0.9891\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1187 - accuracy: 0.9891\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1185 - accuracy: 0.9891\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1183 - accuracy: 0.9891\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1181 - accuracy: 0.9891\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1179 - accuracy: 1.0000\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1176 - accuracy: 0.9891\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1174 - accuracy: 1.0000\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1172 - accuracy: 0.9891\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1169 - accuracy: 0.9891\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1167 - accuracy: 1.0000\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1165 - accuracy: 0.9891\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1163 - accuracy: 1.0000\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1161 - accuracy: 0.9891\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1159 - accuracy: 1.0000\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1157 - accuracy: 0.9891\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1155 - accuracy: 1.0000\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1153 - accuracy: 0.9891\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1151 - accuracy: 1.0000\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1149 - accuracy: 1.0000\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1147 - accuracy: 1.0000\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1146 - accuracy: 0.9891\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1144 - accuracy: 1.0000\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1143 - accuracy: 0.9891\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1140 - accuracy: 0.9891\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1140 - accuracy: 1.0000\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1141 - accuracy: 0.9891\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1142 - accuracy: 1.0000\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1143 - accuracy: 0.9891\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1147 - accuracy: 1.0000\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1158 - accuracy: 0.9891\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1172 - accuracy: 1.0000\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1197 - accuracy: 0.9891\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1237 - accuracy: 1.0000\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1299 - accuracy: 0.9891\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1425 - accuracy: 0.9891\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1679 - accuracy: 0.9783\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2559 - accuracy: 0.8587\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5326 - accuracy: 0.7391\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 1.6406 - accuracy: 0.7065\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 1.0196 - accuracy: 0.7283\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.5594 - accuracy: 0.9130\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6877 - accuracy: 0.9783\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.8681 - accuracy: 0.5217\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6422 - accuracy: 0.9565\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6389 - accuracy: 0.8152\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.7750 - accuracy: 0.7935\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.8253 - accuracy: 0.7935\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.7658 - accuracy: 0.8043\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6799 - accuracy: 0.8261\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6893 - accuracy: 0.8587\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7242 - accuracy: 0.8913\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6832 - accuracy: 0.9239\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6013 - accuracy: 0.9565\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5453 - accuracy: 0.9565\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.5128 - accuracy: 0.9565\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.4840 - accuracy: 0.9457\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4606 - accuracy: 0.9565\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.4388 - accuracy: 0.9565\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4176 - accuracy: 0.9674\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3977 - accuracy: 0.9674\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3792 - accuracy: 0.9783\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3628 - accuracy: 0.9783\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3487 - accuracy: 0.9783\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3363 - accuracy: 0.9891\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3257 - accuracy: 0.9891\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3163 - accuracy: 0.9891\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3070 - accuracy: 0.9891\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2971 - accuracy: 0.9891\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2847 - accuracy: 0.9891\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2717 - accuracy: 0.9891\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2638 - accuracy: 0.9891\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2561 - accuracy: 0.9891\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2493 - accuracy: 0.9891\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2420 - accuracy: 0.9891\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2355 - accuracy: 0.9891\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2290 - accuracy: 0.9891\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2234 - accuracy: 0.9891\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2179 - accuracy: 0.9891\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2130 - accuracy: 0.9891\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2083 - accuracy: 0.9891\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2042 - accuracy: 0.9891\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2000 - accuracy: 0.9891\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1963 - accuracy: 0.9891\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1925 - accuracy: 0.9891\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1891 - accuracy: 0.9891\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1857 - accuracy: 0.9891\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1826 - accuracy: 0.9891\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1796 - accuracy: 0.9891\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1768 - accuracy: 0.9891\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1741 - accuracy: 0.9891\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1717 - accuracy: 0.9891\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1693 - accuracy: 0.9891\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1671 - accuracy: 0.9891\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1650 - accuracy: 0.9891\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1631 - accuracy: 0.9891\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1613 - accuracy: 0.9891\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1597 - accuracy: 0.9891\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1580 - accuracy: 0.9891\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1565 - accuracy: 0.9891\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1551 - accuracy: 0.9891\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1537 - accuracy: 0.9891\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1524 - accuracy: 0.9891\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1512 - accuracy: 0.9891\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1501 - accuracy: 0.9891\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1490 - accuracy: 0.9891\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1480 - accuracy: 0.9891\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1470 - accuracy: 0.9891\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1461 - accuracy: 0.9891\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1452 - accuracy: 0.9891\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1444 - accuracy: 0.9891\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1436 - accuracy: 0.9891\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1428 - accuracy: 0.9891\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1420 - accuracy: 0.9891\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1412 - accuracy: 0.9891\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1405 - accuracy: 0.9891\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1396 - accuracy: 0.9891\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1389 - accuracy: 0.9891\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1381 - accuracy: 0.9891\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1374 - accuracy: 0.9891\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1366 - accuracy: 0.9891\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1359 - accuracy: 0.9891\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1352 - accuracy: 0.9891\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1344 - accuracy: 0.9891\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1338 - accuracy: 0.9891\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1329 - accuracy: 0.9891\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1321 - accuracy: 0.9891\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1314 - accuracy: 0.9891\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1306 - accuracy: 0.9891\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1297 - accuracy: 0.9891\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1290 - accuracy: 0.9891\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1281 - accuracy: 0.9891\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1272 - accuracy: 0.9891\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1263 - accuracy: 0.9891\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1254 - accuracy: 0.9891\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1245 - accuracy: 0.9891\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1236 - accuracy: 0.9891\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1228 - accuracy: 0.9891\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1221 - accuracy: 0.9891\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1214 - accuracy: 0.9891\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1206 - accuracy: 0.9891\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1200 - accuracy: 0.9891\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1193 - accuracy: 1.0000\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1186 - accuracy: 0.9891\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1180 - accuracy: 1.0000\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1174 - accuracy: 1.0000\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1168 - accuracy: 1.0000\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1163 - accuracy: 1.0000\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1158 - accuracy: 1.0000\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1153 - accuracy: 1.0000\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1148 - accuracy: 1.0000\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1144 - accuracy: 1.0000\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1140 - accuracy: 1.0000\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1136 - accuracy: 1.0000\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1132 - accuracy: 1.0000\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1128 - accuracy: 1.0000\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1123 - accuracy: 1.0000\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1118 - accuracy: 1.0000\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1113 - accuracy: 1.0000\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1108 - accuracy: 1.0000\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1103 - accuracy: 1.0000\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1098 - accuracy: 1.0000\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1094 - accuracy: 1.0000\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1090 - accuracy: 1.0000\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 96us/step - loss: 0.1086 - accuracy: 1.0000\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1082 - accuracy: 1.0000\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1079 - accuracy: 1.0000\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1075 - accuracy: 1.0000\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1072 - accuracy: 1.0000\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1069 - accuracy: 1.0000\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1067 - accuracy: 1.0000\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1064 - accuracy: 1.0000\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1061 - accuracy: 1.0000\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1059 - accuracy: 1.0000\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1057 - accuracy: 1.0000\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1055 - accuracy: 1.0000\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1052 - accuracy: 1.0000\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1050 - accuracy: 1.0000\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1049 - accuracy: 1.0000\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1047 - accuracy: 1.0000\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1046 - accuracy: 1.0000\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1046 - accuracy: 1.0000\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1046 - accuracy: 1.0000\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1047 - accuracy: 1.0000\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1048 - accuracy: 1.0000\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1052 - accuracy: 0.9891\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1058 - accuracy: 1.0000\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1069 - accuracy: 0.9891\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1085 - accuracy: 1.0000\n",
            "30/30 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "10c504e8-d914-46fb-e2a5-1dab5056fe5f",
        "id": "caloL2HE457m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "92f887e5-eeba-4de6-99a1-c65b484b6581",
        "id": "Pw5EEtXv457s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333373069763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZadRXhHi4574"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}