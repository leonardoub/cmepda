{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brats_classification_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0dJodZD/8eywb50reEA3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/brats_classification_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkyGJ1ldXJ8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "d3f32fc3-2a9d-4076-b9f5-5a93ccfb5dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "f32ff460-2512-43d6-b50d-9f303563c9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>141</td>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>142</td>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>143</td>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>144</td>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>145</td>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows Ã— 708 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0            ID        Date  ...  TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0             0  TCGA-02-0006  1996.08.23  ...     0.052741    2.00000        GBM\n",
              "1             1  TCGA-02-0009  1997.06.14  ...     0.094336   91.47360        GBM\n",
              "2             2  TCGA-02-0011  1998.02.01  ...     0.096035  272.42900        GBM\n",
              "3             3  TCGA-02-0027  1999.03.28  ...     0.096470  128.46800        GBM\n",
              "4             4  TCGA-02-0033  1997.05.26  ...     0.096894  240.77800        GBM\n",
              "..          ...           ...         ...  ...          ...        ...        ...\n",
              "141         141  TCGA-HT-7694  1995.04.04  ...     0.090456  719.23800        LGG\n",
              "142         142  TCGA-HT-8018  1997.04.11  ...     0.054307    2.00000        LGG\n",
              "143         143  TCGA-HT-8111  1998.03.30  ...     0.126712    7.06744        LGG\n",
              "144         144  TCGA-HT-8114  1998.10.30  ...     0.107433   15.52240        LGG\n",
              "145         145  TCGA-HT-8563  1998.12.09  ...     0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 708 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "c3e08aee-55a0-4330-c1e9-1ff0cb4e5760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'ID', 'Date', 'VOLUME_ET', 'VOLUME_NET', 'VOLUME_ED',\n",
              "       'VOLUME_TC', 'VOLUME_WT', 'VOLUME_BRAIN', 'VOLUME_ET_OVER_NET',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=708)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'Unnamed: 0', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BoXqyfbpydt",
        "colab_type": "text"
      },
      "source": [
        "#NO K-FOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=0.2, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6iyOqcBq0RC"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "63cb8030-5ef2-45d6-dce9-11d4f906bf17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "outputId": "ec390044-e273-4558-b8df-59b81c059b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(9,), kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a0da9d1e-4d51-4dd6-90f0-8368595c3f86",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9a8b4112-fa50-4991-9316-8625fe090ca6",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=10)\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-924704984b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n\u001b[1;32m      5\u001b[0m                       epochs= num_epochs, batch_size=10)\n",
            "\u001b[0;32m<ipython-input-49-aba359fdb162>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_subclassed_network\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_base_init\u001b[0;34m(self, name, trainable, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m'It looks like you are trying to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;34m'a version of multi-backend Keras that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m'does not support TensorFlow 2.0. We recommend '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5de62dc4-a760-4547-98d9-7c9c98079e7d",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d8a892e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni3MeCJxzLs55IHDOuTj3/1oSznFf9Km4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2eadb653-9952-4361-f70f-4018d98f61ac",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d89ef6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAWOKQLDMIwC5/8DAkcGwuUf3u8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3b42c5fa-33d7-4012-b9ad-fec3e34b7dad",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=105, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "105/105 [==============================] - 3s 25ms/step - loss: 1.3937 - acc: 0.1905\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 1.4110 - acc: 0.1810\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.3383 - acc: 0.2571\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.3043 - acc: 0.2381\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.3153 - acc: 0.2286\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.3610 - acc: 0.2381\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2880 - acc: 0.1810\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.3193 - acc: 0.2286\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2666 - acc: 0.2381\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2252 - acc: 0.2952\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.2234 - acc: 0.3238\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1984 - acc: 0.3619\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2300 - acc: 0.3524\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.2067 - acc: 0.3810\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.1403 - acc: 0.3333\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1621 - acc: 0.3810\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1743 - acc: 0.3905\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1227 - acc: 0.4667\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.1019 - acc: 0.4571\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1252 - acc: 0.4667\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.1100 - acc: 0.5238\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0915 - acc: 0.4762\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.1084 - acc: 0.4762\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1050 - acc: 0.4857\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0968 - acc: 0.5143\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 1.0940 - acc: 0.4857\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0622 - acc: 0.5333\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0457 - acc: 0.4762\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0667 - acc: 0.5619\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0476 - acc: 0.5238\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0391 - acc: 0.5048\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0422 - acc: 0.4857\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0038 - acc: 0.5619\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0137 - acc: 0.5810\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0271 - acc: 0.5143\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.9952 - acc: 0.5810\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9775 - acc: 0.5905\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9867 - acc: 0.5524\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9978 - acc: 0.5333\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9850 - acc: 0.5524\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0007 - acc: 0.5238\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9849 - acc: 0.5714\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9770 - acc: 0.5333\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.9846 - acc: 0.5905\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.9231 - acc: 0.6286\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.9772 - acc: 0.6286\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9700 - acc: 0.5524\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9697 - acc: 0.5619\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.9433 - acc: 0.6000\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9482 - acc: 0.5905\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0039 - acc: 0.5238\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.9589 - acc: 0.5619\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9850 - acc: 0.5238\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9561 - acc: 0.6381\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9695 - acc: 0.6000\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.9591 - acc: 0.5714\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9592 - acc: 0.5810\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.9183 - acc: 0.6857\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9185 - acc: 0.6667\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9377 - acc: 0.6095\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9160 - acc: 0.6286\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.8839 - acc: 0.6571\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9107 - acc: 0.6095\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9032 - acc: 0.6000\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8975 - acc: 0.6476\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9169 - acc: 0.6381\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.9451 - acc: 0.5714\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9131 - acc: 0.5905\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9219 - acc: 0.6381\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9005 - acc: 0.6286\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9056 - acc: 0.6381\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9099 - acc: 0.6286\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.9113 - acc: 0.6286\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8849 - acc: 0.6476\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9197 - acc: 0.6381\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8924 - acc: 0.6667\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8921 - acc: 0.6571\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8889 - acc: 0.6381\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8970 - acc: 0.6286\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8806 - acc: 0.6286\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8833 - acc: 0.6667\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9167 - acc: 0.6000\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.8883 - acc: 0.6095\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8831 - acc: 0.6190\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8809 - acc: 0.6286\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8579 - acc: 0.6762\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8746 - acc: 0.6095\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.9011 - acc: 0.6095\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8654 - acc: 0.6381\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8840 - acc: 0.6286\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8554 - acc: 0.6667\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8368 - acc: 0.7048\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8601 - acc: 0.6571\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8930 - acc: 0.6190\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8767 - acc: 0.6762\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8916 - acc: 0.6190\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8798 - acc: 0.6476\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8708 - acc: 0.6571\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8654 - acc: 0.6762\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8620 - acc: 0.6476\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8637 - acc: 0.6286\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8318 - acc: 0.6667\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8762 - acc: 0.6762\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8185 - acc: 0.7048\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8340 - acc: 0.7048\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8421 - acc: 0.6476\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.8477 - acc: 0.6857\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8383 - acc: 0.6571\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.6667\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.8129 - acc: 0.6952\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8356 - acc: 0.6762\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8471 - acc: 0.6952\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8344 - acc: 0.6571\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8355 - acc: 0.6667\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8443 - acc: 0.6571\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8416 - acc: 0.6857\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8074 - acc: 0.7048\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8425 - acc: 0.6667\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8138 - acc: 0.6857\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8200 - acc: 0.6571\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8418 - acc: 0.6667\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8391 - acc: 0.6476\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8314 - acc: 0.7143\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7967 - acc: 0.7048\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7981 - acc: 0.7048\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8365 - acc: 0.6476\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8111 - acc: 0.6667\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8153 - acc: 0.6571\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8377 - acc: 0.6381\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.8188 - acc: 0.6667\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8090 - acc: 0.7333\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8031 - acc: 0.6762\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8067 - acc: 0.6667\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7981 - acc: 0.6952\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8168 - acc: 0.6571\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8258 - acc: 0.6762\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8166 - acc: 0.6762\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7743 - acc: 0.7333\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7744 - acc: 0.7048\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.7991 - acc: 0.7238\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8120 - acc: 0.6381\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7765 - acc: 0.7238\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7893 - acc: 0.7143\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7863 - acc: 0.6381\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8006 - acc: 0.6667\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7592 - acc: 0.7143\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7818 - acc: 0.6762\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7868 - acc: 0.6857\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7676 - acc: 0.6857\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.7504 - acc: 0.7143\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7969 - acc: 0.6952\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7639 - acc: 0.7143\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8001 - acc: 0.6571\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7951 - acc: 0.6667\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7913 - acc: 0.7238\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7812 - acc: 0.6857\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8231 - acc: 0.6476\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7422 - acc: 0.7333\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7702 - acc: 0.6667\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7592 - acc: 0.6762\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7420 - acc: 0.7143\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7269 - acc: 0.7714\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7695 - acc: 0.6667\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7669 - acc: 0.7333\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7546 - acc: 0.7048\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7713 - acc: 0.6667\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.7573 - acc: 0.6952\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7701 - acc: 0.7238\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7329 - acc: 0.7143\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7711 - acc: 0.7238\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7547 - acc: 0.6857\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7535 - acc: 0.7048\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7643 - acc: 0.6857\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7505 - acc: 0.6952\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7354 - acc: 0.7524\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7263 - acc: 0.7143\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7716 - acc: 0.6381\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7174 - acc: 0.7048\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7159 - acc: 0.7238\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.7346 - acc: 0.7143\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7297 - acc: 0.7143\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7288 - acc: 0.7143\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7298 - acc: 0.7143\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7516 - acc: 0.7714\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6870 - acc: 0.7238\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.7613 - acc: 0.6952\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6978 - acc: 0.7429\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7369 - acc: 0.6667\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7168 - acc: 0.7048\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7670 - acc: 0.6762\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7620 - acc: 0.6762\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7233 - acc: 0.7238\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7024 - acc: 0.7810\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7226 - acc: 0.6857\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7392 - acc: 0.7143\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7229 - acc: 0.7143\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7348 - acc: 0.6762\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7374 - acc: 0.7143\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7492 - acc: 0.7048\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7019 - acc: 0.6952\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7199 - acc: 0.7048\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6964 - acc: 0.7143\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7337 - acc: 0.7048\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7473 - acc: 0.6857\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7461 - acc: 0.6857\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7218 - acc: 0.7238\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7239 - acc: 0.7524\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7065 - acc: 0.7333\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7237 - acc: 0.7238\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7258 - acc: 0.7238\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7144 - acc: 0.7619\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7182 - acc: 0.6857\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6942 - acc: 0.7333\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7236 - acc: 0.6857\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6900 - acc: 0.7143\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7060 - acc: 0.7143\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6999 - acc: 0.7238\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6915 - acc: 0.7048\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6888 - acc: 0.7238\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6937 - acc: 0.7333\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7502 - acc: 0.6476\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6903 - acc: 0.7429\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6657 - acc: 0.7429\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6865 - acc: 0.7143\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7035 - acc: 0.7048\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6866 - acc: 0.6952\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.6632 - acc: 0.7238\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 108us/step - loss: 0.6999 - acc: 0.6762\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.6837 - acc: 0.7143\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.7002 - acc: 0.7238\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6856 - acc: 0.7524\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6607 - acc: 0.7333\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6844 - acc: 0.6952\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6536 - acc: 0.7619\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6380 - acc: 0.7333\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6792 - acc: 0.7143\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6552 - acc: 0.7714\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6618 - acc: 0.7619\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.6750 - acc: 0.7143\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6748 - acc: 0.6857\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6498 - acc: 0.7714\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6580 - acc: 0.7048\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6946 - acc: 0.7429\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.6465 - acc: 0.7619\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6815 - acc: 0.7333\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6653 - acc: 0.7429\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 110us/step - loss: 0.6445 - acc: 0.7524\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6649 - acc: 0.7333\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6202 - acc: 0.7905\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6587 - acc: 0.7333\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6323 - acc: 0.8000\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6880 - acc: 0.7524\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6480 - acc: 0.7619\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6189 - acc: 0.7619\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6628 - acc: 0.7048\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6104 - acc: 0.7810\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6605 - acc: 0.7619\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6486 - acc: 0.7143\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6182 - acc: 0.7905\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6133 - acc: 0.7714\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6410 - acc: 0.7429\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6380 - acc: 0.7810\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6291 - acc: 0.7714\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7031 - acc: 0.7048\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6810 - acc: 0.6857\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6209 - acc: 0.7714\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6352 - acc: 0.7143\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6085 - acc: 0.7619\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6462 - acc: 0.7524\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6233 - acc: 0.7714\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6891 - acc: 0.7429\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6082 - acc: 0.7714\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5983 - acc: 0.7810\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6324 - acc: 0.7524\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6702 - acc: 0.7333\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6971 - acc: 0.7143\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6366 - acc: 0.7619\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6421 - acc: 0.7810\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6061 - acc: 0.7619\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6071 - acc: 0.7905\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6328 - acc: 0.7429\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 109us/step - loss: 0.6007 - acc: 0.7714\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.5822 - acc: 0.7524\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6877 - acc: 0.7048\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6212 - acc: 0.7429\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6150 - acc: 0.7905\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6213 - acc: 0.7524\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5799 - acc: 0.7619\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5769 - acc: 0.7619\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5980 - acc: 0.7905\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5831 - acc: 0.7619\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6825 - acc: 0.6857\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6457 - acc: 0.7143\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.6066 - acc: 0.7524\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6260 - acc: 0.7905\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6378 - acc: 0.7333\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.6368 - acc: 0.7238\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6001 - acc: 0.7810\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6120 - acc: 0.7714\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.5727 - acc: 0.8000\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6005 - acc: 0.8000\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.5702 - acc: 0.8095\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6287 - acc: 0.7524\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.5972 - acc: 0.7714\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.6073 - acc: 0.7524\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5926 - acc: 0.7714\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5714 - acc: 0.7238\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.5869 - acc: 0.7810\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6113 - acc: 0.7810\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6140 - acc: 0.7429\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.5803 - acc: 0.7619\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5706 - acc: 0.7810\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6087 - acc: 0.7714\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5813 - acc: 0.7810\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5561 - acc: 0.7905\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5470 - acc: 0.7524\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5669 - acc: 0.7810\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.5655 - acc: 0.8000\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5719 - acc: 0.7429\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5305 - acc: 0.8190\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5607 - acc: 0.7810\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.6242 - acc: 0.7429\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5409 - acc: 0.8095\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.5532 - acc: 0.7714\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5900 - acc: 0.7524\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5708 - acc: 0.7714\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5650 - acc: 0.7810\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5533 - acc: 0.7619\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5497 - acc: 0.7810\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5423 - acc: 0.7905\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5761 - acc: 0.7810\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5994 - acc: 0.7619\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5869 - acc: 0.7714\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5826 - acc: 0.7524\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5619 - acc: 0.7810\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5462 - acc: 0.7810\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5880 - acc: 0.7619\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4954 - acc: 0.8000\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5612 - acc: 0.8190\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5274 - acc: 0.8095\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.5572 - acc: 0.8000\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.5916 - acc: 0.7524\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5793 - acc: 0.7714\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5331 - acc: 0.8190\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 92us/step - loss: 0.5683 - acc: 0.7810\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5823 - acc: 0.7524\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.5648 - acc: 0.8190\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5606 - acc: 0.7714\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5593 - acc: 0.7714\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5455 - acc: 0.8095\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5024 - acc: 0.7905\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5504 - acc: 0.8000\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5124 - acc: 0.8190\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5426 - acc: 0.7810\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5371 - acc: 0.7810\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5563 - acc: 0.8000\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5406 - acc: 0.7810\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5252 - acc: 0.8190\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6024 - acc: 0.7810\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5729 - acc: 0.7905\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5388 - acc: 0.8190\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5742 - acc: 0.7714\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5632 - acc: 0.7429\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5894 - acc: 0.7238\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5630 - acc: 0.8095\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5484 - acc: 0.7714\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5091 - acc: 0.8476\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5136 - acc: 0.8095\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5206 - acc: 0.8476\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5600 - acc: 0.8000\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5480 - acc: 0.7905\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4730 - acc: 0.8190\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5260 - acc: 0.8190\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5064 - acc: 0.8190\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5475 - acc: 0.7905\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4981 - acc: 0.8095\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5318 - acc: 0.8000\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5693 - acc: 0.7810\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5723 - acc: 0.7905\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5717 - acc: 0.8000\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5135 - acc: 0.8190\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5572 - acc: 0.8095\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5282 - acc: 0.7905\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5126 - acc: 0.8381\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4881 - acc: 0.8095\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4776 - acc: 0.8476\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5178 - acc: 0.7905\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5326 - acc: 0.8000\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5182 - acc: 0.8095\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4901 - acc: 0.8381\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4890 - acc: 0.8286\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4861 - acc: 0.8571\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4824 - acc: 0.8286\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5039 - acc: 0.7810\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5207 - acc: 0.8000\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5202 - acc: 0.8095\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4921 - acc: 0.8571\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5152 - acc: 0.7810\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5603 - acc: 0.7714\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5383 - acc: 0.8095\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6292 - acc: 0.7429\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5140 - acc: 0.8476\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5244 - acc: 0.7714\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4850 - acc: 0.8286\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4961 - acc: 0.8571\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5365 - acc: 0.8190\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5057 - acc: 0.8476\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5148 - acc: 0.8476\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5360 - acc: 0.8000\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4821 - acc: 0.8381\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5210 - acc: 0.8000\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4814 - acc: 0.8381\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5187 - acc: 0.8381\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5380 - acc: 0.7905\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4846 - acc: 0.8190\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4936 - acc: 0.8286\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5070 - acc: 0.8000\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5516 - acc: 0.7810\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5179 - acc: 0.7905\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4860 - acc: 0.8381\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5674 - acc: 0.8000\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5396 - acc: 0.7810\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4614 - acc: 0.8571\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5257 - acc: 0.8190\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4681 - acc: 0.8286\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5512 - acc: 0.7619\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4854 - acc: 0.8381\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4858 - acc: 0.8000\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4860 - acc: 0.8000\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4807 - acc: 0.8571\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4906 - acc: 0.8000\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4780 - acc: 0.7905\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5096 - acc: 0.8476\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4743 - acc: 0.8190\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4636 - acc: 0.8190\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4974 - acc: 0.7905\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5082 - acc: 0.8286\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5294 - acc: 0.7619\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4874 - acc: 0.8095\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4933 - acc: 0.8095\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4857 - acc: 0.8571\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5505 - acc: 0.7810\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4831 - acc: 0.8571\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5045 - acc: 0.8476\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5620 - acc: 0.7619\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4750 - acc: 0.8571\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4606 - acc: 0.8476\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4912 - acc: 0.8095\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4785 - acc: 0.8381\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5239 - acc: 0.8190\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.5097 - acc: 0.8190\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4812 - acc: 0.8381\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5310 - acc: 0.8571\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4752 - acc: 0.8286\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4298 - acc: 0.8762\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5146 - acc: 0.8095\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4754 - acc: 0.8286\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4081 - acc: 0.9048\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4854 - acc: 0.8571\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4772 - acc: 0.8381\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4651 - acc: 0.8952\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5254 - acc: 0.8190\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4216 - acc: 0.8857\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5029 - acc: 0.8000\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4360 - acc: 0.8667\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4753 - acc: 0.8381\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4594 - acc: 0.8190\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4676 - acc: 0.8381\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5211 - acc: 0.8190\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4348 - acc: 0.8952\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4550 - acc: 0.8667\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4293 - acc: 0.8667\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4544 - acc: 0.8381\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5068 - acc: 0.8667\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5010 - acc: 0.8286\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5277 - acc: 0.7714\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4518 - acc: 0.8762\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4479 - acc: 0.8667\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4102 - acc: 0.8381\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4426 - acc: 0.8857\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4721 - acc: 0.8286\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.4752 - acc: 0.8667\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4016 - acc: 0.8667\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4381 - acc: 0.8667\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4737 - acc: 0.8190\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4214 - acc: 0.9048\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5498 - acc: 0.8000\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4226 - acc: 0.8667\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4171 - acc: 0.8857\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4739 - acc: 0.8762\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5061 - acc: 0.8000\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4919 - acc: 0.8571\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4796 - acc: 0.8381\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3873 - acc: 0.8952\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4389 - acc: 0.8571\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.4484 - acc: 0.8571\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4019 - acc: 0.9048\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4456 - acc: 0.8476\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4179 - acc: 0.8952\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3975 - acc: 0.8762\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4385 - acc: 0.8762\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4336 - acc: 0.8762\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4994 - acc: 0.7905\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3745 - acc: 0.9333\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3977 - acc: 0.9048\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4423 - acc: 0.8762\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4538 - acc: 0.8762\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3897 - acc: 0.8952\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4028 - acc: 0.8476\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3861 - acc: 0.8857\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4313 - acc: 0.8762\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4624 - acc: 0.8381\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3772 - acc: 0.8952\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5134 - acc: 0.8286\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4695 - acc: 0.8762\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4434 - acc: 0.8476\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4539 - acc: 0.8000\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4144 - acc: 0.8571\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4762 - acc: 0.8286\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4579 - acc: 0.8190\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4248 - acc: 0.8952\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4672 - acc: 0.8857\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4722 - acc: 0.8762\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4828 - acc: 0.8000\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4192 - acc: 0.8571\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4599 - acc: 0.8190\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3661 - acc: 0.9048\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4184 - acc: 0.8952\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4072 - acc: 0.8667\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4245 - acc: 0.8571\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4678 - acc: 0.8476\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4224 - acc: 0.9048\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4046 - acc: 0.8571\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4076 - acc: 0.8857\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4369 - acc: 0.8857\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3940 - acc: 0.8667\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4163 - acc: 0.8476\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3502 - acc: 0.8952\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3672 - acc: 0.8952\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4026 - acc: 0.9048\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3913 - acc: 0.9048\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4593 - acc: 0.8286\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3873 - acc: 0.9048\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4513 - acc: 0.8476\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3996 - acc: 0.9048\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3739 - acc: 0.8857\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4336 - acc: 0.8762\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4501 - acc: 0.8571\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3540 - acc: 0.9143\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4088 - acc: 0.8857\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3663 - acc: 0.9238\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4599 - acc: 0.8571\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4180 - acc: 0.8857\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4177 - acc: 0.8476\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4711 - acc: 0.8381\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3752 - acc: 0.9048\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4234 - acc: 0.8571\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3704 - acc: 0.9238\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4490 - acc: 0.8286\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4360 - acc: 0.8762\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3578 - acc: 0.9143\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.4461 - acc: 0.8381\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3965 - acc: 0.8571\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4586 - acc: 0.8190\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4281 - acc: 0.8667\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4172 - acc: 0.9143\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4162 - acc: 0.8762\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3813 - acc: 0.8952\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3790 - acc: 0.9143\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4441 - acc: 0.8857\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3928 - acc: 0.8762\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4202 - acc: 0.8952\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3211 - acc: 0.9524\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4011 - acc: 0.8857\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4116 - acc: 0.8952\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4072 - acc: 0.8857\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4103 - acc: 0.8762\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4144 - acc: 0.8857\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4339 - acc: 0.8381\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4116 - acc: 0.8857\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4076 - acc: 0.8857\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3374 - acc: 0.8952\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4114 - acc: 0.8952\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3763 - acc: 0.9429\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.8857\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3930 - acc: 0.9048\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4283 - acc: 0.8667\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4002 - acc: 0.8952\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4396 - acc: 0.8476\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3580 - acc: 0.9143\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3413 - acc: 0.9333\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4069 - acc: 0.8762\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3863 - acc: 0.9048\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3636 - acc: 0.9238\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3624 - acc: 0.9238\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3756 - acc: 0.9143\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3441 - acc: 0.9143\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4243 - acc: 0.9048\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4019 - acc: 0.8476\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4213 - acc: 0.8762\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4197 - acc: 0.8762\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3770 - acc: 0.8952\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3680 - acc: 0.9048\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3686 - acc: 0.9238\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3992 - acc: 0.8857\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4400 - acc: 0.8762\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3825 - acc: 0.8857\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3245 - acc: 0.9619\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3952 - acc: 0.8667\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3555 - acc: 0.9048\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3488 - acc: 0.9143\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3560 - acc: 0.9143\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3519 - acc: 0.9238\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3646 - acc: 0.9048\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4008 - acc: 0.8667\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4256 - acc: 0.8762\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4372 - acc: 0.8286\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3627 - acc: 0.9048\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3695 - acc: 0.9048\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3597 - acc: 0.8952\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3990 - acc: 0.9048\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3542 - acc: 0.8952\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3681 - acc: 0.8952\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4121 - acc: 0.8667\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4011 - acc: 0.8952\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3884 - acc: 0.9238\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3514 - acc: 0.8952\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3564 - acc: 0.9143\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3340 - acc: 0.9238\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3491 - acc: 0.8857\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3444 - acc: 0.8952\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3400 - acc: 0.9238\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3887 - acc: 0.8667\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3900 - acc: 0.9048\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3497 - acc: 0.9238\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3638 - acc: 0.9048\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3723 - acc: 0.8571\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3636 - acc: 0.9333\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3626 - acc: 0.9143\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3728 - acc: 0.8762\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3782 - acc: 0.9238\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3645 - acc: 0.8857\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4445 - acc: 0.8667\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4511 - acc: 0.8857\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3846 - acc: 0.9333\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3771 - acc: 0.8857\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3995 - acc: 0.8952\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3425 - acc: 0.8952\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3519 - acc: 0.9238\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4161 - acc: 0.8476\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3776 - acc: 0.9048\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3395 - acc: 0.9143\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3927 - acc: 0.8762\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3972 - acc: 0.8381\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4097 - acc: 0.8667\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3954 - acc: 0.8667\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3211 - acc: 0.9619\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3614 - acc: 0.8762\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4170 - acc: 0.8571\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3720 - acc: 0.8857\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4011 - acc: 0.8571\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3415 - acc: 0.9048\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3250 - acc: 0.9143\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3016 - acc: 0.9333\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3384 - acc: 0.9238\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3508 - acc: 0.9143\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4041 - acc: 0.8952\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3876 - acc: 0.8667\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3593 - acc: 0.9048\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3421 - acc: 0.9333\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4155 - acc: 0.8857\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3745 - acc: 0.9143\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3582 - acc: 0.8952\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3687 - acc: 0.9048\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3498 - acc: 0.9143\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3376 - acc: 0.9429\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4174 - acc: 0.8857\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4081 - acc: 0.8762\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3452 - acc: 0.9333\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3209 - acc: 0.9143\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3170 - acc: 0.9238\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3024 - acc: 0.9524\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3640 - acc: 0.9048\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3165 - acc: 0.9333\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3376 - acc: 0.9143\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3761 - acc: 0.8952\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3442 - acc: 0.9048\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4584 - acc: 0.9238\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3818 - acc: 0.8857\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3093 - acc: 0.9524\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3184 - acc: 0.9238\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3437 - acc: 0.9048\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3521 - acc: 0.9048\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2882 - acc: 0.9714\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8476\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3566 - acc: 0.9143\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3377 - acc: 0.9143\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3698 - acc: 0.8952\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2954 - acc: 0.9143\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3197 - acc: 0.9143\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2871 - acc: 0.9333\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2806 - acc: 0.9619\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3126 - acc: 0.9333\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2936 - acc: 0.9333\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3742 - acc: 0.9143\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3663 - acc: 0.9048\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3087 - acc: 0.9238\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3344 - acc: 0.9429\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2963 - acc: 0.9524\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3995 - acc: 0.9048\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3300 - acc: 0.9143\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3349 - acc: 0.8857\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3319 - acc: 0.9238\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3100 - acc: 0.9333\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4045 - acc: 0.8571\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2995 - acc: 0.9333\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3400 - acc: 0.9429\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3172 - acc: 0.9429\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3277 - acc: 0.9048\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3377 - acc: 0.9238\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3058 - acc: 0.9524\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2997 - acc: 0.9429\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3298 - acc: 0.9429\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3733 - acc: 0.9048\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3700 - acc: 0.8952\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3314 - acc: 0.9238\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3104 - acc: 0.9143\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3294 - acc: 0.9333\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3817 - acc: 0.8952\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3914 - acc: 0.8952\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3228 - acc: 0.9333\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3240 - acc: 0.9048\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3127 - acc: 0.9429\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3161 - acc: 0.9048\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3123 - acc: 0.9238\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3153 - acc: 0.9143\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3129 - acc: 0.9143\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3502 - acc: 0.8667\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2868 - acc: 0.9524\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3523 - acc: 0.8857\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3407 - acc: 0.9238\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3311 - acc: 0.9238\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3410 - acc: 0.9333\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3568 - acc: 0.9238\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3504 - acc: 0.8762\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3587 - acc: 0.9333\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3769 - acc: 0.9238\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3099 - acc: 0.9524\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3133 - acc: 0.9333\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3678 - acc: 0.9048\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2864 - acc: 0.9333\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2896 - acc: 0.9429\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3903 - acc: 0.9048\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2851 - acc: 0.9429\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3082 - acc: 0.9048\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3461 - acc: 0.9048\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3200 - acc: 0.9238\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3359 - acc: 0.9143\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3264 - acc: 0.9333\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2983 - acc: 0.9429\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3267 - acc: 0.9524\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2996 - acc: 0.9238\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3666 - acc: 0.8667\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4223 - acc: 0.9143\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2862 - acc: 0.9429\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2958 - acc: 0.9333\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3446 - acc: 0.9333\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3221 - acc: 0.8952\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3007 - acc: 0.9238\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3186 - acc: 0.9429\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3486 - acc: 0.9143\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2811 - acc: 0.9429\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3619 - acc: 0.8857\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2931 - acc: 0.9429\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3619 - acc: 0.9048\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3024 - acc: 0.9238\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3093 - acc: 0.9333\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2788 - acc: 0.9429\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3064 - acc: 0.9238\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2678 - acc: 0.9619\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3292 - acc: 0.8952\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3468 - acc: 0.9238\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3353 - acc: 0.9238\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3012 - acc: 0.9238\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3243 - acc: 0.9333\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3304 - acc: 0.8952\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3782 - acc: 0.8762\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2678 - acc: 0.9333\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3010 - acc: 0.9143\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2659 - acc: 0.9619\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2936 - acc: 0.9429\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2897 - acc: 0.9429\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3110 - acc: 0.9238\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2843 - acc: 0.9333\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3317 - acc: 0.9143\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3197 - acc: 0.9143\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3111 - acc: 0.9333\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3524 - acc: 0.8857\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3328 - acc: 0.9238\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3213 - acc: 0.9048\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2380 - acc: 0.9714\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3274 - acc: 0.9048\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2990 - acc: 0.9333\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3002 - acc: 0.9333\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.2905 - acc: 0.9524\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3729 - acc: 0.8952\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3255 - acc: 0.9048\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3232 - acc: 0.9048\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3057 - acc: 0.9429\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3216 - acc: 0.9143\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3002 - acc: 0.9238\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3030 - acc: 0.9238\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3089 - acc: 0.9524\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3469 - acc: 0.9143\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3144 - acc: 0.9238\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3255 - acc: 0.9143\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3117 - acc: 0.9238\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2567 - acc: 0.9524\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3124 - acc: 0.9238\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2572 - acc: 0.9619\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3172 - acc: 0.9429\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3155 - acc: 0.8952\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2933 - acc: 0.9429\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3064 - acc: 0.9143\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2614 - acc: 0.9714\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2842 - acc: 0.9429\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3219 - acc: 0.9429\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3016 - acc: 0.9143\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3246 - acc: 0.9238\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3190 - acc: 0.9143\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3018 - acc: 0.9238\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2801 - acc: 0.9524\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2518 - acc: 0.9714\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3283 - acc: 0.9429\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3036 - acc: 0.9524\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2667 - acc: 0.9429\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3155 - acc: 0.9238\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3430 - acc: 0.8952\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3161 - acc: 0.9238\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3301 - acc: 0.8952\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2629 - acc: 0.9619\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2464 - acc: 0.9714\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2679 - acc: 0.9619\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2666 - acc: 0.9619\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3165 - acc: 0.9333\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2845 - acc: 0.9143\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3345 - acc: 0.9238\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2713 - acc: 0.9429\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2727 - acc: 0.9619\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2933 - acc: 0.9143\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2638 - acc: 0.9619\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3139 - acc: 0.9143\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2830 - acc: 0.9333\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2800 - acc: 0.9619\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2940 - acc: 0.9143\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3349 - acc: 0.9619\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2758 - acc: 0.9429\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3298 - acc: 0.9048\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3106 - acc: 0.9048\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2884 - acc: 0.9429\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2903 - acc: 0.9429\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2934 - acc: 0.9524\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3022 - acc: 0.9524\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3691 - acc: 0.8857\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2757 - acc: 0.9333\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2845 - acc: 0.9333\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3219 - acc: 0.9333\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2441 - acc: 0.9714\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2870 - acc: 0.9429\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3348 - acc: 0.9143\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2704 - acc: 0.9429\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3211 - acc: 0.9333\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2973 - acc: 0.9238\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3100 - acc: 0.9048\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2579 - acc: 0.9619\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3476 - acc: 0.9048\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2880 - acc: 0.9333\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2538 - acc: 0.9524\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2826 - acc: 0.9429\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2292 - acc: 0.9714\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3137 - acc: 0.9048\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2789 - acc: 0.9619\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2585 - acc: 0.9714\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3563 - acc: 0.9143\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2816 - acc: 0.9619\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2794 - acc: 0.9524\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3106 - acc: 0.9429\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2951 - acc: 0.9429\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2624 - acc: 0.9714\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2765 - acc: 0.9524\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2588 - acc: 0.9619\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2923 - acc: 0.9238\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2850 - acc: 0.9429\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2480 - acc: 0.9524\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2871 - acc: 0.9619\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2391 - acc: 0.9714\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2386 - acc: 0.9524\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2861 - acc: 0.9524\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2535 - acc: 0.9714\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2579 - acc: 0.9429\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2405 - acc: 0.9714\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2597 - acc: 0.9429\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2847 - acc: 0.9524\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2970 - acc: 0.9429\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2932 - acc: 0.9048\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3279 - acc: 0.9143\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3312 - acc: 0.9048\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3120 - acc: 0.9619\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2328 - acc: 0.9619\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3102 - acc: 0.9238\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3018 - acc: 0.9429\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2797 - acc: 0.9429\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3074 - acc: 0.9333\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2675 - acc: 0.9619\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3064 - acc: 0.9143\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2243 - acc: 0.9714\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2530 - acc: 0.9429\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2894 - acc: 0.9048\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2599 - acc: 0.9619\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2470 - acc: 0.9524\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2711 - acc: 0.9238\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2165 - acc: 0.9810\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2663 - acc: 0.9714\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2501 - acc: 0.9333\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2512 - acc: 0.9524\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3126 - acc: 0.9429\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2858 - acc: 0.9714\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2801 - acc: 0.9524\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2646 - acc: 0.9429\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2568 - acc: 0.9524\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2450 - acc: 0.9619\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2994 - acc: 0.9143\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2059 - acc: 0.9905\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2515 - acc: 0.9238\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2499 - acc: 0.9619\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2577 - acc: 0.9619\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2609 - acc: 0.9524\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2663 - acc: 0.9429\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2238 - acc: 0.9810\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3529 - acc: 0.8952\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.3644 - acc: 0.8857\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2540 - acc: 0.9429\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3209 - acc: 0.9143\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2728 - acc: 0.9333\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2832 - acc: 0.9333\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2770 - acc: 0.9429\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2314 - acc: 0.9810\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2286 - acc: 0.9619\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2739 - acc: 0.9524\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2516 - acc: 0.9714\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3039 - acc: 0.9619\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2512 - acc: 0.9619\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3096 - acc: 0.9333\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2533 - acc: 0.9429\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2335 - acc: 0.9619\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2975 - acc: 0.9238\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2402 - acc: 0.9619\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2494 - acc: 0.9429\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3083 - acc: 0.9238\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3057 - acc: 0.9524\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2864 - acc: 0.9429\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2509 - acc: 0.9524\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2604 - acc: 0.9429\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2255 - acc: 0.9810\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2244 - acc: 0.9619\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2286 - acc: 0.9619\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2938 - acc: 0.9429\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3292 - acc: 0.9143\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3720 - acc: 0.8667\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2987 - acc: 0.9238\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2663 - acc: 0.9524\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2483 - acc: 0.9524\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3037 - acc: 0.9238\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3137 - acc: 0.9048\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2818 - acc: 0.9238\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3126 - acc: 0.9048\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2672 - acc: 0.9714\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2308 - acc: 0.9714\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2599 - acc: 0.9429\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2695 - acc: 0.9524\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2321 - acc: 0.9714\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2632 - acc: 0.9333\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2566 - acc: 0.9524\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2715 - acc: 0.9429\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2441 - acc: 0.9619\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2301 - acc: 0.9619\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2331 - acc: 0.9714\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2937 - acc: 0.9238\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2546 - acc: 0.9524\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3295 - acc: 0.9143\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2731 - acc: 0.9619\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2780 - acc: 0.9333\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2737 - acc: 0.9333\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2695 - acc: 0.9238\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2373 - acc: 0.9619\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2809 - acc: 0.9429\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2419 - acc: 0.9524\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2392 - acc: 0.9524\n",
            "13/13 [==============================] - 1s 85ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff7576f8-bef3-4d03-d19d-4ff558c89c12",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6a12ba6a-9f05-4702-f549-37f98e0cff61",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}