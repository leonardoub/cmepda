{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brats_classification_NN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMmwD+v9nlliV6Pn7ujVTSR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/brats_classification_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkyGJ1ldXJ8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "821a274e-6d2a-4af3-83b1-dfda86154b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_without_HISTO_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "4961d3b1-6dd0-4b65-f5f0-355f1895fd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>141</td>\n",
              "      <td>141</td>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>142</td>\n",
              "      <td>142</td>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>143</td>\n",
              "      <td>143</td>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>144</td>\n",
              "      <td>144</td>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>145</td>\n",
              "      <td>145</td>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 589 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  Unnamed: 0.1            ID  ... TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0             0             0  TCGA-02-0006  ...    0.052741    2.00000        GBM\n",
              "1             1             1  TCGA-02-0009  ...    0.094336   91.47360        GBM\n",
              "2             2             2  TCGA-02-0011  ...    0.096035  272.42900        GBM\n",
              "3             3             3  TCGA-02-0027  ...    0.096470  128.46800        GBM\n",
              "4             4             4  TCGA-02-0033  ...    0.096894  240.77800        GBM\n",
              "..          ...           ...           ...  ...         ...        ...        ...\n",
              "141         141           141  TCGA-HT-7694  ...    0.090456  719.23800        LGG\n",
              "142         142           142  TCGA-HT-8018  ...    0.054307    2.00000        LGG\n",
              "143         143           143  TCGA-HT-8111  ...    0.126712    7.06744        LGG\n",
              "144         144           144  TCGA-HT-8114  ...    0.107433   15.52240        LGG\n",
              "145         145           145  TCGA-HT-8563  ...    0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 589 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "5b27e25c-4aff-437a-e532-7d5deadd18b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Unnamed: 0.1', 'ID', 'Date', 'VOLUME_ET', 'VOLUME_NET',\n",
              "       'VOLUME_ED', 'VOLUME_TC', 'VOLUME_WT', 'VOLUME_BRAIN',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=589)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'Unnamed: 0', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BoXqyfbpydt",
        "colab_type": "text"
      },
      "source": [
        "#NO K-FOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=0.2, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76RgJTpcGf3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faa93114-dcca-4a25-dbc9-62a384bbe9cd"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 585)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6iyOqcBq0RC"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "0682d532-2307-4eb3-83c8-da480ee79e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = MinMaxScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(30,), kernel_regularizer=regularizers.l2(l=0.005)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fad0efc5-4d8f-4a92-d66e-0f6f18dfca9a",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a3d1fda9-6878-4a58-8205-0300d6ddb0e9",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=92)\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.8130 - accuracy: 0.7174 - val_loss: 0.7775 - val_accuracy: 0.7500\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.8075 - accuracy: 0.7174 - val_loss: 0.7732 - val_accuracy: 0.7500\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.8022 - accuracy: 0.7174 - val_loss: 0.7689 - val_accuracy: 0.7500\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.7968 - accuracy: 0.7174 - val_loss: 0.7647 - val_accuracy: 0.7500\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.7915 - accuracy: 0.7228 - val_loss: 0.7605 - val_accuracy: 0.7500\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7863 - accuracy: 0.7337 - val_loss: 0.7563 - val_accuracy: 0.7500\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.7810 - accuracy: 0.7391 - val_loss: 0.7521 - val_accuracy: 0.7917\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7759 - accuracy: 0.7500 - val_loss: 0.7480 - val_accuracy: 0.7917\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.7707 - accuracy: 0.7554 - val_loss: 0.7440 - val_accuracy: 0.7917\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.7657 - accuracy: 0.7554 - val_loss: 0.7399 - val_accuracy: 0.7917\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7606 - accuracy: 0.7554 - val_loss: 0.7359 - val_accuracy: 0.7917\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.7556 - accuracy: 0.7554 - val_loss: 0.7319 - val_accuracy: 0.7917\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.7506 - accuracy: 0.7554 - val_loss: 0.7278 - val_accuracy: 0.7917\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7456 - accuracy: 0.7554 - val_loss: 0.7239 - val_accuracy: 0.7917\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.7407 - accuracy: 0.7554 - val_loss: 0.7199 - val_accuracy: 0.7917\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.7358 - accuracy: 0.7554 - val_loss: 0.7160 - val_accuracy: 0.7917\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.7310 - accuracy: 0.7554 - val_loss: 0.7120 - val_accuracy: 0.7917\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.7262 - accuracy: 0.7554 - val_loss: 0.7081 - val_accuracy: 0.7917\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.7215 - accuracy: 0.7609 - val_loss: 0.7042 - val_accuracy: 0.7917\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.7168 - accuracy: 0.7663 - val_loss: 0.7004 - val_accuracy: 0.7917\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.7121 - accuracy: 0.7717 - val_loss: 0.6966 - val_accuracy: 0.7917\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.7074 - accuracy: 0.7717 - val_loss: 0.6927 - val_accuracy: 0.7917\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.7028 - accuracy: 0.7772 - val_loss: 0.6890 - val_accuracy: 0.7917\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6981 - accuracy: 0.7772 - val_loss: 0.6852 - val_accuracy: 0.7917\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6935 - accuracy: 0.7717 - val_loss: 0.6815 - val_accuracy: 0.7917\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6889 - accuracy: 0.7717 - val_loss: 0.6778 - val_accuracy: 0.7917\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6844 - accuracy: 0.7717 - val_loss: 0.6742 - val_accuracy: 0.7917\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6799 - accuracy: 0.7717 - val_loss: 0.6705 - val_accuracy: 0.7917\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.6755 - accuracy: 0.7717 - val_loss: 0.6669 - val_accuracy: 0.8125\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.6711 - accuracy: 0.7717 - val_loss: 0.6633 - val_accuracy: 0.8125\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6668 - accuracy: 0.7717 - val_loss: 0.6598 - val_accuracy: 0.8125\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.6624 - accuracy: 0.7717 - val_loss: 0.6563 - val_accuracy: 0.8125\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.6581 - accuracy: 0.7772 - val_loss: 0.6527 - val_accuracy: 0.8125\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6538 - accuracy: 0.7826 - val_loss: 0.6492 - val_accuracy: 0.7917\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6495 - accuracy: 0.7935 - val_loss: 0.6457 - val_accuracy: 0.7917\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.6452 - accuracy: 0.7989 - val_loss: 0.6422 - val_accuracy: 0.7917\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.6410 - accuracy: 0.7989 - val_loss: 0.6387 - val_accuracy: 0.7917\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.6367 - accuracy: 0.8043 - val_loss: 0.6352 - val_accuracy: 0.8125\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6325 - accuracy: 0.8098 - val_loss: 0.6317 - val_accuracy: 0.8125\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.6283 - accuracy: 0.8098 - val_loss: 0.6282 - val_accuracy: 0.8125\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6242 - accuracy: 0.8098 - val_loss: 0.6247 - val_accuracy: 0.8125\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.6200 - accuracy: 0.8098 - val_loss: 0.6213 - val_accuracy: 0.8333\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.6159 - accuracy: 0.8098 - val_loss: 0.6178 - val_accuracy: 0.8542\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6117 - accuracy: 0.8152 - val_loss: 0.6144 - val_accuracy: 0.8542\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.6076 - accuracy: 0.8152 - val_loss: 0.6109 - val_accuracy: 0.8542\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.6036 - accuracy: 0.8261 - val_loss: 0.6075 - val_accuracy: 0.8542\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5995 - accuracy: 0.8261 - val_loss: 0.6041 - val_accuracy: 0.8542\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5955 - accuracy: 0.8261 - val_loss: 0.6008 - val_accuracy: 0.8542\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5915 - accuracy: 0.8261 - val_loss: 0.5974 - val_accuracy: 0.8542\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5875 - accuracy: 0.8261 - val_loss: 0.5940 - val_accuracy: 0.8542\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.5836 - accuracy: 0.8261 - val_loss: 0.5906 - val_accuracy: 0.8542\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.5796 - accuracy: 0.8261 - val_loss: 0.5873 - val_accuracy: 0.8542\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5756 - accuracy: 0.8261 - val_loss: 0.5840 - val_accuracy: 0.8542\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.5717 - accuracy: 0.8261 - val_loss: 0.5806 - val_accuracy: 0.8542\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5677 - accuracy: 0.8261 - val_loss: 0.5773 - val_accuracy: 0.8542\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.5638 - accuracy: 0.8261 - val_loss: 0.5740 - val_accuracy: 0.8542\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.5599 - accuracy: 0.8315 - val_loss: 0.5708 - val_accuracy: 0.8542\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5560 - accuracy: 0.8315 - val_loss: 0.5675 - val_accuracy: 0.8542\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.5522 - accuracy: 0.8424 - val_loss: 0.5643 - val_accuracy: 0.8542\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.5483 - accuracy: 0.8478 - val_loss: 0.5610 - val_accuracy: 0.8542\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.5445 - accuracy: 0.8533 - val_loss: 0.5578 - val_accuracy: 0.8542\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.5406 - accuracy: 0.8533 - val_loss: 0.5546 - val_accuracy: 0.8542\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.5368 - accuracy: 0.8533 - val_loss: 0.5514 - val_accuracy: 0.8542\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5331 - accuracy: 0.8533 - val_loss: 0.5483 - val_accuracy: 0.8542\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5293 - accuracy: 0.8533 - val_loss: 0.5451 - val_accuracy: 0.8542\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.5255 - accuracy: 0.8587 - val_loss: 0.5419 - val_accuracy: 0.8542\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.5218 - accuracy: 0.8587 - val_loss: 0.5388 - val_accuracy: 0.8542\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.5181 - accuracy: 0.8587 - val_loss: 0.5357 - val_accuracy: 0.8542\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5144 - accuracy: 0.8587 - val_loss: 0.5326 - val_accuracy: 0.8542\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.5107 - accuracy: 0.8641 - val_loss: 0.5295 - val_accuracy: 0.8542\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.5070 - accuracy: 0.8641 - val_loss: 0.5264 - val_accuracy: 0.8542\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 83us/step - loss: 0.5033 - accuracy: 0.8750 - val_loss: 0.5234 - val_accuracy: 0.8542\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.4997 - accuracy: 0.8750 - val_loss: 0.5203 - val_accuracy: 0.8542\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4961 - accuracy: 0.8804 - val_loss: 0.5173 - val_accuracy: 0.8542\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4925 - accuracy: 0.8859 - val_loss: 0.5143 - val_accuracy: 0.8750\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.4890 - accuracy: 0.8859 - val_loss: 0.5114 - val_accuracy: 0.8750\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.4855 - accuracy: 0.8859 - val_loss: 0.5084 - val_accuracy: 0.8750\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.4819 - accuracy: 0.8859 - val_loss: 0.5055 - val_accuracy: 0.8750\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.4785 - accuracy: 0.8913 - val_loss: 0.5026 - val_accuracy: 0.8750\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.4750 - accuracy: 0.8913 - val_loss: 0.4997 - val_accuracy: 0.8750\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 136us/step - loss: 0.4716 - accuracy: 0.8967 - val_loss: 0.4968 - val_accuracy: 0.8750\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4682 - accuracy: 0.8967 - val_loss: 0.4940 - val_accuracy: 0.8750\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.4648 - accuracy: 0.8967 - val_loss: 0.4912 - val_accuracy: 0.8750\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.4615 - accuracy: 0.8967 - val_loss: 0.4884 - val_accuracy: 0.8750\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.4581 - accuracy: 0.8967 - val_loss: 0.4857 - val_accuracy: 0.8750\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.4548 - accuracy: 0.9076 - val_loss: 0.4829 - val_accuracy: 0.8750\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.4515 - accuracy: 0.9076 - val_loss: 0.4802 - val_accuracy: 0.8750\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4483 - accuracy: 0.9076 - val_loss: 0.4775 - val_accuracy: 0.8750\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.4451 - accuracy: 0.9130 - val_loss: 0.4749 - val_accuracy: 0.8750\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.4419 - accuracy: 0.9130 - val_loss: 0.4723 - val_accuracy: 0.8750\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.4387 - accuracy: 0.9130 - val_loss: 0.4697 - val_accuracy: 0.8750\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.4355 - accuracy: 0.9130 - val_loss: 0.4671 - val_accuracy: 0.8750\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.4323 - accuracy: 0.9185 - val_loss: 0.4646 - val_accuracy: 0.8750\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4292 - accuracy: 0.9185 - val_loss: 0.4620 - val_accuracy: 0.8750\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.4261 - accuracy: 0.9185 - val_loss: 0.4595 - val_accuracy: 0.8750\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.4231 - accuracy: 0.9185 - val_loss: 0.4570 - val_accuracy: 0.8750\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.4200 - accuracy: 0.9185 - val_loss: 0.4546 - val_accuracy: 0.8750\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4170 - accuracy: 0.9185 - val_loss: 0.4522 - val_accuracy: 0.8750\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.4141 - accuracy: 0.9185 - val_loss: 0.4498 - val_accuracy: 0.8750\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.4111 - accuracy: 0.9185 - val_loss: 0.4474 - val_accuracy: 0.8750\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.4082 - accuracy: 0.9348 - val_loss: 0.4450 - val_accuracy: 0.8750\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4052 - accuracy: 0.9348 - val_loss: 0.4427 - val_accuracy: 0.8750\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4023 - accuracy: 0.9348 - val_loss: 0.4405 - val_accuracy: 0.8750\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3995 - accuracy: 0.9348 - val_loss: 0.4382 - val_accuracy: 0.8750\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3966 - accuracy: 0.9348 - val_loss: 0.4360 - val_accuracy: 0.8750\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3938 - accuracy: 0.9348 - val_loss: 0.4337 - val_accuracy: 0.8750\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.3909 - accuracy: 0.9348 - val_loss: 0.4316 - val_accuracy: 0.8750\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3881 - accuracy: 0.9348 - val_loss: 0.4294 - val_accuracy: 0.8750\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3854 - accuracy: 0.9348 - val_loss: 0.4273 - val_accuracy: 0.8750\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3826 - accuracy: 0.9348 - val_loss: 0.4252 - val_accuracy: 0.8750\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3798 - accuracy: 0.9348 - val_loss: 0.4231 - val_accuracy: 0.8958\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3771 - accuracy: 0.9348 - val_loss: 0.4210 - val_accuracy: 0.8958\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3744 - accuracy: 0.9348 - val_loss: 0.4190 - val_accuracy: 0.8958\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3717 - accuracy: 0.9402 - val_loss: 0.4169 - val_accuracy: 0.8958\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3690 - accuracy: 0.9402 - val_loss: 0.4149 - val_accuracy: 0.9167\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3664 - accuracy: 0.9457 - val_loss: 0.4130 - val_accuracy: 0.9167\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3638 - accuracy: 0.9457 - val_loss: 0.4110 - val_accuracy: 0.9167\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3612 - accuracy: 0.9511 - val_loss: 0.4091 - val_accuracy: 0.9167\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3586 - accuracy: 0.9565 - val_loss: 0.4072 - val_accuracy: 0.9167\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3560 - accuracy: 0.9565 - val_loss: 0.4053 - val_accuracy: 0.9167\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3535 - accuracy: 0.9565 - val_loss: 0.4034 - val_accuracy: 0.9167\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3510 - accuracy: 0.9565 - val_loss: 0.4016 - val_accuracy: 0.9167\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3485 - accuracy: 0.9565 - val_loss: 0.3998 - val_accuracy: 0.9167\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3460 - accuracy: 0.9565 - val_loss: 0.3980 - val_accuracy: 0.9167\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3435 - accuracy: 0.9565 - val_loss: 0.3963 - val_accuracy: 0.9167\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.3411 - accuracy: 0.9565 - val_loss: 0.3945 - val_accuracy: 0.9167\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.3387 - accuracy: 0.9565 - val_loss: 0.3928 - val_accuracy: 0.9167\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.3364 - accuracy: 0.9565 - val_loss: 0.3911 - val_accuracy: 0.9375\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3340 - accuracy: 0.9565 - val_loss: 0.3895 - val_accuracy: 0.9375\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3316 - accuracy: 0.9620 - val_loss: 0.3878 - val_accuracy: 0.9375\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3293 - accuracy: 0.9620 - val_loss: 0.3862 - val_accuracy: 0.9375\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3271 - accuracy: 0.9620 - val_loss: 0.3846 - val_accuracy: 0.9375\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3248 - accuracy: 0.9620 - val_loss: 0.3830 - val_accuracy: 0.9375\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3225 - accuracy: 0.9620 - val_loss: 0.3815 - val_accuracy: 0.9375\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.3203 - accuracy: 0.9620 - val_loss: 0.3799 - val_accuracy: 0.9375\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.3181 - accuracy: 0.9620 - val_loss: 0.3784 - val_accuracy: 0.9375\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.3159 - accuracy: 0.9620 - val_loss: 0.3769 - val_accuracy: 0.9375\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3137 - accuracy: 0.9620 - val_loss: 0.3754 - val_accuracy: 0.9375\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.3116 - accuracy: 0.9620 - val_loss: 0.3738 - val_accuracy: 0.9375\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.3094 - accuracy: 0.9674 - val_loss: 0.3724 - val_accuracy: 0.9583\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.3073 - accuracy: 0.9674 - val_loss: 0.3709 - val_accuracy: 0.9583\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3052 - accuracy: 0.9674 - val_loss: 0.3695 - val_accuracy: 0.9583\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3032 - accuracy: 0.9674 - val_loss: 0.3680 - val_accuracy: 0.9583\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3011 - accuracy: 0.9674 - val_loss: 0.3666 - val_accuracy: 0.9583\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2991 - accuracy: 0.9674 - val_loss: 0.3652 - val_accuracy: 0.9583\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2971 - accuracy: 0.9674 - val_loss: 0.3638 - val_accuracy: 0.9583\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2951 - accuracy: 0.9674 - val_loss: 0.3625 - val_accuracy: 0.9583\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2931 - accuracy: 0.9674 - val_loss: 0.3612 - val_accuracy: 0.9583\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2912 - accuracy: 0.9674 - val_loss: 0.3599 - val_accuracy: 0.9583\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2893 - accuracy: 0.9674 - val_loss: 0.3586 - val_accuracy: 0.9583\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2874 - accuracy: 0.9674 - val_loss: 0.3573 - val_accuracy: 0.9583\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2855 - accuracy: 0.9674 - val_loss: 0.3561 - val_accuracy: 0.9583\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2836 - accuracy: 0.9674 - val_loss: 0.3549 - val_accuracy: 0.9583\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2818 - accuracy: 0.9674 - val_loss: 0.3537 - val_accuracy: 0.9583\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2799 - accuracy: 0.9674 - val_loss: 0.3525 - val_accuracy: 0.9583\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2781 - accuracy: 0.9674 - val_loss: 0.3513 - val_accuracy: 0.9583\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2763 - accuracy: 0.9674 - val_loss: 0.3501 - val_accuracy: 0.9583\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2745 - accuracy: 0.9674 - val_loss: 0.3490 - val_accuracy: 0.9583\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2728 - accuracy: 0.9674 - val_loss: 0.3479 - val_accuracy: 0.9583\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2710 - accuracy: 0.9674 - val_loss: 0.3467 - val_accuracy: 0.9583\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2693 - accuracy: 0.9728 - val_loss: 0.3456 - val_accuracy: 0.9583\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2675 - accuracy: 0.9783 - val_loss: 0.3445 - val_accuracy: 0.9583\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2658 - accuracy: 0.9783 - val_loss: 0.3434 - val_accuracy: 0.9583\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2642 - accuracy: 0.9783 - val_loss: 0.3423 - val_accuracy: 0.9583\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2625 - accuracy: 0.9783 - val_loss: 0.3413 - val_accuracy: 0.9583\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2608 - accuracy: 0.9783 - val_loss: 0.3402 - val_accuracy: 0.9583\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2592 - accuracy: 0.9783 - val_loss: 0.3392 - val_accuracy: 0.9583\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2576 - accuracy: 0.9783 - val_loss: 0.3382 - val_accuracy: 0.9583\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2560 - accuracy: 0.9783 - val_loss: 0.3372 - val_accuracy: 0.9583\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2544 - accuracy: 0.9783 - val_loss: 0.3363 - val_accuracy: 0.9583\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2528 - accuracy: 0.9783 - val_loss: 0.3353 - val_accuracy: 0.9583\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2513 - accuracy: 0.9783 - val_loss: 0.3343 - val_accuracy: 0.9583\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2498 - accuracy: 0.9783 - val_loss: 0.3333 - val_accuracy: 0.9583\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2482 - accuracy: 0.9783 - val_loss: 0.3324 - val_accuracy: 0.9583\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2467 - accuracy: 0.9783 - val_loss: 0.3315 - val_accuracy: 0.9583\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2453 - accuracy: 0.9783 - val_loss: 0.3306 - val_accuracy: 0.9583\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2438 - accuracy: 0.9783 - val_loss: 0.3297 - val_accuracy: 0.9583\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2423 - accuracy: 0.9783 - val_loss: 0.3288 - val_accuracy: 0.9583\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2409 - accuracy: 0.9783 - val_loss: 0.3279 - val_accuracy: 0.9583\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2395 - accuracy: 0.9783 - val_loss: 0.3271 - val_accuracy: 0.9583\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2381 - accuracy: 0.9783 - val_loss: 0.3262 - val_accuracy: 0.9583\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2367 - accuracy: 0.9783 - val_loss: 0.3254 - val_accuracy: 0.9583\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2353 - accuracy: 0.9783 - val_loss: 0.3246 - val_accuracy: 0.9583\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2340 - accuracy: 0.9783 - val_loss: 0.3239 - val_accuracy: 0.9583\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2326 - accuracy: 0.9783 - val_loss: 0.3231 - val_accuracy: 0.9583\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2313 - accuracy: 0.9783 - val_loss: 0.3224 - val_accuracy: 0.9583\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2300 - accuracy: 0.9783 - val_loss: 0.3216 - val_accuracy: 0.9583\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2287 - accuracy: 0.9783 - val_loss: 0.3209 - val_accuracy: 0.9583\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2274 - accuracy: 0.9783 - val_loss: 0.3202 - val_accuracy: 0.9583\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2261 - accuracy: 0.9783 - val_loss: 0.3195 - val_accuracy: 0.9583\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2248 - accuracy: 0.9891 - val_loss: 0.3189 - val_accuracy: 0.9583\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.2236 - accuracy: 0.9891 - val_loss: 0.3182 - val_accuracy: 0.9583\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.2223 - accuracy: 0.9891 - val_loss: 0.3175 - val_accuracy: 0.9583\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2211 - accuracy: 0.9891 - val_loss: 0.3167 - val_accuracy: 0.9583\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2199 - accuracy: 0.9891 - val_loss: 0.3160 - val_accuracy: 0.9583\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2187 - accuracy: 0.9891 - val_loss: 0.3152 - val_accuracy: 0.9583\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.2175 - accuracy: 0.9891 - val_loss: 0.3145 - val_accuracy: 0.9583\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2163 - accuracy: 0.9891 - val_loss: 0.3137 - val_accuracy: 0.9583\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2151 - accuracy: 0.9891 - val_loss: 0.3130 - val_accuracy: 0.9583\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2140 - accuracy: 0.9891 - val_loss: 0.3122 - val_accuracy: 0.9583\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2128 - accuracy: 0.9891 - val_loss: 0.3115 - val_accuracy: 0.9583\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2117 - accuracy: 0.9891 - val_loss: 0.3107 - val_accuracy: 0.9583\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2105 - accuracy: 0.9891 - val_loss: 0.3100 - val_accuracy: 0.9583\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2094 - accuracy: 0.9891 - val_loss: 0.3092 - val_accuracy: 0.9583\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2083 - accuracy: 0.9891 - val_loss: 0.3085 - val_accuracy: 0.9583\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2072 - accuracy: 0.9891 - val_loss: 0.3078 - val_accuracy: 0.9583\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.2061 - accuracy: 0.9891 - val_loss: 0.3071 - val_accuracy: 0.9583\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2050 - accuracy: 0.9891 - val_loss: 0.3064 - val_accuracy: 0.9583\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2039 - accuracy: 0.9891 - val_loss: 0.3057 - val_accuracy: 0.9583\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2028 - accuracy: 0.9891 - val_loss: 0.3050 - val_accuracy: 0.9583\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2017 - accuracy: 0.9891 - val_loss: 0.3043 - val_accuracy: 0.9583\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2007 - accuracy: 0.9891 - val_loss: 0.3037 - val_accuracy: 0.9583\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1997 - accuracy: 0.9891 - val_loss: 0.3030 - val_accuracy: 0.9583\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1986 - accuracy: 0.9891 - val_loss: 0.3024 - val_accuracy: 0.9583\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1976 - accuracy: 0.9891 - val_loss: 0.3018 - val_accuracy: 0.9583\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1966 - accuracy: 0.9891 - val_loss: 0.3011 - val_accuracy: 0.9583\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1956 - accuracy: 0.9891 - val_loss: 0.3005 - val_accuracy: 0.9583\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1947 - accuracy: 0.9891 - val_loss: 0.2999 - val_accuracy: 0.9583\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1937 - accuracy: 0.9891 - val_loss: 0.2993 - val_accuracy: 0.9583\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1927 - accuracy: 0.9891 - val_loss: 0.2987 - val_accuracy: 0.9583\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1918 - accuracy: 0.9891 - val_loss: 0.2981 - val_accuracy: 0.9583\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1909 - accuracy: 0.9891 - val_loss: 0.2975 - val_accuracy: 0.9583\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1899 - accuracy: 0.9891 - val_loss: 0.2969 - val_accuracy: 0.9583\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.1890 - accuracy: 0.9946 - val_loss: 0.2963 - val_accuracy: 0.9583\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1881 - accuracy: 0.9946 - val_loss: 0.2956 - val_accuracy: 0.9583\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1872 - accuracy: 0.9946 - val_loss: 0.2950 - val_accuracy: 0.9583\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1863 - accuracy: 0.9946 - val_loss: 0.2944 - val_accuracy: 0.9583\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1854 - accuracy: 0.9946 - val_loss: 0.2937 - val_accuracy: 0.9583\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1845 - accuracy: 0.9946 - val_loss: 0.2931 - val_accuracy: 0.9583\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1837 - accuracy: 0.9946 - val_loss: 0.2925 - val_accuracy: 0.9583\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1828 - accuracy: 0.9946 - val_loss: 0.2919 - val_accuracy: 0.9583\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1820 - accuracy: 0.9946 - val_loss: 0.2913 - val_accuracy: 0.9583\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1811 - accuracy: 0.9946 - val_loss: 0.2907 - val_accuracy: 0.9583\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1803 - accuracy: 0.9946 - val_loss: 0.2901 - val_accuracy: 0.9583\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1794 - accuracy: 0.9946 - val_loss: 0.2895 - val_accuracy: 0.9583\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1786 - accuracy: 0.9946 - val_loss: 0.2890 - val_accuracy: 0.9583\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1778 - accuracy: 0.9946 - val_loss: 0.2884 - val_accuracy: 0.9583\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1770 - accuracy: 0.9946 - val_loss: 0.2878 - val_accuracy: 0.9583\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1762 - accuracy: 0.9946 - val_loss: 0.2873 - val_accuracy: 0.9583\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1754 - accuracy: 0.9946 - val_loss: 0.2867 - val_accuracy: 0.9583\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1746 - accuracy: 0.9946 - val_loss: 0.2862 - val_accuracy: 0.9583\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1738 - accuracy: 0.9946 - val_loss: 0.2856 - val_accuracy: 0.9583\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1731 - accuracy: 0.9946 - val_loss: 0.2851 - val_accuracy: 0.9583\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1723 - accuracy: 0.9946 - val_loss: 0.2845 - val_accuracy: 0.9583\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1715 - accuracy: 0.9946 - val_loss: 0.2839 - val_accuracy: 0.9583\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1708 - accuracy: 0.9946 - val_loss: 0.2834 - val_accuracy: 0.9583\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1700 - accuracy: 0.9946 - val_loss: 0.2828 - val_accuracy: 0.9583\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1693 - accuracy: 0.9946 - val_loss: 0.2822 - val_accuracy: 0.9583\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1686 - accuracy: 0.9946 - val_loss: 0.2817 - val_accuracy: 0.9583\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1679 - accuracy: 0.9946 - val_loss: 0.2811 - val_accuracy: 0.9583\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1672 - accuracy: 0.9946 - val_loss: 0.2806 - val_accuracy: 0.9583\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1665 - accuracy: 0.9946 - val_loss: 0.2801 - val_accuracy: 0.9583\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1658 - accuracy: 0.9946 - val_loss: 0.2796 - val_accuracy: 0.9583\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1651 - accuracy: 0.9946 - val_loss: 0.2791 - val_accuracy: 0.9583\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1644 - accuracy: 0.9946 - val_loss: 0.2786 - val_accuracy: 0.9583\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1637 - accuracy: 0.9946 - val_loss: 0.2781 - val_accuracy: 0.9583\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1630 - accuracy: 0.9946 - val_loss: 0.2776 - val_accuracy: 0.9583\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1624 - accuracy: 0.9946 - val_loss: 0.2770 - val_accuracy: 0.9583\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1617 - accuracy: 0.9946 - val_loss: 0.2766 - val_accuracy: 0.9583\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1611 - accuracy: 0.9946 - val_loss: 0.2761 - val_accuracy: 0.9583\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1604 - accuracy: 0.9946 - val_loss: 0.2756 - val_accuracy: 0.9583\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1598 - accuracy: 0.9946 - val_loss: 0.2751 - val_accuracy: 0.9583\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1591 - accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.9583\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1585 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.9583\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1579 - accuracy: 1.0000 - val_loss: 0.2736 - val_accuracy: 0.9583\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1573 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9583\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.1566 - accuracy: 1.0000 - val_loss: 0.2725 - val_accuracy: 0.9583\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1560 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9583\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1554 - accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.9583\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1548 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9583\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1542 - accuracy: 1.0000 - val_loss: 0.2705 - val_accuracy: 0.9583\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1536 - accuracy: 1.0000 - val_loss: 0.2700 - val_accuracy: 0.9583\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1531 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 0.9583\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1525 - accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.9583\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1519 - accuracy: 1.0000 - val_loss: 0.2685 - val_accuracy: 0.9583\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1513 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9583\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1507 - accuracy: 1.0000 - val_loss: 0.2676 - val_accuracy: 0.9583\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1502 - accuracy: 1.0000 - val_loss: 0.2671 - val_accuracy: 0.9583\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1496 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 0.9583\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1491 - accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 0.9583\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1485 - accuracy: 1.0000 - val_loss: 0.2657 - val_accuracy: 0.9583\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1479 - accuracy: 1.0000 - val_loss: 0.2652 - val_accuracy: 0.9583\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1474 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9583\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1469 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.9583\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1463 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 0.9583\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1458 - accuracy: 1.0000 - val_loss: 0.2633 - val_accuracy: 0.9583\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1453 - accuracy: 1.0000 - val_loss: 0.2629 - val_accuracy: 0.9583\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1448 - accuracy: 1.0000 - val_loss: 0.2625 - val_accuracy: 0.9583\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1442 - accuracy: 1.0000 - val_loss: 0.2620 - val_accuracy: 0.9583\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1437 - accuracy: 1.0000 - val_loss: 0.2616 - val_accuracy: 0.9583\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1432 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9583\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1427 - accuracy: 1.0000 - val_loss: 0.2607 - val_accuracy: 0.9583\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1422 - accuracy: 1.0000 - val_loss: 0.2603 - val_accuracy: 0.9583\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1417 - accuracy: 1.0000 - val_loss: 0.2598 - val_accuracy: 0.9583\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1412 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9583\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1407 - accuracy: 1.0000 - val_loss: 0.2589 - val_accuracy: 0.9583\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1402 - accuracy: 1.0000 - val_loss: 0.2585 - val_accuracy: 0.9583\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1398 - accuracy: 1.0000 - val_loss: 0.2581 - val_accuracy: 0.9583\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1393 - accuracy: 1.0000 - val_loss: 0.2577 - val_accuracy: 0.9583\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1388 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 0.9583\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1384 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9583\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1379 - accuracy: 1.0000 - val_loss: 0.2566 - val_accuracy: 0.9583\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.2562 - val_accuracy: 0.9583\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1370 - accuracy: 1.0000 - val_loss: 0.2558 - val_accuracy: 0.9583\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1365 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9583\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1361 - accuracy: 1.0000 - val_loss: 0.2552 - val_accuracy: 0.9583\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1356 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9583\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1352 - accuracy: 1.0000 - val_loss: 0.2546 - val_accuracy: 0.9583\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1347 - accuracy: 1.0000 - val_loss: 0.2543 - val_accuracy: 0.9583\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1343 - accuracy: 1.0000 - val_loss: 0.2540 - val_accuracy: 0.9583\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1338 - accuracy: 1.0000 - val_loss: 0.2537 - val_accuracy: 0.9583\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1334 - accuracy: 1.0000 - val_loss: 0.2534 - val_accuracy: 0.9583\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1330 - accuracy: 1.0000 - val_loss: 0.2531 - val_accuracy: 0.9583\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1326 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.9583\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1321 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9583\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 0.9583\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1313 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9583\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1309 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9583\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1305 - accuracy: 1.0000 - val_loss: 0.2517 - val_accuracy: 0.9583\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1301 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9583\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1297 - accuracy: 1.0000 - val_loss: 0.2513 - val_accuracy: 0.9583\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 0.2511 - val_accuracy: 0.9583\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1289 - accuracy: 1.0000 - val_loss: 0.2510 - val_accuracy: 0.9583\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1285 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9583\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1281 - accuracy: 1.0000 - val_loss: 0.2506 - val_accuracy: 0.9583\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1277 - accuracy: 1.0000 - val_loss: 0.2505 - val_accuracy: 0.9583\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1273 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9583\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1269 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9583\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1266 - accuracy: 1.0000 - val_loss: 0.2500 - val_accuracy: 0.9583\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1262 - accuracy: 1.0000 - val_loss: 0.2499 - val_accuracy: 0.9583\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1258 - accuracy: 1.0000 - val_loss: 0.2497 - val_accuracy: 0.9583\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1254 - accuracy: 1.0000 - val_loss: 0.2495 - val_accuracy: 0.9583\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1251 - accuracy: 1.0000 - val_loss: 0.2494 - val_accuracy: 0.9583\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1247 - accuracy: 1.0000 - val_loss: 0.2492 - val_accuracy: 0.9583\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1243 - accuracy: 1.0000 - val_loss: 0.2490 - val_accuracy: 0.9583\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1240 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9583\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1236 - accuracy: 1.0000 - val_loss: 0.2487 - val_accuracy: 0.9583\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1233 - accuracy: 1.0000 - val_loss: 0.2486 - val_accuracy: 0.9583\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1229 - accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 0.9583\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1226 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9583\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1222 - accuracy: 1.0000 - val_loss: 0.2482 - val_accuracy: 0.9583\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1219 - accuracy: 1.0000 - val_loss: 0.2481 - val_accuracy: 0.9583\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1215 - accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.9583\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1212 - accuracy: 1.0000 - val_loss: 0.2478 - val_accuracy: 0.9583\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1209 - accuracy: 1.0000 - val_loss: 0.2476 - val_accuracy: 0.9583\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1206 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9583\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1202 - accuracy: 1.0000 - val_loss: 0.2473 - val_accuracy: 0.9583\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 0.2472 - val_accuracy: 0.9583\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1196 - accuracy: 1.0000 - val_loss: 0.2470 - val_accuracy: 0.9583\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1193 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9583\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1189 - accuracy: 1.0000 - val_loss: 0.2468 - val_accuracy: 0.9583\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1186 - accuracy: 1.0000 - val_loss: 0.2467 - val_accuracy: 0.9583\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1183 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 0.9583\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1180 - accuracy: 1.0000 - val_loss: 0.2464 - val_accuracy: 0.9583\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1177 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9583\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.9583\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1171 - accuracy: 1.0000 - val_loss: 0.2460 - val_accuracy: 0.9583\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1168 - accuracy: 1.0000 - val_loss: 0.2458 - val_accuracy: 0.9583\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9583\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1162 - accuracy: 1.0000 - val_loss: 0.2455 - val_accuracy: 0.9583\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1159 - accuracy: 1.0000 - val_loss: 0.2454 - val_accuracy: 0.9583\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1156 - accuracy: 1.0000 - val_loss: 0.2453 - val_accuracy: 0.9583\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1153 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 0.9583\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1150 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9583\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1148 - accuracy: 1.0000 - val_loss: 0.2450 - val_accuracy: 0.9583\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1145 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9583\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1142 - accuracy: 1.0000 - val_loss: 0.2448 - val_accuracy: 0.9583\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1139 - accuracy: 1.0000 - val_loss: 0.2446 - val_accuracy: 0.9583\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 0.9583\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1134 - accuracy: 1.0000 - val_loss: 0.2444 - val_accuracy: 0.9583\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1131 - accuracy: 1.0000 - val_loss: 0.2443 - val_accuracy: 0.9583\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1128 - accuracy: 1.0000 - val_loss: 0.2442 - val_accuracy: 0.9583\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1126 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9583\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1123 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.9583\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1120 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.9583\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.2437 - val_accuracy: 0.9583\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1115 - accuracy: 1.0000 - val_loss: 0.2436 - val_accuracy: 0.9583\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 102us/step - loss: 0.1113 - accuracy: 1.0000 - val_loss: 0.2435 - val_accuracy: 0.9583\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1110 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9583\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1107 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9583\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1105 - accuracy: 1.0000 - val_loss: 0.2432 - val_accuracy: 0.9583\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1102 - accuracy: 1.0000 - val_loss: 0.2431 - val_accuracy: 0.9583\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.9583\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1097 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9583\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1095 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1092 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1090 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 0.9583\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1087 - accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 0.9583\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1085 - accuracy: 1.0000 - val_loss: 0.2425 - val_accuracy: 0.9583\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 0.9583\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1080 - accuracy: 1.0000 - val_loss: 0.2423 - val_accuracy: 0.9583\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.2422 - val_accuracy: 0.9583\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1075 - accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 0.9583\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1073 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9583\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9583\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1068 - accuracy: 1.0000 - val_loss: 0.2419 - val_accuracy: 0.9583\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1066 - accuracy: 1.0000 - val_loss: 0.2418 - val_accuracy: 0.9583\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1063 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9583\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1061 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9583\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 0.2416 - val_accuracy: 0.9583\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.2416 - val_accuracy: 0.9583\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1054 - accuracy: 1.0000 - val_loss: 0.2415 - val_accuracy: 0.9583\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1051 - accuracy: 1.0000 - val_loss: 0.2415 - val_accuracy: 0.9583\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1049 - accuracy: 1.0000 - val_loss: 0.2414 - val_accuracy: 0.9583\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1047 - accuracy: 1.0000 - val_loss: 0.2414 - val_accuracy: 0.9583\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1045 - accuracy: 1.0000 - val_loss: 0.2413 - val_accuracy: 0.9583\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1042 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9583\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9583\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1038 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9583\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.1036 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9583\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1033 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9583\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1031 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9583\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1029 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9583\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9583\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1025 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 0.9583\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.9583\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1020 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.9583\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1018 - accuracy: 1.0000 - val_loss: 0.2406 - val_accuracy: 0.9583\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1016 - accuracy: 1.0000 - val_loss: 0.2406 - val_accuracy: 0.9583\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1014 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9583\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1012 - accuracy: 1.0000 - val_loss: 0.2404 - val_accuracy: 0.9583\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.2403 - val_accuracy: 0.9583\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1008 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 0.9583\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1006 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9583\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1004 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9583\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1002 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.9583\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 0.2398 - val_accuracy: 0.9583\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0998 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9583\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0996 - accuracy: 1.0000 - val_loss: 0.2396 - val_accuracy: 0.9583\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0994 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9583\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9583\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0990 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9583\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0988 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 0.9583\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 0.9583\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0984 - accuracy: 1.0000 - val_loss: 0.2393 - val_accuracy: 0.9583\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0982 - accuracy: 1.0000 - val_loss: 0.2393 - val_accuracy: 0.9583\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0980 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9583\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0978 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9583\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0976 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0972 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0970 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0968 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0966 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0965 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 0.2390 - val_accuracy: 0.9583\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0961 - accuracy: 1.0000 - val_loss: 0.2390 - val_accuracy: 0.9583\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9583\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0957 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9583\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.2388 - val_accuracy: 0.9583\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0953 - accuracy: 1.0000 - val_loss: 0.2387 - val_accuracy: 0.9583\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0952 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 0.2385 - val_accuracy: 0.9583\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0946 - accuracy: 1.0000 - val_loss: 0.2385 - val_accuracy: 0.9583\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 0.9583\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0943 - accuracy: 1.0000 - val_loss: 0.2383 - val_accuracy: 0.9583\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0941 - accuracy: 1.0000 - val_loss: 0.2383 - val_accuracy: 0.9583\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0939 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9583\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0937 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9583\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0936 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9583\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0934 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9583\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0932 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9583\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0931 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9583\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0929 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9583\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0927 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.9583\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0925 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.9583\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.9583\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.0922 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.9583\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 0.9583\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9583\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0917 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9583\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.2376 - val_accuracy: 0.9583\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0914 - accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9583\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0912 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 0.9583\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 0.2372 - val_accuracy: 0.9583\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0909 - accuracy: 1.0000 - val_loss: 0.2370 - val_accuracy: 0.9583\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0907 - accuracy: 1.0000 - val_loss: 0.2369 - val_accuracy: 0.9583\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0906 - accuracy: 1.0000 - val_loss: 0.2368 - val_accuracy: 0.9583\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 0.2367 - val_accuracy: 0.9583\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 0.9583\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0901 - accuracy: 1.0000 - val_loss: 0.2365 - val_accuracy: 0.9583\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.2364 - val_accuracy: 0.9583\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.0898 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9583\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9583\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0895 - accuracy: 1.0000 - val_loss: 0.2362 - val_accuracy: 0.9583\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9583\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0892 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9583\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0890 - accuracy: 1.0000 - val_loss: 0.2360 - val_accuracy: 0.9583\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.0889 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9583\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0887 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9583\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0886 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9583\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0884 - accuracy: 1.0000 - val_loss: 0.2358 - val_accuracy: 0.9583\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 0.2358 - val_accuracy: 0.9583\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0881 - accuracy: 1.0000 - val_loss: 0.2357 - val_accuracy: 0.9583\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0880 - accuracy: 1.0000 - val_loss: 0.2357 - val_accuracy: 0.9583\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 0.2357 - val_accuracy: 0.9583\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0877 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9583\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0875 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9583\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "61230da6-53bd-4b2d-c61c-8de9f82ff49c",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd72e144860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3b89d083-9d9d-42c3-aed9-0b11c51b7d99",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd72f93af28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b8d8f84-0e7d-4224-fd33-460bbbe0ded9",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=92, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 898us/step - loss: 0.8883 - accuracy: 0.4728\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8830 - accuracy: 0.4783\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8760 - accuracy: 0.4946\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.8690 - accuracy: 0.4946\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.8621 - accuracy: 0.5163\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.8556 - accuracy: 0.5217\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.8488 - accuracy: 0.5272\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.8421 - accuracy: 0.5272\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.8355 - accuracy: 0.5326\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.8290 - accuracy: 0.5435\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.8226 - accuracy: 0.5598\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.8162 - accuracy: 0.5870\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.8099 - accuracy: 0.6033\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.8036 - accuracy: 0.6087\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.7974 - accuracy: 0.6087\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.7912 - accuracy: 0.6141\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.7852 - accuracy: 0.6141\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7792 - accuracy: 0.6250\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.7733 - accuracy: 0.6413\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.7673 - accuracy: 0.6467\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7615 - accuracy: 0.6630\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.7557 - accuracy: 0.6793\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.7500 - accuracy: 0.6848\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.7444 - accuracy: 0.6848\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.7388 - accuracy: 0.6902\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.7332 - accuracy: 0.7011\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.7278 - accuracy: 0.7065\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.7223 - accuracy: 0.7174\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.7169 - accuracy: 0.7228\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.7115 - accuracy: 0.7337\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.7063 - accuracy: 0.7337\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7011 - accuracy: 0.7446\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.6958 - accuracy: 0.7446\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.6907 - accuracy: 0.7609\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6856 - accuracy: 0.7609\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.6805 - accuracy: 0.7772\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.6754 - accuracy: 0.7989\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.6704 - accuracy: 0.8207\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.6655 - accuracy: 0.8261\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.6606 - accuracy: 0.8207\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6557 - accuracy: 0.8261\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.6508 - accuracy: 0.8261\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.6461 - accuracy: 0.8370\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6413 - accuracy: 0.8370\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.6365 - accuracy: 0.8424\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6318 - accuracy: 0.8424\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.6272 - accuracy: 0.8478\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.6226 - accuracy: 0.8533\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 13us/step - loss: 0.6180 - accuracy: 0.8587\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6134 - accuracy: 0.8641\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.6090 - accuracy: 0.8696\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.6045 - accuracy: 0.8696\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.6001 - accuracy: 0.8750\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5957 - accuracy: 0.8750\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.5914 - accuracy: 0.8967\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5871 - accuracy: 0.9022\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.5828 - accuracy: 0.9022\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.5786 - accuracy: 0.9022\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.5744 - accuracy: 0.9076\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.5702 - accuracy: 0.9076\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5661 - accuracy: 0.9076\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.5620 - accuracy: 0.9076\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.5579 - accuracy: 0.9130\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.5538 - accuracy: 0.9185\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5497 - accuracy: 0.9185\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.5457 - accuracy: 0.9293\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.5417 - accuracy: 0.9293\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.5378 - accuracy: 0.9293\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.5338 - accuracy: 0.9293\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.5299 - accuracy: 0.9293\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.5260 - accuracy: 0.9293\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.5221 - accuracy: 0.9293\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.5183 - accuracy: 0.9293\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.5145 - accuracy: 0.9348\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.5107 - accuracy: 0.9348\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.5069 - accuracy: 0.9348\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.5032 - accuracy: 0.9348\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4995 - accuracy: 0.9348\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4959 - accuracy: 0.9348\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.4922 - accuracy: 0.9457\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.4886 - accuracy: 0.9457\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4850 - accuracy: 0.9457\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.4814 - accuracy: 0.9457\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4778 - accuracy: 0.9457\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.4743 - accuracy: 0.9457\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4708 - accuracy: 0.9457\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.4673 - accuracy: 0.9457\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.4638 - accuracy: 0.9457\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4604 - accuracy: 0.9457\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.4569 - accuracy: 0.9511\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4536 - accuracy: 0.9511\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4502 - accuracy: 0.9511\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.4469 - accuracy: 0.9511\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.4436 - accuracy: 0.9511\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.4404 - accuracy: 0.9511\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4371 - accuracy: 0.9457\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.4339 - accuracy: 0.9457\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.4307 - accuracy: 0.9457\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.4275 - accuracy: 0.9457\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4244 - accuracy: 0.9457\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.4213 - accuracy: 0.9457\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.4182 - accuracy: 0.9457\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4151 - accuracy: 0.9457\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4120 - accuracy: 0.9457\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.4090 - accuracy: 0.9457\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.4059 - accuracy: 0.9457\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.4029 - accuracy: 0.9457\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.4000 - accuracy: 0.9457\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3970 - accuracy: 0.9457\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3941 - accuracy: 0.9457\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3912 - accuracy: 0.9511\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3883 - accuracy: 0.9511\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.3855 - accuracy: 0.9511\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3827 - accuracy: 0.9511\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3799 - accuracy: 0.9511\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3771 - accuracy: 0.9511\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3744 - accuracy: 0.9511\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3716 - accuracy: 0.9511\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3690 - accuracy: 0.9511\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3663 - accuracy: 0.9511\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.3636 - accuracy: 0.9511\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3610 - accuracy: 0.9511\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3584 - accuracy: 0.9511\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.3558 - accuracy: 0.9565\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3533 - accuracy: 0.9565\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3507 - accuracy: 0.9565\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.3482 - accuracy: 0.9565\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3457 - accuracy: 0.9565\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3432 - accuracy: 0.9565\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3408 - accuracy: 0.9565\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3383 - accuracy: 0.9565\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3359 - accuracy: 0.9620\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3336 - accuracy: 0.9620\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3312 - accuracy: 0.9620\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.3289 - accuracy: 0.9620\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.3265 - accuracy: 0.9620\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3243 - accuracy: 0.9620\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.3220 - accuracy: 0.9620\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3198 - accuracy: 0.9620\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3176 - accuracy: 0.9620\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3154 - accuracy: 0.9620\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3132 - accuracy: 0.9620\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.3110 - accuracy: 0.9620\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.3089 - accuracy: 0.9620\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.3068 - accuracy: 0.9674\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3047 - accuracy: 0.9674\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3026 - accuracy: 0.9674\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3006 - accuracy: 0.9674\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2985 - accuracy: 0.9674\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2965 - accuracy: 0.9674\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2945 - accuracy: 0.9674\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2926 - accuracy: 0.9674\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2906 - accuracy: 0.9674\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2887 - accuracy: 0.9674\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2868 - accuracy: 0.9674\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2849 - accuracy: 0.9674\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2831 - accuracy: 0.9674\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2812 - accuracy: 0.9674\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2794 - accuracy: 0.9728\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2777 - accuracy: 0.9728\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2759 - accuracy: 0.9728\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2742 - accuracy: 0.9728\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2724 - accuracy: 0.9783\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2707 - accuracy: 0.9783\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2690 - accuracy: 0.9783\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2674 - accuracy: 0.9783\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2657 - accuracy: 0.9783\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2641 - accuracy: 0.9783\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2625 - accuracy: 0.9783\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2609 - accuracy: 0.9783\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2593 - accuracy: 0.9783\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2578 - accuracy: 0.9783\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2562 - accuracy: 0.9783\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2547 - accuracy: 0.9783\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2532 - accuracy: 0.9837\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2517 - accuracy: 0.9837\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2502 - accuracy: 0.9837\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.2487 - accuracy: 0.9837\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2473 - accuracy: 0.9837\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.2459 - accuracy: 0.9837\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2445 - accuracy: 0.9837\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2431 - accuracy: 0.9837\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.2417 - accuracy: 0.9837\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2403 - accuracy: 0.9837\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2390 - accuracy: 0.9837\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2376 - accuracy: 0.9837\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2363 - accuracy: 0.9837\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2350 - accuracy: 0.9837\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2337 - accuracy: 0.9837\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2324 - accuracy: 0.9837\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2311 - accuracy: 0.9837\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.2299 - accuracy: 0.9837\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2286 - accuracy: 0.9837\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2274 - accuracy: 0.9837\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2262 - accuracy: 0.9837\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2250 - accuracy: 0.9837\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2238 - accuracy: 0.9837\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2226 - accuracy: 0.9837\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2215 - accuracy: 0.9837\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2203 - accuracy: 0.9837\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2192 - accuracy: 0.9837\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2181 - accuracy: 0.9837\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2170 - accuracy: 0.9837\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2159 - accuracy: 0.9837\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2148 - accuracy: 0.9837\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2137 - accuracy: 0.9891\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2127 - accuracy: 0.9891\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.2116 - accuracy: 0.9891\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2106 - accuracy: 0.9891\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.2095 - accuracy: 0.9891\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.2085 - accuracy: 0.9891\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2075 - accuracy: 0.9891\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2065 - accuracy: 0.9891\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2055 - accuracy: 0.9891\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.2045 - accuracy: 0.9891\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2035 - accuracy: 0.9891\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2026 - accuracy: 0.9891\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2016 - accuracy: 0.9891\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.2007 - accuracy: 0.9891\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1997 - accuracy: 0.9891\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1988 - accuracy: 0.9891\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1979 - accuracy: 0.9891\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.1969 - accuracy: 0.9891\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1960 - accuracy: 0.9891\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.1951 - accuracy: 0.9891\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1942 - accuracy: 0.9891\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1934 - accuracy: 0.9891\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1925 - accuracy: 0.9891\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1916 - accuracy: 0.9891\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1908 - accuracy: 0.9891\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1899 - accuracy: 0.9891\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1891 - accuracy: 0.9891\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1883 - accuracy: 0.9891\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1874 - accuracy: 0.9891\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1866 - accuracy: 0.9891\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1858 - accuracy: 0.9891\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1850 - accuracy: 0.9891\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1842 - accuracy: 0.9891\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1834 - accuracy: 0.9891\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1826 - accuracy: 0.9891\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1819 - accuracy: 0.9891\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1811 - accuracy: 0.9891\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1803 - accuracy: 0.9891\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1796 - accuracy: 0.9891\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1788 - accuracy: 0.9891\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1781 - accuracy: 0.9946\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1773 - accuracy: 0.9946\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1766 - accuracy: 0.9946\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.1758 - accuracy: 0.9946\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1751 - accuracy: 0.9946\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1744 - accuracy: 0.9946\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1737 - accuracy: 0.9946\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1730 - accuracy: 0.9946\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1723 - accuracy: 0.9946\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1716 - accuracy: 0.9946\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1709 - accuracy: 0.9946\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1703 - accuracy: 0.9946\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1696 - accuracy: 0.9946\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1689 - accuracy: 0.9946\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1683 - accuracy: 0.9946\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1676 - accuracy: 0.9946\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1670 - accuracy: 0.9946\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1664 - accuracy: 0.9946\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1657 - accuracy: 0.9946\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1651 - accuracy: 0.9946\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1645 - accuracy: 0.9946\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1639 - accuracy: 0.9946\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1632 - accuracy: 0.9946\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1626 - accuracy: 0.9946\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1620 - accuracy: 0.9946\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1614 - accuracy: 0.9946\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1609 - accuracy: 0.9946\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1603 - accuracy: 0.9946\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1597 - accuracy: 0.9946\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1591 - accuracy: 0.9946\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.1585 - accuracy: 0.9946\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1579 - accuracy: 0.9946\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1574 - accuracy: 0.9946\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1568 - accuracy: 0.9946\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1563 - accuracy: 0.9946\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1557 - accuracy: 0.9946\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1552 - accuracy: 0.9946\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1546 - accuracy: 0.9946\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1541 - accuracy: 0.9946\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1536 - accuracy: 0.9946\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1530 - accuracy: 0.9946\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1525 - accuracy: 0.9946\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1520 - accuracy: 0.9946\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1515 - accuracy: 0.9946\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1510 - accuracy: 0.9946\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1505 - accuracy: 0.9946\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1500 - accuracy: 0.9946\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1495 - accuracy: 0.9946\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1490 - accuracy: 0.9946\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1485 - accuracy: 0.9946\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1480 - accuracy: 0.9946\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1475 - accuracy: 0.9946\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1470 - accuracy: 0.9946\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1466 - accuracy: 0.9946\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1461 - accuracy: 0.9946\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1456 - accuracy: 0.9946\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1452 - accuracy: 0.9946\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1447 - accuracy: 0.9946\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1443 - accuracy: 0.9946\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1438 - accuracy: 0.9946\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1434 - accuracy: 0.9946\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1429 - accuracy: 0.9946\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1425 - accuracy: 0.9946\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1420 - accuracy: 0.9946\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1416 - accuracy: 0.9946\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1412 - accuracy: 1.0000\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1407 - accuracy: 1.0000\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1403 - accuracy: 1.0000\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1399 - accuracy: 1.0000\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1394 - accuracy: 1.0000\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1390 - accuracy: 1.0000\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1386 - accuracy: 1.0000\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1382 - accuracy: 1.0000\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1378 - accuracy: 1.0000\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1374 - accuracy: 1.0000\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1370 - accuracy: 1.0000\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1366 - accuracy: 1.0000\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1362 - accuracy: 1.0000\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1358 - accuracy: 1.0000\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.1354 - accuracy: 1.0000\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1350 - accuracy: 1.0000\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1346 - accuracy: 1.0000\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1342 - accuracy: 1.0000\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.1338 - accuracy: 1.0000\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.1335 - accuracy: 1.0000\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 13us/step - loss: 0.1331 - accuracy: 1.0000\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.1327 - accuracy: 1.0000\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1323 - accuracy: 1.0000\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1320 - accuracy: 1.0000\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 16us/step - loss: 0.1316 - accuracy: 1.0000\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1312 - accuracy: 1.0000\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1309 - accuracy: 1.0000\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1305 - accuracy: 1.0000\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 15us/step - loss: 0.1301 - accuracy: 1.0000\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1298 - accuracy: 1.0000\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.1294 - accuracy: 1.0000\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 13us/step - loss: 0.1291 - accuracy: 1.0000\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 14us/step - loss: 0.1287 - accuracy: 1.0000\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1284 - accuracy: 1.0000\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1280 - accuracy: 1.0000\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 13us/step - loss: 0.1277 - accuracy: 1.0000\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.1273 - accuracy: 1.0000\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1270 - accuracy: 1.0000\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1266 - accuracy: 1.0000\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1263 - accuracy: 1.0000\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1260 - accuracy: 1.0000\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1256 - accuracy: 1.0000\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1253 - accuracy: 1.0000\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1250 - accuracy: 1.0000\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1246 - accuracy: 1.0000\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1243 - accuracy: 1.0000\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.1240 - accuracy: 1.0000\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1237 - accuracy: 1.0000\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1233 - accuracy: 1.0000\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1230 - accuracy: 1.0000\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1227 - accuracy: 1.0000\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1224 - accuracy: 1.0000\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1221 - accuracy: 1.0000\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1218 - accuracy: 1.0000\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1215 - accuracy: 1.0000\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1211 - accuracy: 1.0000\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1208 - accuracy: 1.0000\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1205 - accuracy: 1.0000\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1202 - accuracy: 1.0000\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1199 - accuracy: 1.0000\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1196 - accuracy: 1.0000\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1193 - accuracy: 1.0000\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1190 - accuracy: 1.0000\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1188 - accuracy: 1.0000\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1185 - accuracy: 1.0000\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1182 - accuracy: 1.0000\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1179 - accuracy: 1.0000\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1176 - accuracy: 1.0000\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1173 - accuracy: 1.0000\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.1170 - accuracy: 1.0000\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1168 - accuracy: 1.0000\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1165 - accuracy: 1.0000\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1162 - accuracy: 1.0000\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1159 - accuracy: 1.0000\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1157 - accuracy: 1.0000\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1154 - accuracy: 1.0000\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1151 - accuracy: 1.0000\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1148 - accuracy: 1.0000\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1146 - accuracy: 1.0000\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1143 - accuracy: 1.0000\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1138 - accuracy: 1.0000\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1135 - accuracy: 1.0000\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1133 - accuracy: 1.0000\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1130 - accuracy: 1.0000\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1128 - accuracy: 1.0000\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1125 - accuracy: 1.0000\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 17us/step - loss: 0.1122 - accuracy: 1.0000\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1120 - accuracy: 1.0000\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1117 - accuracy: 1.0000\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1115 - accuracy: 1.0000\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1112 - accuracy: 1.0000\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.1110 - accuracy: 1.0000\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1107 - accuracy: 1.0000\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1105 - accuracy: 1.0000\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1103 - accuracy: 1.0000\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1100 - accuracy: 1.0000\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1098 - accuracy: 1.0000\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1095 - accuracy: 1.0000\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1093 - accuracy: 1.0000\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1091 - accuracy: 1.0000\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1088 - accuracy: 1.0000\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1086 - accuracy: 1.0000\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1084 - accuracy: 1.0000\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1081 - accuracy: 1.0000\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1079 - accuracy: 1.0000\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1077 - accuracy: 1.0000\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1074 - accuracy: 1.0000\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1072 - accuracy: 1.0000\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1069 - accuracy: 1.0000\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1067 - accuracy: 1.0000\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1065 - accuracy: 1.0000\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1062 - accuracy: 1.0000\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.1060 - accuracy: 1.0000\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1058 - accuracy: 1.0000\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1055 - accuracy: 1.0000\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1053 - accuracy: 1.0000\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.1051 - accuracy: 1.0000\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 21us/step - loss: 0.1049 - accuracy: 1.0000\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 91us/step - loss: 0.1046 - accuracy: 1.0000\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1044 - accuracy: 1.0000\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1042 - accuracy: 1.0000\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1040 - accuracy: 1.0000\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1037 - accuracy: 1.0000\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1035 - accuracy: 1.0000\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1033 - accuracy: 1.0000\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1031 - accuracy: 1.0000\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.1029 - accuracy: 1.0000\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1026 - accuracy: 1.0000\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1024 - accuracy: 1.0000\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1022 - accuracy: 1.0000\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1020 - accuracy: 1.0000\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1018 - accuracy: 1.0000\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.1016 - accuracy: 1.0000\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1014 - accuracy: 1.0000\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1012 - accuracy: 1.0000\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.1009 - accuracy: 1.0000\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.1007 - accuracy: 1.0000\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1005 - accuracy: 1.0000\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1003 - accuracy: 1.0000\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1001 - accuracy: 1.0000\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0999 - accuracy: 1.0000\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.0997 - accuracy: 1.0000\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0995 - accuracy: 1.0000\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0993 - accuracy: 1.0000\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0991 - accuracy: 1.0000\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0989 - accuracy: 1.0000\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0987 - accuracy: 1.0000\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0985 - accuracy: 1.0000\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0983 - accuracy: 1.0000\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0981 - accuracy: 1.0000\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0979 - accuracy: 1.0000\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.0977 - accuracy: 1.0000\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0975 - accuracy: 1.0000\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0973 - accuracy: 1.0000\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0971 - accuracy: 1.0000\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0969 - accuracy: 1.0000\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0967 - accuracy: 1.0000\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0965 - accuracy: 1.0000\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0963 - accuracy: 1.0000\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0962 - accuracy: 1.0000\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0960 - accuracy: 1.0000\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0958 - accuracy: 1.0000\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0956 - accuracy: 1.0000\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0954 - accuracy: 1.0000\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0952 - accuracy: 1.0000\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0951 - accuracy: 1.0000\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0949 - accuracy: 1.0000\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0947 - accuracy: 1.0000\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0945 - accuracy: 1.0000\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0944 - accuracy: 1.0000\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0942 - accuracy: 1.0000\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0940 - accuracy: 1.0000\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0939 - accuracy: 1.0000\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0937 - accuracy: 1.0000\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0935 - accuracy: 1.0000\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0934 - accuracy: 1.0000\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.0932 - accuracy: 1.0000\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0930 - accuracy: 1.0000\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0929 - accuracy: 1.0000\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0927 - accuracy: 1.0000\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0925 - accuracy: 1.0000\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0924 - accuracy: 1.0000\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 20us/step - loss: 0.0922 - accuracy: 1.0000\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.0921 - accuracy: 1.0000\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0919 - accuracy: 1.0000\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0917 - accuracy: 1.0000\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 18us/step - loss: 0.0916 - accuracy: 1.0000\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 19us/step - loss: 0.0914 - accuracy: 1.0000\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0913 - accuracy: 1.0000\n",
            "30/30 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff06160b-e0f7-4317-fe6d-6fda9974d57d",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b745da0b-9987-499d-9da5-9a399fa087f8",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8500000238418579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzVeFpPUFgv8",
        "colab_type": "text"
      },
      "source": [
        "#SENZA PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yjOvqGHhFl1T"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJgxb6a5Fl1U",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdKgkMePFl1e",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eHCd1eHGFl1n",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKb84eAAFl1w",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "469qraA8Fl15",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F93d4NAtFl2C",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QEgnmw_EFl2L"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztWE5G71Fl2O",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJ7QoIiyFl2Z",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mfSvItJFl2i",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lFKekeMCFl2p",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ko6DZPbBFl2y",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2ii7-J9Fl26",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(585,), kernel_regularizer=regularizers.l2(l=0.005)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lBjwz4bMFl3G",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0a268906-fd26-4e60-f841-4ab0c0d88ef2",
        "id": "7ZqWt-mxFl3P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "deccbe4d-1e64-4864-d62e-7653c8c439dc",
        "id": "_CdS5EKbFl3W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, one_hot_train_labels, validation_data=(val_data_stand, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=92)\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 1.0190 - accuracy: 0.4728 - val_loss: 0.9396 - val_accuracy: 0.6250\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.9490 - accuracy: 0.6141 - val_loss: 0.9000 - val_accuracy: 0.7083\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.9090 - accuracy: 0.6957 - val_loss: 0.8896 - val_accuracy: 0.7083\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.8975 - accuracy: 0.6957 - val_loss: 0.8840 - val_accuracy: 0.7083\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.8899 - accuracy: 0.6957 - val_loss: 0.8703 - val_accuracy: 0.7083\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.8744 - accuracy: 0.7011 - val_loss: 0.8489 - val_accuracy: 0.7292\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.8516 - accuracy: 0.7065 - val_loss: 0.8257 - val_accuracy: 0.7292\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.8258 - accuracy: 0.7065 - val_loss: 0.8045 - val_accuracy: 0.7292\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.8013 - accuracy: 0.7065 - val_loss: 0.7841 - val_accuracy: 0.7500\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.7789 - accuracy: 0.7065 - val_loss: 0.7668 - val_accuracy: 0.7500\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.7603 - accuracy: 0.7065 - val_loss: 0.7537 - val_accuracy: 0.7500\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.7451 - accuracy: 0.7228 - val_loss: 0.7424 - val_accuracy: 0.7917\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.7313 - accuracy: 0.7500 - val_loss: 0.7314 - val_accuracy: 0.8125\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.7173 - accuracy: 0.7772 - val_loss: 0.7199 - val_accuracy: 0.8333\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.7028 - accuracy: 0.8043 - val_loss: 0.7082 - val_accuracy: 0.8333\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6880 - accuracy: 0.8152 - val_loss: 0.6964 - val_accuracy: 0.8333\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6734 - accuracy: 0.8370 - val_loss: 0.6852 - val_accuracy: 0.8333\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.6594 - accuracy: 0.8370 - val_loss: 0.6746 - val_accuracy: 0.8333\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.6466 - accuracy: 0.8533 - val_loss: 0.6643 - val_accuracy: 0.8333\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.6346 - accuracy: 0.8533 - val_loss: 0.6539 - val_accuracy: 0.8542\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.6231 - accuracy: 0.8587 - val_loss: 0.6434 - val_accuracy: 0.8542\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.6118 - accuracy: 0.8587 - val_loss: 0.6322 - val_accuracy: 0.8542\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.6004 - accuracy: 0.8750 - val_loss: 0.6208 - val_accuracy: 0.8542\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.5888 - accuracy: 0.8750 - val_loss: 0.6098 - val_accuracy: 0.8542\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.5772 - accuracy: 0.8804 - val_loss: 0.5997 - val_accuracy: 0.8542\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5659 - accuracy: 0.8913 - val_loss: 0.5909 - val_accuracy: 0.8542\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.5553 - accuracy: 0.9022 - val_loss: 0.5824 - val_accuracy: 0.8750\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5455 - accuracy: 0.9130 - val_loss: 0.5740 - val_accuracy: 0.8750\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.5361 - accuracy: 0.9130 - val_loss: 0.5659 - val_accuracy: 0.8750\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5269 - accuracy: 0.9130 - val_loss: 0.5577 - val_accuracy: 0.8750\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.5180 - accuracy: 0.9130 - val_loss: 0.5496 - val_accuracy: 0.8750\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.5094 - accuracy: 0.9130 - val_loss: 0.5418 - val_accuracy: 0.8750\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.5013 - accuracy: 0.9130 - val_loss: 0.5346 - val_accuracy: 0.8750\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.4937 - accuracy: 0.9130 - val_loss: 0.5279 - val_accuracy: 0.8750\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4863 - accuracy: 0.9130 - val_loss: 0.5217 - val_accuracy: 0.8750\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.4791 - accuracy: 0.9130 - val_loss: 0.5157 - val_accuracy: 0.8958\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.4720 - accuracy: 0.9185 - val_loss: 0.5100 - val_accuracy: 0.8958\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4652 - accuracy: 0.9185 - val_loss: 0.5045 - val_accuracy: 0.8958\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.4586 - accuracy: 0.9185 - val_loss: 0.4991 - val_accuracy: 0.8958\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.4524 - accuracy: 0.9185 - val_loss: 0.4935 - val_accuracy: 0.8958\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.4462 - accuracy: 0.9185 - val_loss: 0.4879 - val_accuracy: 0.9167\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4402 - accuracy: 0.9239 - val_loss: 0.4823 - val_accuracy: 0.9167\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.4344 - accuracy: 0.9239 - val_loss: 0.4768 - val_accuracy: 0.9167\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.4288 - accuracy: 0.9239 - val_loss: 0.4715 - val_accuracy: 0.9167\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.4232 - accuracy: 0.9239 - val_loss: 0.4665 - val_accuracy: 0.9167\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.4179 - accuracy: 0.9239 - val_loss: 0.4617 - val_accuracy: 0.9167\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.4127 - accuracy: 0.9293 - val_loss: 0.4571 - val_accuracy: 0.9167\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.4076 - accuracy: 0.9293 - val_loss: 0.4527 - val_accuracy: 0.9167\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.4026 - accuracy: 0.9293 - val_loss: 0.4485 - val_accuracy: 0.9167\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.3978 - accuracy: 0.9293 - val_loss: 0.4443 - val_accuracy: 0.9167\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.3930 - accuracy: 0.9293 - val_loss: 0.4402 - val_accuracy: 0.9167\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.3884 - accuracy: 0.9348 - val_loss: 0.4362 - val_accuracy: 0.9167\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3838 - accuracy: 0.9402 - val_loss: 0.4323 - val_accuracy: 0.9167\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.3795 - accuracy: 0.9402 - val_loss: 0.4286 - val_accuracy: 0.9167\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.3751 - accuracy: 0.9402 - val_loss: 0.4252 - val_accuracy: 0.9167\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.3710 - accuracy: 0.9402 - val_loss: 0.4219 - val_accuracy: 0.9167\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3668 - accuracy: 0.9402 - val_loss: 0.4187 - val_accuracy: 0.9167\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.3628 - accuracy: 0.9402 - val_loss: 0.4156 - val_accuracy: 0.9167\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.3589 - accuracy: 0.9457 - val_loss: 0.4127 - val_accuracy: 0.9167\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.3550 - accuracy: 0.9457 - val_loss: 0.4099 - val_accuracy: 0.9167\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.3512 - accuracy: 0.9457 - val_loss: 0.4070 - val_accuracy: 0.9167\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.3475 - accuracy: 0.9457 - val_loss: 0.4043 - val_accuracy: 0.9167\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.3438 - accuracy: 0.9457 - val_loss: 0.4016 - val_accuracy: 0.9167\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.3402 - accuracy: 0.9457 - val_loss: 0.3992 - val_accuracy: 0.9167\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.3367 - accuracy: 0.9457 - val_loss: 0.3968 - val_accuracy: 0.9167\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.3332 - accuracy: 0.9457 - val_loss: 0.3944 - val_accuracy: 0.9167\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3298 - accuracy: 0.9457 - val_loss: 0.3921 - val_accuracy: 0.9167\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.3264 - accuracy: 0.9457 - val_loss: 0.3900 - val_accuracy: 0.9167\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.3231 - accuracy: 0.9457 - val_loss: 0.3880 - val_accuracy: 0.9167\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.3199 - accuracy: 0.9457 - val_loss: 0.3859 - val_accuracy: 0.9167\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3167 - accuracy: 0.9457 - val_loss: 0.3839 - val_accuracy: 0.9167\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.3136 - accuracy: 0.9457 - val_loss: 0.3819 - val_accuracy: 0.9167\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.3104 - accuracy: 0.9511 - val_loss: 0.3801 - val_accuracy: 0.9167\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.3073 - accuracy: 0.9511 - val_loss: 0.3784 - val_accuracy: 0.9167\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.3042 - accuracy: 0.9511 - val_loss: 0.3767 - val_accuracy: 0.9167\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.3012 - accuracy: 0.9511 - val_loss: 0.3749 - val_accuracy: 0.9167\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2982 - accuracy: 0.9511 - val_loss: 0.3732 - val_accuracy: 0.9167\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2953 - accuracy: 0.9565 - val_loss: 0.3714 - val_accuracy: 0.9167\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2924 - accuracy: 0.9565 - val_loss: 0.3699 - val_accuracy: 0.9167\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2895 - accuracy: 0.9565 - val_loss: 0.3686 - val_accuracy: 0.9167\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.2866 - accuracy: 0.9565 - val_loss: 0.3675 - val_accuracy: 0.9167\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2839 - accuracy: 0.9565 - val_loss: 0.3662 - val_accuracy: 0.9167\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2811 - accuracy: 0.9620 - val_loss: 0.3646 - val_accuracy: 0.9167\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.2784 - accuracy: 0.9620 - val_loss: 0.3631 - val_accuracy: 0.9167\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.2757 - accuracy: 0.9674 - val_loss: 0.3616 - val_accuracy: 0.9167\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2731 - accuracy: 0.9674 - val_loss: 0.3604 - val_accuracy: 0.9167\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.2705 - accuracy: 0.9674 - val_loss: 0.3593 - val_accuracy: 0.9167\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2679 - accuracy: 0.9674 - val_loss: 0.3583 - val_accuracy: 0.9167\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2654 - accuracy: 0.9674 - val_loss: 0.3573 - val_accuracy: 0.9167\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2630 - accuracy: 0.9674 - val_loss: 0.3562 - val_accuracy: 0.9167\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.2605 - accuracy: 0.9674 - val_loss: 0.3549 - val_accuracy: 0.9167\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2581 - accuracy: 0.9674 - val_loss: 0.3537 - val_accuracy: 0.9167\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.2558 - accuracy: 0.9674 - val_loss: 0.3527 - val_accuracy: 0.9167\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2535 - accuracy: 0.9674 - val_loss: 0.3517 - val_accuracy: 0.9167\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.2512 - accuracy: 0.9674 - val_loss: 0.3505 - val_accuracy: 0.9167\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.2490 - accuracy: 0.9674 - val_loss: 0.3494 - val_accuracy: 0.9167\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2468 - accuracy: 0.9674 - val_loss: 0.3485 - val_accuracy: 0.9167\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2446 - accuracy: 0.9674 - val_loss: 0.3476 - val_accuracy: 0.9167\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.2425 - accuracy: 0.9674 - val_loss: 0.3466 - val_accuracy: 0.9167\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.2404 - accuracy: 0.9674 - val_loss: 0.3456 - val_accuracy: 0.9167\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2383 - accuracy: 0.9674 - val_loss: 0.3448 - val_accuracy: 0.9167\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2363 - accuracy: 0.9674 - val_loss: 0.3439 - val_accuracy: 0.9167\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2343 - accuracy: 0.9674 - val_loss: 0.3429 - val_accuracy: 0.9167\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 115us/step - loss: 0.2323 - accuracy: 0.9674 - val_loss: 0.3421 - val_accuracy: 0.9167\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2303 - accuracy: 0.9674 - val_loss: 0.3412 - val_accuracy: 0.9167\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2284 - accuracy: 0.9674 - val_loss: 0.3402 - val_accuracy: 0.9167\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.2265 - accuracy: 0.9674 - val_loss: 0.3393 - val_accuracy: 0.9167\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.2247 - accuracy: 0.9674 - val_loss: 0.3384 - val_accuracy: 0.9167\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2229 - accuracy: 0.9674 - val_loss: 0.3377 - val_accuracy: 0.9375\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.2211 - accuracy: 0.9783 - val_loss: 0.3368 - val_accuracy: 0.9375\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.2193 - accuracy: 0.9783 - val_loss: 0.3360 - val_accuracy: 0.9375\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2175 - accuracy: 0.9837 - val_loss: 0.3352 - val_accuracy: 0.9375\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.2158 - accuracy: 0.9837 - val_loss: 0.3343 - val_accuracy: 0.9375\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.2141 - accuracy: 0.9837 - val_loss: 0.3334 - val_accuracy: 0.9375\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.2125 - accuracy: 0.9837 - val_loss: 0.3326 - val_accuracy: 0.9375\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.2108 - accuracy: 0.9837 - val_loss: 0.3318 - val_accuracy: 0.9375\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.2092 - accuracy: 0.9837 - val_loss: 0.3310 - val_accuracy: 0.9375\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.2076 - accuracy: 0.9891 - val_loss: 0.3303 - val_accuracy: 0.9375\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.2061 - accuracy: 0.9891 - val_loss: 0.3294 - val_accuracy: 0.9375\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2046 - accuracy: 0.9891 - val_loss: 0.3286 - val_accuracy: 0.9375\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.2030 - accuracy: 0.9891 - val_loss: 0.3277 - val_accuracy: 0.9375\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.2016 - accuracy: 0.9891 - val_loss: 0.3270 - val_accuracy: 0.9375\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.2001 - accuracy: 0.9891 - val_loss: 0.3264 - val_accuracy: 0.9375\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1986 - accuracy: 0.9891 - val_loss: 0.3255 - val_accuracy: 0.9583\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1972 - accuracy: 0.9891 - val_loss: 0.3245 - val_accuracy: 0.9583\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1958 - accuracy: 0.9891 - val_loss: 0.3235 - val_accuracy: 0.9583\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1944 - accuracy: 0.9891 - val_loss: 0.3228 - val_accuracy: 0.9583\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1931 - accuracy: 0.9891 - val_loss: 0.3222 - val_accuracy: 0.9583\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1917 - accuracy: 0.9891 - val_loss: 0.3217 - val_accuracy: 0.9583\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1904 - accuracy: 0.9891 - val_loss: 0.3211 - val_accuracy: 0.9583\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1891 - accuracy: 0.9891 - val_loss: 0.3203 - val_accuracy: 0.9583\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.1878 - accuracy: 0.9891 - val_loss: 0.3193 - val_accuracy: 0.9583\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1865 - accuracy: 0.9891 - val_loss: 0.3183 - val_accuracy: 0.9583\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1853 - accuracy: 0.9891 - val_loss: 0.3176 - val_accuracy: 0.9583\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1841 - accuracy: 0.9891 - val_loss: 0.3171 - val_accuracy: 0.9583\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1828 - accuracy: 0.9891 - val_loss: 0.3166 - val_accuracy: 0.9583\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 90us/step - loss: 0.1816 - accuracy: 0.9891 - val_loss: 0.3161 - val_accuracy: 0.9583\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1804 - accuracy: 0.9891 - val_loss: 0.3155 - val_accuracy: 0.9583\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1793 - accuracy: 0.9891 - val_loss: 0.3148 - val_accuracy: 0.9583\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1781 - accuracy: 0.9891 - val_loss: 0.3139 - val_accuracy: 0.9583\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1770 - accuracy: 0.9891 - val_loss: 0.3132 - val_accuracy: 0.9583\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1759 - accuracy: 0.9891 - val_loss: 0.3126 - val_accuracy: 0.9583\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.1747 - accuracy: 0.9891 - val_loss: 0.3121 - val_accuracy: 0.9583\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1736 - accuracy: 0.9891 - val_loss: 0.3114 - val_accuracy: 0.9583\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1726 - accuracy: 0.9891 - val_loss: 0.3106 - val_accuracy: 0.9583\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1715 - accuracy: 0.9891 - val_loss: 0.3099 - val_accuracy: 0.9583\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 87us/step - loss: 0.1704 - accuracy: 0.9891 - val_loss: 0.3094 - val_accuracy: 0.9583\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1694 - accuracy: 0.9946 - val_loss: 0.3089 - val_accuracy: 0.9583\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1684 - accuracy: 0.9946 - val_loss: 0.3084 - val_accuracy: 0.9583\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1674 - accuracy: 0.9946 - val_loss: 0.3076 - val_accuracy: 0.9583\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1664 - accuracy: 0.9946 - val_loss: 0.3068 - val_accuracy: 0.9583\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1654 - accuracy: 0.9946 - val_loss: 0.3062 - val_accuracy: 0.9583\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1644 - accuracy: 0.9946 - val_loss: 0.3057 - val_accuracy: 0.9583\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1635 - accuracy: 0.9946 - val_loss: 0.3053 - val_accuracy: 0.9583\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1625 - accuracy: 0.9946 - val_loss: 0.3048 - val_accuracy: 0.9583\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.1616 - accuracy: 0.9946 - val_loss: 0.3042 - val_accuracy: 0.9583\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.1607 - accuracy: 0.9946 - val_loss: 0.3035 - val_accuracy: 0.9583\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1598 - accuracy: 0.9946 - val_loss: 0.3030 - val_accuracy: 0.9583\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1588 - accuracy: 0.9946 - val_loss: 0.3026 - val_accuracy: 0.9583\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1580 - accuracy: 0.9946 - val_loss: 0.3022 - val_accuracy: 0.9583\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1571 - accuracy: 0.9946 - val_loss: 0.3018 - val_accuracy: 0.9583\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1562 - accuracy: 0.9946 - val_loss: 0.3013 - val_accuracy: 0.9583\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1553 - accuracy: 0.9946 - val_loss: 0.3007 - val_accuracy: 0.9583\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1545 - accuracy: 0.9946 - val_loss: 0.3001 - val_accuracy: 0.9583\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1536 - accuracy: 0.9946 - val_loss: 0.2995 - val_accuracy: 0.9583\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1528 - accuracy: 0.9946 - val_loss: 0.2990 - val_accuracy: 0.9583\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.1520 - accuracy: 0.9946 - val_loss: 0.2984 - val_accuracy: 0.9583\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.1512 - accuracy: 0.9946 - val_loss: 0.2981 - val_accuracy: 0.9583\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1504 - accuracy: 0.9946 - val_loss: 0.2977 - val_accuracy: 0.9583\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1496 - accuracy: 1.0000 - val_loss: 0.2973 - val_accuracy: 0.9583\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1488 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.9583\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1480 - accuracy: 1.0000 - val_loss: 0.2960 - val_accuracy: 0.9583\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1473 - accuracy: 1.0000 - val_loss: 0.2953 - val_accuracy: 0.9583\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1465 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.9583\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1458 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.9583\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.1450 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.9583\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.1443 - accuracy: 1.0000 - val_loss: 0.2948 - val_accuracy: 0.9583\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1435 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9583\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 134us/step - loss: 0.1428 - accuracy: 1.0000 - val_loss: 0.2939 - val_accuracy: 0.9583\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 109us/step - loss: 0.1421 - accuracy: 1.0000 - val_loss: 0.2933 - val_accuracy: 0.9583\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.1414 - accuracy: 1.0000 - val_loss: 0.2929 - val_accuracy: 0.9583\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 90us/step - loss: 0.1407 - accuracy: 1.0000 - val_loss: 0.2927 - val_accuracy: 0.9583\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1400 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.9583\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1393 - accuracy: 1.0000 - val_loss: 0.2924 - val_accuracy: 0.9583\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1387 - accuracy: 1.0000 - val_loss: 0.2921 - val_accuracy: 0.9583\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1380 - accuracy: 1.0000 - val_loss: 0.2918 - val_accuracy: 0.9583\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1373 - accuracy: 1.0000 - val_loss: 0.2914 - val_accuracy: 0.9583\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1367 - accuracy: 1.0000 - val_loss: 0.2909 - val_accuracy: 0.9583\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1360 - accuracy: 1.0000 - val_loss: 0.2905 - val_accuracy: 0.9583\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1354 - accuracy: 1.0000 - val_loss: 0.2901 - val_accuracy: 0.9583\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.1348 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.9583\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.1341 - accuracy: 1.0000 - val_loss: 0.2895 - val_accuracy: 0.9583\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1335 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 0.9583\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1329 - accuracy: 1.0000 - val_loss: 0.2886 - val_accuracy: 0.9583\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1323 - accuracy: 1.0000 - val_loss: 0.2882 - val_accuracy: 0.9583\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.2879 - val_accuracy: 0.9583\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1311 - accuracy: 1.0000 - val_loss: 0.2877 - val_accuracy: 0.9583\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1305 - accuracy: 1.0000 - val_loss: 0.2874 - val_accuracy: 0.9583\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1299 - accuracy: 1.0000 - val_loss: 0.2869 - val_accuracy: 0.9583\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 0.9583\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1288 - accuracy: 1.0000 - val_loss: 0.2861 - val_accuracy: 0.9583\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.1282 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9583\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.1276 - accuracy: 1.0000 - val_loss: 0.2857 - val_accuracy: 0.9583\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.1271 - accuracy: 1.0000 - val_loss: 0.2854 - val_accuracy: 0.9583\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1265 - accuracy: 1.0000 - val_loss: 0.2848 - val_accuracy: 0.9583\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1260 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.9583\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.1254 - accuracy: 1.0000 - val_loss: 0.2840 - val_accuracy: 0.9583\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1249 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 0.9583\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1244 - accuracy: 1.0000 - val_loss: 0.2834 - val_accuracy: 0.9583\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 95us/step - loss: 0.1238 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9583\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1233 - accuracy: 1.0000 - val_loss: 0.2826 - val_accuracy: 0.9583\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 0.2822 - val_accuracy: 0.9583\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.1223 - accuracy: 1.0000 - val_loss: 0.2819 - val_accuracy: 0.9583\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1218 - accuracy: 1.0000 - val_loss: 0.2817 - val_accuracy: 0.9583\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9583\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1208 - accuracy: 1.0000 - val_loss: 0.2811 - val_accuracy: 0.9583\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1203 - accuracy: 1.0000 - val_loss: 0.2808 - val_accuracy: 0.9583\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.2804 - val_accuracy: 0.9583\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1193 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9583\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1189 - accuracy: 1.0000 - val_loss: 0.2799 - val_accuracy: 0.9583\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1184 - accuracy: 1.0000 - val_loss: 0.2796 - val_accuracy: 0.9583\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1179 - accuracy: 1.0000 - val_loss: 0.2791 - val_accuracy: 0.9583\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 0.9583\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1170 - accuracy: 1.0000 - val_loss: 0.2783 - val_accuracy: 0.9583\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9583\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1161 - accuracy: 1.0000 - val_loss: 0.2779 - val_accuracy: 0.9583\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1156 - accuracy: 1.0000 - val_loss: 0.2775 - val_accuracy: 0.9583\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.1152 - accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 0.9583\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1147 - accuracy: 1.0000 - val_loss: 0.2767 - val_accuracy: 0.9583\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.2765 - val_accuracy: 0.9583\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 129us/step - loss: 0.1139 - accuracy: 1.0000 - val_loss: 0.2764 - val_accuracy: 0.9583\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.1134 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9583\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.2758 - val_accuracy: 0.9583\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.1126 - accuracy: 1.0000 - val_loss: 0.2754 - val_accuracy: 0.9583\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.2751 - val_accuracy: 0.9583\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1117 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9583\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.1113 - accuracy: 1.0000 - val_loss: 0.2747 - val_accuracy: 0.9583\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1109 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9583\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1105 - accuracy: 1.0000 - val_loss: 0.2739 - val_accuracy: 0.9583\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1101 - accuracy: 1.0000 - val_loss: 0.2735 - val_accuracy: 0.9583\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1097 - accuracy: 1.0000 - val_loss: 0.2732 - val_accuracy: 0.9583\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 113us/step - loss: 0.1093 - accuracy: 1.0000 - val_loss: 0.2730 - val_accuracy: 0.9583\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 79us/step - loss: 0.1089 - accuracy: 1.0000 - val_loss: 0.2727 - val_accuracy: 0.9583\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.1085 - accuracy: 1.0000 - val_loss: 0.2723 - val_accuracy: 0.9583\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.1081 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9583\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9583\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1074 - accuracy: 1.0000 - val_loss: 0.2717 - val_accuracy: 0.9583\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.9583\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.1066 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9583\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.1063 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9583\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1059 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9583\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1055 - accuracy: 1.0000 - val_loss: 0.2705 - val_accuracy: 0.9583\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.1052 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.9583\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.1048 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9583\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.1044 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 0.9583\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.1041 - accuracy: 1.0000 - val_loss: 0.2693 - val_accuracy: 0.9583\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1037 - accuracy: 1.0000 - val_loss: 0.2691 - val_accuracy: 0.9583\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.1034 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9583\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1030 - accuracy: 1.0000 - val_loss: 0.2687 - val_accuracy: 0.9583\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 0.9583\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1024 - accuracy: 1.0000 - val_loss: 0.2681 - val_accuracy: 0.9583\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.1020 - accuracy: 1.0000 - val_loss: 0.2679 - val_accuracy: 0.9583\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.1017 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9583\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.1014 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9583\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9583\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.1007 - accuracy: 1.0000 - val_loss: 0.2670 - val_accuracy: 0.9583\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.1004 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 0.9583\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.1001 - accuracy: 1.0000 - val_loss: 0.2664 - val_accuracy: 0.9583\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0997 - accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 0.9583\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0994 - accuracy: 1.0000 - val_loss: 0.2661 - val_accuracy: 0.9583\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.0991 - accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 0.9583\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0988 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9583\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0985 - accuracy: 1.0000 - val_loss: 0.2652 - val_accuracy: 0.9583\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.0982 - accuracy: 1.0000 - val_loss: 0.2650 - val_accuracy: 0.9583\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0979 - accuracy: 1.0000 - val_loss: 0.2648 - val_accuracy: 0.9583\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0976 - accuracy: 1.0000 - val_loss: 0.2648 - val_accuracy: 0.9583\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0973 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9583\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0970 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9583\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0967 - accuracy: 1.0000 - val_loss: 0.2641 - val_accuracy: 0.9583\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9583\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0961 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9583\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0958 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 0.9583\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.2634 - val_accuracy: 0.9583\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.0952 - accuracy: 1.0000 - val_loss: 0.2630 - val_accuracy: 0.9583\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 81us/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.2628 - val_accuracy: 0.9583\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0947 - accuracy: 1.0000 - val_loss: 0.2627 - val_accuracy: 0.9583\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.2625 - val_accuracy: 0.9583\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0941 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9583\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0939 - accuracy: 1.0000 - val_loss: 0.2620 - val_accuracy: 0.9583\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0936 - accuracy: 1.0000 - val_loss: 0.2619 - val_accuracy: 0.9583\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.2618 - val_accuracy: 0.9583\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.2616 - val_accuracy: 0.9583\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0928 - accuracy: 1.0000 - val_loss: 0.2614 - val_accuracy: 0.9583\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.0925 - accuracy: 1.0000 - val_loss: 0.2613 - val_accuracy: 0.9583\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0923 - accuracy: 1.0000 - val_loss: 0.2611 - val_accuracy: 0.9583\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 0.2608 - val_accuracy: 0.9583\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.0917 - accuracy: 1.0000 - val_loss: 0.2606 - val_accuracy: 0.9583\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.2603 - val_accuracy: 0.9583\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0912 - accuracy: 1.0000 - val_loss: 0.2600 - val_accuracy: 0.9583\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0910 - accuracy: 1.0000 - val_loss: 0.2598 - val_accuracy: 0.9583\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.0907 - accuracy: 1.0000 - val_loss: 0.2595 - val_accuracy: 0.9583\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9583\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.0902 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9583\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.2593 - val_accuracy: 0.9583\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0897 - accuracy: 1.0000 - val_loss: 0.2591 - val_accuracy: 0.9583\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.0895 - accuracy: 1.0000 - val_loss: 0.2589 - val_accuracy: 0.9583\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 100us/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 0.2589 - val_accuracy: 0.9583\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.0890 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9583\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 84us/step - loss: 0.0888 - accuracy: 1.0000 - val_loss: 0.2585 - val_accuracy: 0.9583\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 0.2581 - val_accuracy: 0.9583\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 0.2580 - val_accuracy: 0.9583\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 80us/step - loss: 0.0881 - accuracy: 1.0000 - val_loss: 0.2580 - val_accuracy: 0.9583\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 0.9583\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.2576 - val_accuracy: 0.9583\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 0.9583\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.0872 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 0.9583\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0869 - accuracy: 1.0000 - val_loss: 0.2573 - val_accuracy: 0.9583\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.0867 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9583\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.2566 - val_accuracy: 0.9583\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 84us/step - loss: 0.0863 - accuracy: 1.0000 - val_loss: 0.2562 - val_accuracy: 0.9583\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.9583\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.0858 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.9583\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0856 - accuracy: 1.0000 - val_loss: 0.2560 - val_accuracy: 0.9583\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0854 - accuracy: 1.0000 - val_loss: 0.2557 - val_accuracy: 0.9583\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0852 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9583\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0850 - accuracy: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.9583\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.0848 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9583\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0846 - accuracy: 1.0000 - val_loss: 0.2553 - val_accuracy: 0.9583\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0844 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9583\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0842 - accuracy: 1.0000 - val_loss: 0.2546 - val_accuracy: 0.9583\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.0840 - accuracy: 1.0000 - val_loss: 0.2546 - val_accuracy: 0.9583\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0838 - accuracy: 1.0000 - val_loss: 0.2546 - val_accuracy: 0.9583\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0836 - accuracy: 1.0000 - val_loss: 0.2545 - val_accuracy: 0.9583\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9583\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 0.2540 - val_accuracy: 0.9583\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 0.2539 - val_accuracy: 0.9583\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.2540 - val_accuracy: 0.9583\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0826 - accuracy: 1.0000 - val_loss: 0.2539 - val_accuracy: 0.9583\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0824 - accuracy: 1.0000 - val_loss: 0.2537 - val_accuracy: 0.9583\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0822 - accuracy: 1.0000 - val_loss: 0.2534 - val_accuracy: 0.9583\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 0.2531 - val_accuracy: 0.9583\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.2528 - val_accuracy: 0.9583\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0816 - accuracy: 1.0000 - val_loss: 0.2528 - val_accuracy: 0.9583\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.9583\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 0.2527 - val_accuracy: 0.9583\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0811 - accuracy: 1.0000 - val_loss: 0.2523 - val_accuracy: 0.9583\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0809 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9583\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0807 - accuracy: 1.0000 - val_loss: 0.2523 - val_accuracy: 0.9583\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0805 - accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 0.9583\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.0803 - accuracy: 1.0000 - val_loss: 0.2522 - val_accuracy: 0.9583\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0801 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9583\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0800 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9583\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0798 - accuracy: 1.0000 - val_loss: 0.2514 - val_accuracy: 0.9583\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 0.2513 - val_accuracy: 0.9583\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.2512 - val_accuracy: 0.9583\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0793 - accuracy: 1.0000 - val_loss: 0.2512 - val_accuracy: 0.9583\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0791 - accuracy: 1.0000 - val_loss: 0.2511 - val_accuracy: 0.9583\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0789 - accuracy: 1.0000 - val_loss: 0.2509 - val_accuracy: 0.9583\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.0787 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9583\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0786 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9583\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0784 - accuracy: 1.0000 - val_loss: 0.2509 - val_accuracy: 0.9583\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.0782 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9583\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0781 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.9583\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0779 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.9583\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 0.2506 - val_accuracy: 0.9583\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0776 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9583\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 66us/step - loss: 0.0774 - accuracy: 1.0000 - val_loss: 0.2501 - val_accuracy: 0.9583\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0772 - accuracy: 1.0000 - val_loss: 0.2498 - val_accuracy: 0.9583\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 0.2497 - val_accuracy: 0.9583\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0769 - accuracy: 1.0000 - val_loss: 0.2496 - val_accuracy: 0.9583\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0768 - accuracy: 1.0000 - val_loss: 0.2494 - val_accuracy: 0.9583\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 97us/step - loss: 0.0766 - accuracy: 1.0000 - val_loss: 0.2492 - val_accuracy: 0.9583\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 61us/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 0.9583\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0763 - accuracy: 1.0000 - val_loss: 0.2490 - val_accuracy: 0.9583\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0761 - accuracy: 1.0000 - val_loss: 0.2490 - val_accuracy: 0.9583\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9583\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0758 - accuracy: 1.0000 - val_loss: 0.2488 - val_accuracy: 0.9583\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0757 - accuracy: 1.0000 - val_loss: 0.2486 - val_accuracy: 0.9583\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0755 - accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 0.9583\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 0.2484 - val_accuracy: 0.9583\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0752 - accuracy: 1.0000 - val_loss: 0.2482 - val_accuracy: 0.9583\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0751 - accuracy: 1.0000 - val_loss: 0.2480 - val_accuracy: 0.9583\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0749 - accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.9583\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 0.2480 - val_accuracy: 0.9583\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0746 - accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.9583\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0745 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 0.9583\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9583\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 0.2473 - val_accuracy: 0.9583\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0741 - accuracy: 1.0000 - val_loss: 0.2473 - val_accuracy: 0.9583\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.0739 - accuracy: 1.0000 - val_loss: 0.2474 - val_accuracy: 0.9583\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.0738 - accuracy: 1.0000 - val_loss: 0.2474 - val_accuracy: 0.9583\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0736 - accuracy: 1.0000 - val_loss: 0.2473 - val_accuracy: 0.9583\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0735 - accuracy: 1.0000 - val_loss: 0.2472 - val_accuracy: 0.9583\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0734 - accuracy: 1.0000 - val_loss: 0.2470 - val_accuracy: 0.9583\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0732 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9583\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0731 - accuracy: 1.0000 - val_loss: 0.2467 - val_accuracy: 0.9583\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.0729 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9583\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.9583\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 85us/step - loss: 0.0727 - accuracy: 1.0000 - val_loss: 0.2460 - val_accuracy: 0.9583\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.0725 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.9583\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 0.2459 - val_accuracy: 0.9583\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0723 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9583\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0721 - accuracy: 1.0000 - val_loss: 0.2454 - val_accuracy: 0.9583\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 76us/step - loss: 0.0720 - accuracy: 1.0000 - val_loss: 0.2455 - val_accuracy: 0.9583\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.0719 - accuracy: 1.0000 - val_loss: 0.2456 - val_accuracy: 0.9583\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.2455 - val_accuracy: 0.9583\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 0.2453 - val_accuracy: 0.9583\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0715 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9583\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0713 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9583\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.2448 - val_accuracy: 0.9583\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9583\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0710 - accuracy: 1.0000 - val_loss: 0.2450 - val_accuracy: 0.9583\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0708 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9583\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0707 - accuracy: 1.0000 - val_loss: 0.2448 - val_accuracy: 0.9583\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 0.2448 - val_accuracy: 0.9583\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0705 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9583\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 0.2446 - val_accuracy: 0.9583\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.9583\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.0701 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.9583\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0700 - accuracy: 1.0000 - val_loss: 0.2442 - val_accuracy: 0.9583\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.2442 - val_accuracy: 0.9583\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0697 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9583\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.0696 - accuracy: 1.0000 - val_loss: 0.2435 - val_accuracy: 0.9583\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9583\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0694 - accuracy: 1.0000 - val_loss: 0.2442 - val_accuracy: 0.9583\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0693 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.9583\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 75us/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 0.2435 - val_accuracy: 0.9583\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.0690 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9583\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0689 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.9583\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.9583\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 0.2431 - val_accuracy: 0.9583\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.9583\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.0685 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 0.9583\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.0683 - accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 0.9583\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 0.9583\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 0.9583\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9583\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0678 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9583\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.0677 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9583\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0675 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0674 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9583\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0672 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 0.9583\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 0.9583\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0669 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9583\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0668 - accuracy: 1.0000 - val_loss: 0.2425 - val_accuracy: 0.9583\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 0.9583\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 73us/step - loss: 0.0666 - accuracy: 1.0000 - val_loss: 0.2419 - val_accuracy: 0.9583\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0665 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9583\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.0664 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9583\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.2416 - val_accuracy: 0.9583\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.2414 - val_accuracy: 0.9583\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0661 - accuracy: 1.0000 - val_loss: 0.2413 - val_accuracy: 0.9583\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9583\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9583\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0658 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9583\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.0657 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9583\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9583\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0655 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9583\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 67us/step - loss: 0.0654 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9583\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.0653 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9583\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9583\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 0.9583\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 0.9583\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9583\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 65us/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9583\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0647 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9583\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9583\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0645 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9583\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.9583\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9583\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9583\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9583\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9583\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0640 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.9583\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0639 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.9583\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 71us/step - loss: 0.0638 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9583\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0637 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9583\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.2398 - val_accuracy: 0.9583\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 0.2396 - val_accuracy: 0.9583\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.2393 - val_accuracy: 0.9583\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.0633 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 0.9583\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 0.9583\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.2393 - val_accuracy: 0.9583\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9583\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9583\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 95us/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9583\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.0628 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 0.9583\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9583\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.2387 - val_accuracy: 0.9583\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 60us/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9583\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9583\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 0.9583\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 63us/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 70us/step - loss: 0.0622 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 69us/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8nZswCbNFl3d"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6g1W5VoYFl3d",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TDnMuIJMFl3k",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9cb54696-2325-4f07-99b5-1f52e97f72d1",
        "id": "QNntaA6rFl3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd72df96a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Pcr8dYkFl3x"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7e402f5a-95e1-44cf-b394-aef081c63fa7",
        "id": "xF6aLHBgFl3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd72df80da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 317
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0f7Lx6xFl36"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "abf56940-e2e0-45ef-ef9f-0faa3de95906",
        "id": "5H6LrO6jFl3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_labels, epochs= num_epochs, batch_size=92, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 0s 845us/step - loss: 0.9322 - accuracy: 0.7120\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.9000 - accuracy: 0.7011\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.8808 - accuracy: 0.7065\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.8576 - accuracy: 0.7065\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.8343 - accuracy: 0.7120\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.8139 - accuracy: 0.7228\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.7963 - accuracy: 0.7337\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.7792 - accuracy: 0.7500\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.7611 - accuracy: 0.7663\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.7430 - accuracy: 0.7772\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.7257 - accuracy: 0.8098\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.7088 - accuracy: 0.8043\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.6925 - accuracy: 0.8152\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6767 - accuracy: 0.8315\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6610 - accuracy: 0.8587\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.6457 - accuracy: 0.8696\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.6315 - accuracy: 0.8804\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.6183 - accuracy: 0.8859\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.6056 - accuracy: 0.8913\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.5930 - accuracy: 0.9022\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5809 - accuracy: 0.9076\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.5693 - accuracy: 0.9076\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.5585 - accuracy: 0.9130\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.5482 - accuracy: 0.9130\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.5381 - accuracy: 0.9130\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.5284 - accuracy: 0.9185\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.5191 - accuracy: 0.9185\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.5103 - accuracy: 0.9239\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.5019 - accuracy: 0.9239\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 0s 47us/step - loss: 0.4936 - accuracy: 0.9239\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 0s 83us/step - loss: 0.4855 - accuracy: 0.9239\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.4777 - accuracy: 0.9293\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.4704 - accuracy: 0.9293\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.4632 - accuracy: 0.9239\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 0s 54us/step - loss: 0.4562 - accuracy: 0.9293\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.4493 - accuracy: 0.9239\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.4428 - accuracy: 0.9293\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.4364 - accuracy: 0.9293\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.4302 - accuracy: 0.9293\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.4241 - accuracy: 0.9293\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.4181 - accuracy: 0.9239\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.4124 - accuracy: 0.9239\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.4068 - accuracy: 0.9239\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.4012 - accuracy: 0.9239\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.3958 - accuracy: 0.9239\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3905 - accuracy: 0.9293\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 0s 55us/step - loss: 0.3854 - accuracy: 0.9348\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.3804 - accuracy: 0.9402\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.3755 - accuracy: 0.9402\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.3707 - accuracy: 0.9402\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3660 - accuracy: 0.9402\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3614 - accuracy: 0.9402\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 0s 62us/step - loss: 0.3569 - accuracy: 0.9402\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.3525 - accuracy: 0.9402\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.3482 - accuracy: 0.9402\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 0s 72us/step - loss: 0.3440 - accuracy: 0.9402\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.3398 - accuracy: 0.9457\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.3357 - accuracy: 0.9457\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3317 - accuracy: 0.9457\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.3278 - accuracy: 0.9457\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 0s 57us/step - loss: 0.3240 - accuracy: 0.9457\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.3202 - accuracy: 0.9511\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.3165 - accuracy: 0.9511\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.3129 - accuracy: 0.9511\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.3094 - accuracy: 0.9511\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3059 - accuracy: 0.9511\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.3024 - accuracy: 0.9511\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2990 - accuracy: 0.9511\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2957 - accuracy: 0.9511\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2925 - accuracy: 0.9511\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2893 - accuracy: 0.9565\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2861 - accuracy: 0.9565\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.2831 - accuracy: 0.9565\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.2800 - accuracy: 0.9565\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2771 - accuracy: 0.9620\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2741 - accuracy: 0.9674\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2713 - accuracy: 0.9674\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2684 - accuracy: 0.9674\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2657 - accuracy: 0.9674\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2629 - accuracy: 0.9674\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2602 - accuracy: 0.9674\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2576 - accuracy: 0.9674\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.2550 - accuracy: 0.9674\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2525 - accuracy: 0.9674\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.2500 - accuracy: 0.9674\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2475 - accuracy: 0.9674\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2451 - accuracy: 0.9674\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2428 - accuracy: 0.9674\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.2405 - accuracy: 0.9728\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2382 - accuracy: 0.9728\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2359 - accuracy: 0.9728\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.2337 - accuracy: 0.9728\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.2315 - accuracy: 0.9728\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.2294 - accuracy: 0.9728\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2273 - accuracy: 0.9728\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.2252 - accuracy: 0.9728\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.2232 - accuracy: 0.9728\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.2212 - accuracy: 0.9728\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.2192 - accuracy: 0.9728\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2173 - accuracy: 0.9728\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.2154 - accuracy: 0.9728\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2135 - accuracy: 0.9728\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.2117 - accuracy: 0.9728\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.2099 - accuracy: 0.9783\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.2081 - accuracy: 0.9783\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2064 - accuracy: 0.9783\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.2047 - accuracy: 0.9783\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.2030 - accuracy: 0.9783\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.2013 - accuracy: 0.9837\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1997 - accuracy: 0.9837\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1981 - accuracy: 0.9837\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1965 - accuracy: 0.9837\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1949 - accuracy: 0.9837\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1934 - accuracy: 0.9837\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1919 - accuracy: 0.9837\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1904 - accuracy: 0.9837\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1890 - accuracy: 0.9891\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1875 - accuracy: 0.9891\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1861 - accuracy: 0.9891\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1847 - accuracy: 0.9891\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1834 - accuracy: 0.9891\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1820 - accuracy: 0.9891\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1807 - accuracy: 0.9891\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1794 - accuracy: 0.9891\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1781 - accuracy: 0.9891\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1768 - accuracy: 0.9891\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1756 - accuracy: 0.9891\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1743 - accuracy: 0.9891\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1731 - accuracy: 0.9946\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1719 - accuracy: 0.9946\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1707 - accuracy: 0.9946\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1696 - accuracy: 0.9946\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1684 - accuracy: 0.9946\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1673 - accuracy: 0.9946\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1662 - accuracy: 0.9946\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1651 - accuracy: 0.9946\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1640 - accuracy: 0.9946\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1630 - accuracy: 0.9946\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.1619 - accuracy: 0.9946\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1609 - accuracy: 0.9946\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1599 - accuracy: 0.9946\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1589 - accuracy: 0.9946\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1579 - accuracy: 0.9946\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1569 - accuracy: 0.9946\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1559 - accuracy: 0.9946\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1550 - accuracy: 0.9946\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1540 - accuracy: 0.9946\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1531 - accuracy: 0.9946\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1522 - accuracy: 0.9946\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1513 - accuracy: 0.9946\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1504 - accuracy: 0.9946\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.1495 - accuracy: 0.9946\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1487 - accuracy: 0.9946\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1478 - accuracy: 0.9946\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1470 - accuracy: 0.9946\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1461 - accuracy: 0.9946\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1453 - accuracy: 0.9946\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1445 - accuracy: 0.9946\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1437 - accuracy: 0.9946\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1429 - accuracy: 0.9946\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1421 - accuracy: 0.9946\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1413 - accuracy: 0.9946\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1406 - accuracy: 0.9946\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1398 - accuracy: 0.9946\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1391 - accuracy: 0.9946\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1383 - accuracy: 0.9946\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1376 - accuracy: 0.9946\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1369 - accuracy: 0.9946\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1362 - accuracy: 0.9946\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1355 - accuracy: 1.0000\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1348 - accuracy: 1.0000\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1341 - accuracy: 1.0000\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1334 - accuracy: 1.0000\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1328 - accuracy: 1.0000\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1321 - accuracy: 1.0000\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1315 - accuracy: 1.0000\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1308 - accuracy: 1.0000\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1301 - accuracy: 1.0000\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1295 - accuracy: 1.0000\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1289 - accuracy: 1.0000\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1283 - accuracy: 1.0000\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1276 - accuracy: 1.0000\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1270 - accuracy: 1.0000\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.1264 - accuracy: 1.0000\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1258 - accuracy: 1.0000\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 0s 56us/step - loss: 0.1252 - accuracy: 1.0000\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1246 - accuracy: 1.0000\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1240 - accuracy: 1.0000\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1234 - accuracy: 1.0000\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1229 - accuracy: 1.0000\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1223 - accuracy: 1.0000\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1217 - accuracy: 1.0000\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1211 - accuracy: 1.0000\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1205 - accuracy: 1.0000\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1199 - accuracy: 1.0000\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1194 - accuracy: 1.0000\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1188 - accuracy: 1.0000\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1183 - accuracy: 1.0000\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1177 - accuracy: 1.0000\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1172 - accuracy: 1.0000\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1167 - accuracy: 1.0000\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1161 - accuracy: 1.0000\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1156 - accuracy: 1.0000\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1151 - accuracy: 1.0000\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1146 - accuracy: 1.0000\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1141 - accuracy: 1.0000\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1136 - accuracy: 1.0000\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.1131 - accuracy: 1.0000\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1126 - accuracy: 1.0000\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1121 - accuracy: 1.0000\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1116 - accuracy: 1.0000\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1111 - accuracy: 1.0000\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.1107 - accuracy: 1.0000\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1102 - accuracy: 1.0000\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1097 - accuracy: 1.0000\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1093 - accuracy: 1.0000\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1088 - accuracy: 1.0000\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.1084 - accuracy: 1.0000\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1079 - accuracy: 1.0000\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 0s 82us/step - loss: 0.1075 - accuracy: 1.0000\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.1070 - accuracy: 1.0000\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1066 - accuracy: 1.0000\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1061 - accuracy: 1.0000\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1057 - accuracy: 1.0000\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1053 - accuracy: 1.0000\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.1049 - accuracy: 1.0000\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1044 - accuracy: 1.0000\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1040 - accuracy: 1.0000\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1036 - accuracy: 1.0000\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.1032 - accuracy: 1.0000\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.1028 - accuracy: 1.0000\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.1024 - accuracy: 1.0000\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1020 - accuracy: 1.0000\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.1016 - accuracy: 1.0000\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.1012 - accuracy: 1.0000\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.1008 - accuracy: 1.0000\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1004 - accuracy: 1.0000\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.1001 - accuracy: 1.0000\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0997 - accuracy: 1.0000\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0993 - accuracy: 1.0000\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0989 - accuracy: 1.0000\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0986 - accuracy: 1.0000\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0982 - accuracy: 1.0000\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0978 - accuracy: 1.0000\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0975 - accuracy: 1.0000\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 0s 22us/step - loss: 0.0971 - accuracy: 1.0000\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0967 - accuracy: 1.0000\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0964 - accuracy: 1.0000\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0960 - accuracy: 1.0000\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0957 - accuracy: 1.0000\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0954 - accuracy: 1.0000\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0950 - accuracy: 1.0000\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0947 - accuracy: 1.0000\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0943 - accuracy: 1.0000\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0940 - accuracy: 1.0000\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0937 - accuracy: 1.0000\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0933 - accuracy: 1.0000\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0930 - accuracy: 1.0000\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 0s 68us/step - loss: 0.0927 - accuracy: 1.0000\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0924 - accuracy: 1.0000\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0920 - accuracy: 1.0000\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0917 - accuracy: 1.0000\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0914 - accuracy: 1.0000\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0911 - accuracy: 1.0000\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0908 - accuracy: 1.0000\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 0s 64us/step - loss: 0.0905 - accuracy: 1.0000\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 0s 53us/step - loss: 0.0902 - accuracy: 1.0000\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0899 - accuracy: 1.0000\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0896 - accuracy: 1.0000\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0893 - accuracy: 1.0000\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.0890 - accuracy: 1.0000\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0887 - accuracy: 1.0000\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0884 - accuracy: 1.0000\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0881 - accuracy: 1.0000\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0878 - accuracy: 1.0000\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0876 - accuracy: 1.0000\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0873 - accuracy: 1.0000\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0870 - accuracy: 1.0000\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0867 - accuracy: 1.0000\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 0s 38us/step - loss: 0.0864 - accuracy: 1.0000\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0861 - accuracy: 1.0000\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0858 - accuracy: 1.0000\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0855 - accuracy: 1.0000\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0853 - accuracy: 1.0000\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0850 - accuracy: 1.0000\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0847 - accuracy: 1.0000\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0844 - accuracy: 1.0000\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0842 - accuracy: 1.0000\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0839 - accuracy: 1.0000\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0836 - accuracy: 1.0000\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0834 - accuracy: 1.0000\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0831 - accuracy: 1.0000\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0828 - accuracy: 1.0000\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 0s 78us/step - loss: 0.0826 - accuracy: 1.0000\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0823 - accuracy: 1.0000\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 0s 77us/step - loss: 0.0821 - accuracy: 1.0000\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0818 - accuracy: 1.0000\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0815 - accuracy: 1.0000\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0813 - accuracy: 1.0000\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 0s 48us/step - loss: 0.0811 - accuracy: 1.0000\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0808 - accuracy: 1.0000\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0806 - accuracy: 1.0000\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0803 - accuracy: 1.0000\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0801 - accuracy: 1.0000\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0798 - accuracy: 1.0000\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0796 - accuracy: 1.0000\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0794 - accuracy: 1.0000\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0791 - accuracy: 1.0000\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0789 - accuracy: 1.0000\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 0s 49us/step - loss: 0.0787 - accuracy: 1.0000\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0784 - accuracy: 1.0000\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0782 - accuracy: 1.0000\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0780 - accuracy: 1.0000\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0777 - accuracy: 1.0000\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0775 - accuracy: 1.0000\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0773 - accuracy: 1.0000\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0771 - accuracy: 1.0000\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0769 - accuracy: 1.0000\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0766 - accuracy: 1.0000\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 0s 43us/step - loss: 0.0764 - accuracy: 1.0000\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0762 - accuracy: 1.0000\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0760 - accuracy: 1.0000\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 0s 59us/step - loss: 0.0758 - accuracy: 1.0000\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0756 - accuracy: 1.0000\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0754 - accuracy: 1.0000\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0752 - accuracy: 1.0000\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0750 - accuracy: 1.0000\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0748 - accuracy: 1.0000\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 0s 42us/step - loss: 0.0746 - accuracy: 1.0000\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 0s 81us/step - loss: 0.0743 - accuracy: 1.0000\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 0s 58us/step - loss: 0.0741 - accuracy: 1.0000\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 0s 41us/step - loss: 0.0739 - accuracy: 1.0000\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 0s 51us/step - loss: 0.0737 - accuracy: 1.0000\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 0s 46us/step - loss: 0.0736 - accuracy: 1.0000\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0734 - accuracy: 1.0000\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0731 - accuracy: 1.0000\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0730 - accuracy: 1.0000\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0728 - accuracy: 1.0000\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0726 - accuracy: 1.0000\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0724 - accuracy: 1.0000\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0722 - accuracy: 1.0000\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 0s 52us/step - loss: 0.0720 - accuracy: 1.0000\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 0s 44us/step - loss: 0.0718 - accuracy: 1.0000\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0716 - accuracy: 1.0000\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0715 - accuracy: 1.0000\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0713 - accuracy: 1.0000\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0711 - accuracy: 1.0000\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 0s 45us/step - loss: 0.0709 - accuracy: 1.0000\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0707 - accuracy: 1.0000\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0705 - accuracy: 1.0000\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 0s 23us/step - loss: 0.0704 - accuracy: 1.0000\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0702 - accuracy: 1.0000\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0700 - accuracy: 1.0000\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0698 - accuracy: 1.0000\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0696 - accuracy: 1.0000\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0695 - accuracy: 1.0000\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0693 - accuracy: 1.0000\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0691 - accuracy: 1.0000\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0690 - accuracy: 1.0000\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0688 - accuracy: 1.0000\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0686 - accuracy: 1.0000\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0684 - accuracy: 1.0000\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0682 - accuracy: 1.0000\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0681 - accuracy: 1.0000\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0679 - accuracy: 1.0000\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0677 - accuracy: 1.0000\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0675 - accuracy: 1.0000\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0673 - accuracy: 1.0000\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0671 - accuracy: 1.0000\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0669 - accuracy: 1.0000\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0668 - accuracy: 1.0000\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0666 - accuracy: 1.0000\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0664 - accuracy: 1.0000\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0663 - accuracy: 1.0000\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0661 - accuracy: 1.0000\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0659 - accuracy: 1.0000\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0658 - accuracy: 1.0000\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0656 - accuracy: 1.0000\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 0s 81us/step - loss: 0.0655 - accuracy: 1.0000\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0653 - accuracy: 1.0000\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0651 - accuracy: 1.0000\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0650 - accuracy: 1.0000\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0648 - accuracy: 1.0000\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0647 - accuracy: 1.0000\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0645 - accuracy: 1.0000\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0644 - accuracy: 1.0000\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0642 - accuracy: 1.0000\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0640 - accuracy: 1.0000\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0639 - accuracy: 1.0000\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0637 - accuracy: 1.0000\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0636 - accuracy: 1.0000\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0634 - accuracy: 1.0000\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0633 - accuracy: 1.0000\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0632 - accuracy: 1.0000\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0630 - accuracy: 1.0000\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0629 - accuracy: 1.0000\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0627 - accuracy: 1.0000\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0626 - accuracy: 1.0000\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0624 - accuracy: 1.0000\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0623 - accuracy: 1.0000\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0622 - accuracy: 1.0000\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0620 - accuracy: 1.0000\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0619 - accuracy: 1.0000\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0617 - accuracy: 1.0000\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0616 - accuracy: 1.0000\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 0s 28us/step - loss: 0.0615 - accuracy: 1.0000\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0613 - accuracy: 1.0000\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0612 - accuracy: 1.0000\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0611 - accuracy: 1.0000\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0609 - accuracy: 1.0000\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 0s 40us/step - loss: 0.0608 - accuracy: 1.0000\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0607 - accuracy: 1.0000\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0606 - accuracy: 1.0000\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0604 - accuracy: 1.0000\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0603 - accuracy: 1.0000\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0602 - accuracy: 1.0000\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0600 - accuracy: 1.0000\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 0s 25us/step - loss: 0.0599 - accuracy: 1.0000\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0598 - accuracy: 1.0000\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0597 - accuracy: 1.0000\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0595 - accuracy: 1.0000\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0594 - accuracy: 1.0000\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0593 - accuracy: 1.0000\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0592 - accuracy: 1.0000\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0590 - accuracy: 1.0000\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 0s 50us/step - loss: 0.0589 - accuracy: 1.0000\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0588 - accuracy: 1.0000\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0587 - accuracy: 1.0000\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0586 - accuracy: 1.0000\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0584 - accuracy: 1.0000\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0583 - accuracy: 1.0000\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0582 - accuracy: 1.0000\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0581 - accuracy: 1.0000\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0580 - accuracy: 1.0000\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0579 - accuracy: 1.0000\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0578 - accuracy: 1.0000\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 0s 39us/step - loss: 0.0577 - accuracy: 1.0000\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0575 - accuracy: 1.0000\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0574 - accuracy: 1.0000\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0573 - accuracy: 1.0000\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0572 - accuracy: 1.0000\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0571 - accuracy: 1.0000\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0570 - accuracy: 1.0000\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0569 - accuracy: 1.0000\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0568 - accuracy: 1.0000\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0567 - accuracy: 1.0000\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0566 - accuracy: 1.0000\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0565 - accuracy: 1.0000\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 0s 36us/step - loss: 0.0563 - accuracy: 1.0000\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0562 - accuracy: 1.0000\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 0s 35us/step - loss: 0.0561 - accuracy: 1.0000\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0560 - accuracy: 1.0000\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0559 - accuracy: 1.0000\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0558 - accuracy: 1.0000\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0557 - accuracy: 1.0000\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0556 - accuracy: 1.0000\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0555 - accuracy: 1.0000\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0554 - accuracy: 1.0000\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0553 - accuracy: 1.0000\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 0s 29us/step - loss: 0.0552 - accuracy: 1.0000\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0551 - accuracy: 1.0000\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 0s 24us/step - loss: 0.0550 - accuracy: 1.0000\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0549 - accuracy: 1.0000\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0548 - accuracy: 1.0000\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 0s 37us/step - loss: 0.0547 - accuracy: 1.0000\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0546 - accuracy: 1.0000\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0545 - accuracy: 1.0000\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 0s 27us/step - loss: 0.0544 - accuracy: 1.0000\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0544 - accuracy: 1.0000\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0543 - accuracy: 1.0000\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0542 - accuracy: 1.0000\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0541 - accuracy: 1.0000\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0540 - accuracy: 1.0000\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0540 - accuracy: 1.0000\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0539 - accuracy: 1.0000\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0538 - accuracy: 1.0000\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0536 - accuracy: 1.0000\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 0s 34us/step - loss: 0.0535 - accuracy: 1.0000\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0534 - accuracy: 1.0000\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0534 - accuracy: 1.0000\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0533 - accuracy: 1.0000\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0532 - accuracy: 1.0000\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 0s 30us/step - loss: 0.0531 - accuracy: 1.0000\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0530 - accuracy: 1.0000\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0529 - accuracy: 1.0000\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0528 - accuracy: 1.0000\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0527 - accuracy: 1.0000\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0526 - accuracy: 1.0000\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0526 - accuracy: 1.0000\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0525 - accuracy: 1.0000\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0524 - accuracy: 1.0000\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0523 - accuracy: 1.0000\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0522 - accuracy: 1.0000\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0521 - accuracy: 1.0000\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0521 - accuracy: 1.0000\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0520 - accuracy: 1.0000\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 0s 31us/step - loss: 0.0519 - accuracy: 1.0000\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 0s 33us/step - loss: 0.0518 - accuracy: 1.0000\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 0s 32us/step - loss: 0.0517 - accuracy: 1.0000\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 0s 26us/step - loss: 0.0516 - accuracy: 1.0000\n",
            "30/30 [==============================] - 0s 964us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "10c504e8-d914-46fb-e2a5-1dab5056fe5f",
        "id": "3ZA9uHPGFl4F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3cdec042-f53c-484c-ba85-1d807067ed82",
        "id": "NqEhhZesFl4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8666666746139526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dzxDs6lEFl4U"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}