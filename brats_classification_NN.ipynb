{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brats_classification_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNI7tmHgrzH/DwJ1YHCyJBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/cmepda/blob/master/brats_classification_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaXxnQhQFHOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d0154519-83ce-469a-ce6a-82e519382cdc"
      },
      "source": [
        "pip install -U keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnnoNlXoCsZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkyGJ1ldXJ8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cnDB8p7Cs7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "#import tensorflow as tf\n",
        "#tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "#from keras import backend as K\n",
        "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "#K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "71798f0d-0450-476a-ca9e-25be77272fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/gdrive/My Drive/BRATS/data_without_NAN_with_histologies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znKJzW7bsbx",
        "colab_type": "code",
        "outputId": "c7d133d9-22d6-485d-86b7-d26399591036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "df_data"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>Date</th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_ED_FLAIR_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1Gd_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>Histology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>TCGA-02-0006</td>\n",
              "      <td>1996.08.23</td>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.045826</td>\n",
              "      <td>0.010588</td>\n",
              "      <td>0.812320</td>\n",
              "      <td>0.187680</td>\n",
              "      <td>17.726300</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86315</td>\n",
              "      <td>1479.9762</td>\n",
              "      <td>1.10870</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.40937</td>\n",
              "      <td>1.47070</td>\n",
              "      <td>2992.2698</td>\n",
              "      <td>0.71642</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.28977</td>\n",
              "      <td>1.8815</td>\n",
              "      <td>1872.0528</td>\n",
              "      <td>0.75986</td>\n",
              "      <td>0.026040</td>\n",
              "      <td>0.37869</td>\n",
              "      <td>0.060929</td>\n",
              "      <td>1675.0041</td>\n",
              "      <td>14.11380</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>0.41942</td>\n",
              "      <td>0.026740</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.31290</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.32930</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>TCGA-02-0009</td>\n",
              "      <td>1997.06.14</td>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.500750</td>\n",
              "      <td>0.499250</td>\n",
              "      <td>1.805000</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>...</td>\n",
              "      <td>0.40004</td>\n",
              "      <td>2378.9184</td>\n",
              "      <td>2.54730</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.70926</td>\n",
              "      <td>0.78063</td>\n",
              "      <td>5719.2847</td>\n",
              "      <td>1.29980</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.48919</td>\n",
              "      <td>1.8243</td>\n",
              "      <td>2954.8148</td>\n",
              "      <td>0.77199</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.29324</td>\n",
              "      <td>1.223600</td>\n",
              "      <td>539.3057</td>\n",
              "      <td>0.53125</td>\n",
              "      <td>0.005712</td>\n",
              "      <td>0.20995</td>\n",
              "      <td>0.315580</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.74440</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.59990</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.47360</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>TCGA-02-0011</td>\n",
              "      <td>1998.02.01</td>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.061444</td>\n",
              "      <td>0.407290</td>\n",
              "      <td>0.592710</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>...</td>\n",
              "      <td>1.51780</td>\n",
              "      <td>1750.3404</td>\n",
              "      <td>0.56482</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.59301</td>\n",
              "      <td>1.81810</td>\n",
              "      <td>4990.3388</td>\n",
              "      <td>0.54747</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.59184</td>\n",
              "      <td>2.4243</td>\n",
              "      <td>4703.9458</td>\n",
              "      <td>0.41937</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.37863</td>\n",
              "      <td>1.957500</td>\n",
              "      <td>2509.3979</td>\n",
              "      <td>0.42842</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.19849</td>\n",
              "      <td>1.395800</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.74730</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.80480</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.42900</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>TCGA-02-0027</td>\n",
              "      <td>1999.03.28</td>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.614890</td>\n",
              "      <td>0.385110</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78104</td>\n",
              "      <td>1870.7630</td>\n",
              "      <td>1.37070</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.65247</td>\n",
              "      <td>1.46450</td>\n",
              "      <td>5625.0240</td>\n",
              "      <td>0.66930</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.66446</td>\n",
              "      <td>1.5863</td>\n",
              "      <td>5585.3565</td>\n",
              "      <td>0.60995</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.89121</td>\n",
              "      <td>0.485160</td>\n",
              "      <td>7372.7070</td>\n",
              "      <td>2.03510</td>\n",
              "      <td>0.005390</td>\n",
              "      <td>0.23036</td>\n",
              "      <td>0.143560</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.94490</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.73360</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.46800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>TCGA-02-0033</td>\n",
              "      <td>1997.05.26</td>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.828750</td>\n",
              "      <td>0.171250</td>\n",
              "      <td>1.575400</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>...</td>\n",
              "      <td>1.80660</td>\n",
              "      <td>1959.4667</td>\n",
              "      <td>0.56070</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.48428</td>\n",
              "      <td>2.18490</td>\n",
              "      <td>4083.7014</td>\n",
              "      <td>0.46492</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.40305</td>\n",
              "      <td>1.8266</td>\n",
              "      <td>3592.2992</td>\n",
              "      <td>0.56135</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.42666</td>\n",
              "      <td>0.950220</td>\n",
              "      <td>2072.5900</td>\n",
              "      <td>1.17490</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.14562</td>\n",
              "      <td>0.713820</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.14360</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.45320</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.77800</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>141</td>\n",
              "      <td>TCGA-HT-7694</td>\n",
              "      <td>1995.04.04</td>\n",
              "      <td>1036</td>\n",
              "      <td>189152</td>\n",
              "      <td>171595</td>\n",
              "      <td>190188</td>\n",
              "      <td>361783</td>\n",
              "      <td>1611350</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.006037</td>\n",
              "      <td>1.102317</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.994550</td>\n",
              "      <td>0.902240</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.522833</td>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.117387</td>\n",
              "      <td>0.106490</td>\n",
              "      <td>0.118030</td>\n",
              "      <td>0.224522</td>\n",
              "      <td>1.5561</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>130.5401</td>\n",
              "      <td>10.8604</td>\n",
              "      <td>158.2426</td>\n",
              "      <td>5.1363</td>\n",
              "      <td>160.5840</td>\n",
              "      <td>13.3742</td>\n",
              "      <td>196.0449</td>\n",
              "      <td>12.1558</td>\n",
              "      <td>85.7372</td>\n",
              "      <td>14.1637</td>\n",
              "      <td>135.7749</td>\n",
              "      <td>12.9578</td>\n",
              "      <td>172.2660</td>\n",
              "      <td>25.9874</td>\n",
              "      <td>...</td>\n",
              "      <td>3.89200</td>\n",
              "      <td>1050.8760</td>\n",
              "      <td>0.26584</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.28803</td>\n",
              "      <td>3.76680</td>\n",
              "      <td>2246.2262</td>\n",
              "      <td>0.26343</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.32326</td>\n",
              "      <td>3.7144</td>\n",
              "      <td>2862.7663</td>\n",
              "      <td>0.26864</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.39033</td>\n",
              "      <td>4.843700</td>\n",
              "      <td>3149.1624</td>\n",
              "      <td>0.20185</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.17338</td>\n",
              "      <td>4.129200</td>\n",
              "      <td>1181.3019</td>\n",
              "      <td>0.23864</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.33542</td>\n",
              "      <td>4.444300</td>\n",
              "      <td>2706.6360</td>\n",
              "      <td>0.22259</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.25558</td>\n",
              "      <td>3.698700</td>\n",
              "      <td>2033.8540</td>\n",
              "      <td>0.26785</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.070503</td>\n",
              "      <td>0.090456</td>\n",
              "      <td>719.23800</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>142</td>\n",
              "      <td>TCGA-HT-8018</td>\n",
              "      <td>1997.04.11</td>\n",
              "      <td>2093</td>\n",
              "      <td>8685</td>\n",
              "      <td>39142</td>\n",
              "      <td>10778</td>\n",
              "      <td>49920</td>\n",
              "      <td>1493262</td>\n",
              "      <td>0.240990</td>\n",
              "      <td>0.053472</td>\n",
              "      <td>0.221884</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.805810</td>\n",
              "      <td>3.631700</td>\n",
              "      <td>0.041927</td>\n",
              "      <td>0.173978</td>\n",
              "      <td>0.784095</td>\n",
              "      <td>0.215905</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>0.005816</td>\n",
              "      <td>0.026212</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.033430</td>\n",
              "      <td>7.8703</td>\n",
              "      <td>1.2296</td>\n",
              "      <td>122.5820</td>\n",
              "      <td>24.4042</td>\n",
              "      <td>90.7803</td>\n",
              "      <td>9.1876</td>\n",
              "      <td>189.3704</td>\n",
              "      <td>11.4401</td>\n",
              "      <td>176.2758</td>\n",
              "      <td>14.7584</td>\n",
              "      <td>81.0780</td>\n",
              "      <td>10.4078</td>\n",
              "      <td>88.8951</td>\n",
              "      <td>9.1065</td>\n",
              "      <td>189.3633</td>\n",
              "      <td>14.4565</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56593</td>\n",
              "      <td>1255.6524</td>\n",
              "      <td>1.74930</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.48939</td>\n",
              "      <td>1.56420</td>\n",
              "      <td>3817.4564</td>\n",
              "      <td>0.62083</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.38268</td>\n",
              "      <td>1.2343</td>\n",
              "      <td>3032.0641</td>\n",
              "      <td>0.77990</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.37981</td>\n",
              "      <td>0.402750</td>\n",
              "      <td>2605.8492</td>\n",
              "      <td>2.57200</td>\n",
              "      <td>0.004937</td>\n",
              "      <td>0.14295</td>\n",
              "      <td>0.201910</td>\n",
              "      <td>882.1737</td>\n",
              "      <td>4.27000</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.37387</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>2336.3329</td>\n",
              "      <td>2.22420</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.200950</td>\n",
              "      <td>1446.4163</td>\n",
              "      <td>3.99730</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168857</td>\n",
              "      <td>0.120586</td>\n",
              "      <td>0.054307</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>143</td>\n",
              "      <td>TCGA-HT-8111</td>\n",
              "      <td>1998.03.30</td>\n",
              "      <td>1929</td>\n",
              "      <td>437</td>\n",
              "      <td>54079</td>\n",
              "      <td>2366</td>\n",
              "      <td>56445</td>\n",
              "      <td>1821157</td>\n",
              "      <td>4.414188</td>\n",
              "      <td>0.035670</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.815300</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>22.856700</td>\n",
              "      <td>0.034175</td>\n",
              "      <td>0.007742</td>\n",
              "      <td>0.958083</td>\n",
              "      <td>0.041917</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.029695</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.030994</td>\n",
              "      <td>19.5113</td>\n",
              "      <td>2.7359</td>\n",
              "      <td>114.8266</td>\n",
              "      <td>16.4708</td>\n",
              "      <td>88.3256</td>\n",
              "      <td>5.7475</td>\n",
              "      <td>135.0452</td>\n",
              "      <td>10.8131</td>\n",
              "      <td>153.4996</td>\n",
              "      <td>7.2622</td>\n",
              "      <td>84.3018</td>\n",
              "      <td>8.0198</td>\n",
              "      <td>88.9795</td>\n",
              "      <td>5.3935</td>\n",
              "      <td>131.7430</td>\n",
              "      <td>11.2399</td>\n",
              "      <td>...</td>\n",
              "      <td>0.80255</td>\n",
              "      <td>863.0606</td>\n",
              "      <td>1.39180</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.34568</td>\n",
              "      <td>1.24340</td>\n",
              "      <td>2832.2946</td>\n",
              "      <td>0.78981</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.32099</td>\n",
              "      <td>1.6823</td>\n",
              "      <td>2470.0227</td>\n",
              "      <td>0.55317</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>0.86464</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>5330.9937</td>\n",
              "      <td>14.26100</td>\n",
              "      <td>0.053508</td>\n",
              "      <td>0.17277</td>\n",
              "      <td>0.029481</td>\n",
              "      <td>879.6829</td>\n",
              "      <td>34.79070</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.039567</td>\n",
              "      <td>1317.6443</td>\n",
              "      <td>22.83400</td>\n",
              "      <td>0.052586</td>\n",
              "      <td>0.20996</td>\n",
              "      <td>0.031829</td>\n",
              "      <td>803.8863</td>\n",
              "      <td>27.48750</td>\n",
              "      <td>1.96875</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.148932</td>\n",
              "      <td>0.073453</td>\n",
              "      <td>0.126712</td>\n",
              "      <td>7.06744</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>144</td>\n",
              "      <td>TCGA-HT-8114</td>\n",
              "      <td>1998.10.30</td>\n",
              "      <td>8755</td>\n",
              "      <td>168606</td>\n",
              "      <td>11325</td>\n",
              "      <td>177361</td>\n",
              "      <td>188686</td>\n",
              "      <td>1693971</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.773068</td>\n",
              "      <td>14.887947</td>\n",
              "      <td>0.049363</td>\n",
              "      <td>0.950640</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.046400</td>\n",
              "      <td>0.893580</td>\n",
              "      <td>0.060020</td>\n",
              "      <td>0.939980</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>0.099533</td>\n",
              "      <td>0.006686</td>\n",
              "      <td>0.104700</td>\n",
              "      <td>0.111387</td>\n",
              "      <td>2.2261</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>92.3248</td>\n",
              "      <td>10.9722</td>\n",
              "      <td>96.4461</td>\n",
              "      <td>7.0449</td>\n",
              "      <td>120.4493</td>\n",
              "      <td>18.3507</td>\n",
              "      <td>168.2873</td>\n",
              "      <td>13.7084</td>\n",
              "      <td>76.0316</td>\n",
              "      <td>15.3670</td>\n",
              "      <td>98.1388</td>\n",
              "      <td>11.9586</td>\n",
              "      <td>127.2041</td>\n",
              "      <td>26.8906</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31348</td>\n",
              "      <td>1119.2382</td>\n",
              "      <td>2.66250</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.68191</td>\n",
              "      <td>0.60512</td>\n",
              "      <td>5246.9633</td>\n",
              "      <td>1.69490</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>1.15310</td>\n",
              "      <td>3.3277</td>\n",
              "      <td>6027.3574</td>\n",
              "      <td>0.55024</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.37937</td>\n",
              "      <td>4.644300</td>\n",
              "      <td>2996.8473</td>\n",
              "      <td>0.21714</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.15073</td>\n",
              "      <td>3.012000</td>\n",
              "      <td>1054.1171</td>\n",
              "      <td>0.36431</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.30578</td>\n",
              "      <td>3.346700</td>\n",
              "      <td>2515.2461</td>\n",
              "      <td>0.28794</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.25687</td>\n",
              "      <td>2.991600</td>\n",
              "      <td>2055.4227</td>\n",
              "      <td>0.30710</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.168182</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.107433</td>\n",
              "      <td>15.52240</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>145</td>\n",
              "      <td>TCGA-HT-8563</td>\n",
              "      <td>1998.12.09</td>\n",
              "      <td>11757</td>\n",
              "      <td>1012</td>\n",
              "      <td>138755</td>\n",
              "      <td>12769</td>\n",
              "      <td>151524</td>\n",
              "      <td>1605161</td>\n",
              "      <td>11.617589</td>\n",
              "      <td>0.084732</td>\n",
              "      <td>0.007293</td>\n",
              "      <td>0.920750</td>\n",
              "      <td>0.079254</td>\n",
              "      <td>10.866600</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.915730</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>0.007324</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.086443</td>\n",
              "      <td>0.007955</td>\n",
              "      <td>0.094398</td>\n",
              "      <td>6.3847</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>154.6832</td>\n",
              "      <td>49.8662</td>\n",
              "      <td>103.6185</td>\n",
              "      <td>5.3827</td>\n",
              "      <td>108.7191</td>\n",
              "      <td>12.4944</td>\n",
              "      <td>168.1385</td>\n",
              "      <td>15.0086</td>\n",
              "      <td>87.1151</td>\n",
              "      <td>9.9561</td>\n",
              "      <td>98.4603</td>\n",
              "      <td>3.5746</td>\n",
              "      <td>112.2253</td>\n",
              "      <td>7.8119</td>\n",
              "      <td>...</td>\n",
              "      <td>3.98400</td>\n",
              "      <td>724.9046</td>\n",
              "      <td>0.26198</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.37976</td>\n",
              "      <td>3.41390</td>\n",
              "      <td>3293.8152</td>\n",
              "      <td>0.28105</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.29310</td>\n",
              "      <td>2.6220</td>\n",
              "      <td>2582.0410</td>\n",
              "      <td>0.36389</td>\n",
              "      <td>0.007180</td>\n",
              "      <td>1.27720</td>\n",
              "      <td>0.102260</td>\n",
              "      <td>10178.0572</td>\n",
              "      <td>9.39250</td>\n",
              "      <td>0.015050</td>\n",
              "      <td>0.23963</td>\n",
              "      <td>0.220530</td>\n",
              "      <td>731.4574</td>\n",
              "      <td>5.35820</td>\n",
              "      <td>0.015620</td>\n",
              "      <td>0.40833</td>\n",
              "      <td>0.076820</td>\n",
              "      <td>2324.7276</td>\n",
              "      <td>12.31230</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.21704</td>\n",
              "      <td>0.065338</td>\n",
              "      <td>1056.9519</td>\n",
              "      <td>20.27440</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.213120e-07</td>\n",
              "      <td>0.072868</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.069101</td>\n",
              "      <td>7.62280</td>\n",
              "      <td>LGG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 708 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0            ID        Date  ...  TGM_Cog_Z_1    TGM_T_1  Histology\n",
              "0             0  TCGA-02-0006  1996.08.23  ...     0.052741    2.00000        GBM\n",
              "1             1  TCGA-02-0009  1997.06.14  ...     0.094336   91.47360        GBM\n",
              "2             2  TCGA-02-0011  1998.02.01  ...     0.096035  272.42900        GBM\n",
              "3             3  TCGA-02-0027  1999.03.28  ...     0.096470  128.46800        GBM\n",
              "4             4  TCGA-02-0033  1997.05.26  ...     0.096894  240.77800        GBM\n",
              "..          ...           ...         ...  ...          ...        ...        ...\n",
              "141         141  TCGA-HT-7694  1995.04.04  ...     0.090456  719.23800        LGG\n",
              "142         142  TCGA-HT-8018  1997.04.11  ...     0.054307    2.00000        LGG\n",
              "143         143  TCGA-HT-8111  1998.03.30  ...     0.126712    7.06744        LGG\n",
              "144         144  TCGA-HT-8114  1998.10.30  ...     0.107433   15.52240        LGG\n",
              "145         145  TCGA-HT-8563  1998.12.09  ...     0.069101    7.62280        LGG\n",
              "\n",
              "[146 rows x 708 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZviWnrbyAT",
        "colab_type": "code",
        "outputId": "f6858482-4616-424b-81e6-0ab061e58d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df_data.columns"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'ID', 'Date', 'VOLUME_ET', 'VOLUME_NET', 'VOLUME_ED',\n",
              "       'VOLUME_TC', 'VOLUME_WT', 'VOLUME_BRAIN', 'VOLUME_ET_OVER_NET',\n",
              "       ...\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Busyness',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Complexity',\n",
              "       'TEXTURE_NGTDM_NET_FLAIR_Strength', 'TGM_p1', 'TGM_dw', 'TGM_Cog_X_1',\n",
              "       'TGM_Cog_Y_1', 'TGM_Cog_Z_1', 'TGM_T_1', 'Histology'],\n",
              "      dtype='object', length=708)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data.drop(['Histology', 'Unnamed: 0', 'ID', 'Date'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df_data.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAsUzr5DnKrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)\n",
        "encoded_label = encoder.transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEYe7l7cnQ9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "425db36e-0295-4b62-97bf-a8818273e35c"
      },
      "source": [
        "encoded_label"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BoXqyfbpydt",
        "colab_type": "text"
      },
      "source": [
        "#NO K-FOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(data, encoded_label, test_size=0.3, stratify=encoded_label, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=0.2, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6iyOqcBq0RC"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "fbe7694f-b904-4b21-bdbd-7f93d6d732c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'GBM':0, 'LGG':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sxf8wiXmwgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhIbj5YiEbTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qxD411MFVSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(704,), kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "  #model.add(layers.Dropout(0.8))\n",
        "  #model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.005, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  rmsprop = RMSprop(lr=0.01)\n",
        "\n",
        "  model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d0b00e8b-32f9-47a4-eed7-c42bb7728223",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-a6a5a407654f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mone_hot_val_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'one_hot_val_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7245e32c-25ba-485d-f976-57e3134bf072",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, y_train, validation_data=(val_data_stand, y_val), \n",
        "                      epochs= num_epochs, batch_size=81)\n",
        "  \n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_accuracy']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81 samples, validate on 21 samples\n",
            "Epoch 1/1000\n",
            "81/81 [==============================] - 0s 784us/step - loss: 1.3207 - accuracy: 0.3086 - val_loss: 1.2413 - val_accuracy: 0.2857\n",
            "Epoch 2/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 1.1796 - accuracy: 0.3210 - val_loss: 1.0293 - val_accuracy: 0.2857\n",
            "Epoch 3/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.9922 - accuracy: 0.3333 - val_loss: 0.8458 - val_accuracy: 0.5714\n",
            "Epoch 4/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.8529 - accuracy: 0.6420 - val_loss: 0.7593 - val_accuracy: 0.7619\n",
            "Epoch 5/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.8027 - accuracy: 0.6914 - val_loss: 0.7442 - val_accuracy: 0.7143\n",
            "Epoch 6/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.8164 - accuracy: 0.6914 - val_loss: 0.7722 - val_accuracy: 0.7143\n",
            "Epoch 7/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.8467 - accuracy: 0.6914 - val_loss: 0.7930 - val_accuracy: 0.7143\n",
            "Epoch 8/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.8667 - accuracy: 0.6914 - val_loss: 0.8043 - val_accuracy: 0.7143\n",
            "Epoch 9/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.8744 - accuracy: 0.6914 - val_loss: 0.8071 - val_accuracy: 0.7143\n",
            "Epoch 10/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.8717 - accuracy: 0.6914 - val_loss: 0.8015 - val_accuracy: 0.7143\n",
            "Epoch 11/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.8594 - accuracy: 0.6914 - val_loss: 0.7894 - val_accuracy: 0.7143\n",
            "Epoch 12/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.8397 - accuracy: 0.6914 - val_loss: 0.7736 - val_accuracy: 0.7143\n",
            "Epoch 13/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.8161 - accuracy: 0.6914 - val_loss: 0.7577 - val_accuracy: 0.7143\n",
            "Epoch 14/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.7921 - accuracy: 0.6914 - val_loss: 0.7444 - val_accuracy: 0.7143\n",
            "Epoch 15/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.7710 - accuracy: 0.6914 - val_loss: 0.7358 - val_accuracy: 0.7143\n",
            "Epoch 16/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.7549 - accuracy: 0.6914 - val_loss: 0.7322 - val_accuracy: 0.7143\n",
            "Epoch 17/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.7451 - accuracy: 0.6914 - val_loss: 0.7323 - val_accuracy: 0.7619\n",
            "Epoch 18/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.7411 - accuracy: 0.6914 - val_loss: 0.7348 - val_accuracy: 0.7619\n",
            "Epoch 19/1000\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.7405 - accuracy: 0.6914 - val_loss: 0.7369 - val_accuracy: 0.7619\n",
            "Epoch 20/1000\n",
            "81/81 [==============================] - 0s 41us/step - loss: 0.7428 - accuracy: 0.6914 - val_loss: 0.7359 - val_accuracy: 0.7619\n",
            "Epoch 21/1000\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.7439 - accuracy: 0.6914 - val_loss: 0.7310 - val_accuracy: 0.7619\n",
            "Epoch 22/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.7400 - accuracy: 0.7160 - val_loss: 0.7203 - val_accuracy: 0.7619\n",
            "Epoch 23/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.7306 - accuracy: 0.7160 - val_loss: 0.7074 - val_accuracy: 0.7619\n",
            "Epoch 24/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.7178 - accuracy: 0.7037 - val_loss: 0.6947 - val_accuracy: 0.7619\n",
            "Epoch 25/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.7050 - accuracy: 0.6914 - val_loss: 0.6836 - val_accuracy: 0.7619\n",
            "Epoch 26/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.6946 - accuracy: 0.6914 - val_loss: 0.6753 - val_accuracy: 0.7619\n",
            "Epoch 27/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.6881 - accuracy: 0.6914 - val_loss: 0.6694 - val_accuracy: 0.7619\n",
            "Epoch 28/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.6841 - accuracy: 0.6914 - val_loss: 0.6651 - val_accuracy: 0.7619\n",
            "Epoch 29/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.6813 - accuracy: 0.6914 - val_loss: 0.6614 - val_accuracy: 0.7619\n",
            "Epoch 30/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.6786 - accuracy: 0.6914 - val_loss: 0.6577 - val_accuracy: 0.7619\n",
            "Epoch 31/1000\n",
            "81/81 [==============================] - 0s 41us/step - loss: 0.6753 - accuracy: 0.6914 - val_loss: 0.6535 - val_accuracy: 0.7619\n",
            "Epoch 32/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.6710 - accuracy: 0.6914 - val_loss: 0.6485 - val_accuracy: 0.7619\n",
            "Epoch 33/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.6655 - accuracy: 0.6914 - val_loss: 0.6430 - val_accuracy: 0.7619\n",
            "Epoch 34/1000\n",
            "81/81 [==============================] - 0s 46us/step - loss: 0.6589 - accuracy: 0.7037 - val_loss: 0.6371 - val_accuracy: 0.7619\n",
            "Epoch 35/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.6517 - accuracy: 0.7037 - val_loss: 0.6312 - val_accuracy: 0.7619\n",
            "Epoch 36/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.6457 - accuracy: 0.7160 - val_loss: 0.6268 - val_accuracy: 0.7619\n",
            "Epoch 37/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.6413 - accuracy: 0.7160 - val_loss: 0.6236 - val_accuracy: 0.7619\n",
            "Epoch 38/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.6385 - accuracy: 0.7160 - val_loss: 0.6205 - val_accuracy: 0.7619\n",
            "Epoch 39/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.6357 - accuracy: 0.7284 - val_loss: 0.6174 - val_accuracy: 0.7619\n",
            "Epoch 40/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.6328 - accuracy: 0.7407 - val_loss: 0.6143 - val_accuracy: 0.7619\n",
            "Epoch 41/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.6298 - accuracy: 0.7778 - val_loss: 0.6112 - val_accuracy: 0.7619\n",
            "Epoch 42/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.6265 - accuracy: 0.7778 - val_loss: 0.6080 - val_accuracy: 0.7619\n",
            "Epoch 43/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.6227 - accuracy: 0.7901 - val_loss: 0.6052 - val_accuracy: 0.7619\n",
            "Epoch 44/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.6186 - accuracy: 0.7901 - val_loss: 0.6028 - val_accuracy: 0.7619\n",
            "Epoch 45/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.6148 - accuracy: 0.8025 - val_loss: 0.6008 - val_accuracy: 0.7619\n",
            "Epoch 46/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.6116 - accuracy: 0.7778 - val_loss: 0.5992 - val_accuracy: 0.7619\n",
            "Epoch 47/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.6088 - accuracy: 0.7778 - val_loss: 0.5977 - val_accuracy: 0.7619\n",
            "Epoch 48/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.6061 - accuracy: 0.7654 - val_loss: 0.5962 - val_accuracy: 0.7619\n",
            "Epoch 49/1000\n",
            "81/81 [==============================] - 0s 42us/step - loss: 0.6036 - accuracy: 0.7901 - val_loss: 0.5945 - val_accuracy: 0.7619\n",
            "Epoch 50/1000\n",
            "81/81 [==============================] - 0s 39us/step - loss: 0.6010 - accuracy: 0.7901 - val_loss: 0.5926 - val_accuracy: 0.7619\n",
            "Epoch 51/1000\n",
            "81/81 [==============================] - 0s 101us/step - loss: 0.5983 - accuracy: 0.8148 - val_loss: 0.5904 - val_accuracy: 0.7619\n",
            "Epoch 52/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.5954 - accuracy: 0.8148 - val_loss: 0.5878 - val_accuracy: 0.7619\n",
            "Epoch 53/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.5924 - accuracy: 0.8272 - val_loss: 0.5849 - val_accuracy: 0.7619\n",
            "Epoch 54/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.5897 - accuracy: 0.8272 - val_loss: 0.5821 - val_accuracy: 0.7619\n",
            "Epoch 55/1000\n",
            "81/81 [==============================] - 0s 80us/step - loss: 0.5872 - accuracy: 0.8272 - val_loss: 0.5794 - val_accuracy: 0.7619\n",
            "Epoch 56/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.5848 - accuracy: 0.8272 - val_loss: 0.5769 - val_accuracy: 0.8095\n",
            "Epoch 57/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.5823 - accuracy: 0.8395 - val_loss: 0.5745 - val_accuracy: 0.8095\n",
            "Epoch 58/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.5798 - accuracy: 0.8395 - val_loss: 0.5722 - val_accuracy: 0.8095\n",
            "Epoch 59/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.5773 - accuracy: 0.8395 - val_loss: 0.5700 - val_accuracy: 0.8095\n",
            "Epoch 60/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.5748 - accuracy: 0.8519 - val_loss: 0.5681 - val_accuracy: 0.8095\n",
            "Epoch 61/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.5723 - accuracy: 0.8519 - val_loss: 0.5663 - val_accuracy: 0.8095\n",
            "Epoch 62/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.5698 - accuracy: 0.8642 - val_loss: 0.5646 - val_accuracy: 0.8571\n",
            "Epoch 63/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.5672 - accuracy: 0.8642 - val_loss: 0.5629 - val_accuracy: 0.8571\n",
            "Epoch 64/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.5646 - accuracy: 0.8642 - val_loss: 0.5612 - val_accuracy: 0.8571\n",
            "Epoch 65/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.5620 - accuracy: 0.8642 - val_loss: 0.5595 - val_accuracy: 0.8571\n",
            "Epoch 66/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.5594 - accuracy: 0.8642 - val_loss: 0.5578 - val_accuracy: 0.8571\n",
            "Epoch 67/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.5568 - accuracy: 0.8889 - val_loss: 0.5562 - val_accuracy: 0.8571\n",
            "Epoch 68/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.5541 - accuracy: 0.8889 - val_loss: 0.5545 - val_accuracy: 0.8571\n",
            "Epoch 69/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.5515 - accuracy: 0.8889 - val_loss: 0.5528 - val_accuracy: 0.8571\n",
            "Epoch 70/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.5488 - accuracy: 0.9012 - val_loss: 0.5511 - val_accuracy: 0.8571\n",
            "Epoch 71/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.5461 - accuracy: 0.9012 - val_loss: 0.5493 - val_accuracy: 0.8571\n",
            "Epoch 72/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.5435 - accuracy: 0.9012 - val_loss: 0.5476 - val_accuracy: 0.8571\n",
            "Epoch 73/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.5408 - accuracy: 0.9012 - val_loss: 0.5460 - val_accuracy: 0.8571\n",
            "Epoch 74/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.5381 - accuracy: 0.9012 - val_loss: 0.5444 - val_accuracy: 0.8571\n",
            "Epoch 75/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.5353 - accuracy: 0.9012 - val_loss: 0.5429 - val_accuracy: 0.8571\n",
            "Epoch 76/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.5325 - accuracy: 0.9012 - val_loss: 0.5414 - val_accuracy: 0.8571\n",
            "Epoch 77/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.5297 - accuracy: 0.9012 - val_loss: 0.5398 - val_accuracy: 0.8571\n",
            "Epoch 78/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.5269 - accuracy: 0.9012 - val_loss: 0.5381 - val_accuracy: 0.8571\n",
            "Epoch 79/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.5241 - accuracy: 0.9012 - val_loss: 0.5364 - val_accuracy: 0.8571\n",
            "Epoch 80/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.5212 - accuracy: 0.9012 - val_loss: 0.5346 - val_accuracy: 0.8571\n",
            "Epoch 81/1000\n",
            "81/81 [==============================] - 0s 86us/step - loss: 0.5183 - accuracy: 0.9012 - val_loss: 0.5328 - val_accuracy: 0.8571\n",
            "Epoch 82/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.5153 - accuracy: 0.9012 - val_loss: 0.5311 - val_accuracy: 0.8571\n",
            "Epoch 83/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.5124 - accuracy: 0.9012 - val_loss: 0.5295 - val_accuracy: 0.8571\n",
            "Epoch 84/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.5094 - accuracy: 0.9136 - val_loss: 0.5278 - val_accuracy: 0.8571\n",
            "Epoch 85/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.5063 - accuracy: 0.9136 - val_loss: 0.5260 - val_accuracy: 0.8571\n",
            "Epoch 86/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.5033 - accuracy: 0.9136 - val_loss: 0.5243 - val_accuracy: 0.8571\n",
            "Epoch 87/1000\n",
            "81/81 [==============================] - 0s 93us/step - loss: 0.5002 - accuracy: 0.9136 - val_loss: 0.5226 - val_accuracy: 0.8571\n",
            "Epoch 88/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.4971 - accuracy: 0.9136 - val_loss: 0.5209 - val_accuracy: 0.8571\n",
            "Epoch 89/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.4940 - accuracy: 0.9136 - val_loss: 0.5193 - val_accuracy: 0.8571\n",
            "Epoch 90/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.4909 - accuracy: 0.9136 - val_loss: 0.5177 - val_accuracy: 0.8571\n",
            "Epoch 91/1000\n",
            "81/81 [==============================] - 0s 114us/step - loss: 0.4878 - accuracy: 0.9136 - val_loss: 0.5162 - val_accuracy: 0.8571\n",
            "Epoch 92/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.4846 - accuracy: 0.9136 - val_loss: 0.5148 - val_accuracy: 0.8571\n",
            "Epoch 93/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.4814 - accuracy: 0.9136 - val_loss: 0.5134 - val_accuracy: 0.8571\n",
            "Epoch 94/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.4782 - accuracy: 0.9136 - val_loss: 0.5121 - val_accuracy: 0.8571\n",
            "Epoch 95/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.4750 - accuracy: 0.9259 - val_loss: 0.5108 - val_accuracy: 0.8571\n",
            "Epoch 96/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.4718 - accuracy: 0.9259 - val_loss: 0.5096 - val_accuracy: 0.8571\n",
            "Epoch 97/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.4686 - accuracy: 0.9259 - val_loss: 0.5085 - val_accuracy: 0.8571\n",
            "Epoch 98/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.4654 - accuracy: 0.9259 - val_loss: 0.5074 - val_accuracy: 0.8571\n",
            "Epoch 99/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.4622 - accuracy: 0.9259 - val_loss: 0.5063 - val_accuracy: 0.8571\n",
            "Epoch 100/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.4589 - accuracy: 0.9259 - val_loss: 0.5052 - val_accuracy: 0.8571\n",
            "Epoch 101/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.4557 - accuracy: 0.9259 - val_loss: 0.5041 - val_accuracy: 0.8571\n",
            "Epoch 102/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.4526 - accuracy: 0.9259 - val_loss: 0.5030 - val_accuracy: 0.8571\n",
            "Epoch 103/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.4495 - accuracy: 0.9259 - val_loss: 0.5017 - val_accuracy: 0.8571\n",
            "Epoch 104/1000\n",
            "81/81 [==============================] - 0s 42us/step - loss: 0.4464 - accuracy: 0.9259 - val_loss: 0.5004 - val_accuracy: 0.8571\n",
            "Epoch 105/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.4434 - accuracy: 0.9259 - val_loss: 0.4990 - val_accuracy: 0.9048\n",
            "Epoch 106/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.4404 - accuracy: 0.9259 - val_loss: 0.4977 - val_accuracy: 0.9048\n",
            "Epoch 107/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.4374 - accuracy: 0.9259 - val_loss: 0.4964 - val_accuracy: 0.9048\n",
            "Epoch 108/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.4345 - accuracy: 0.9259 - val_loss: 0.4951 - val_accuracy: 0.9048\n",
            "Epoch 109/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.4316 - accuracy: 0.9259 - val_loss: 0.4940 - val_accuracy: 0.9048\n",
            "Epoch 110/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.4287 - accuracy: 0.9259 - val_loss: 0.4929 - val_accuracy: 0.9048\n",
            "Epoch 111/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.4259 - accuracy: 0.9259 - val_loss: 0.4920 - val_accuracy: 0.9048\n",
            "Epoch 112/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.4231 - accuracy: 0.9259 - val_loss: 0.4912 - val_accuracy: 0.9524\n",
            "Epoch 113/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.4204 - accuracy: 0.9259 - val_loss: 0.4904 - val_accuracy: 0.9524\n",
            "Epoch 114/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.4176 - accuracy: 0.9259 - val_loss: 0.4898 - val_accuracy: 0.9524\n",
            "Epoch 115/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.4150 - accuracy: 0.9383 - val_loss: 0.4892 - val_accuracy: 0.9524\n",
            "Epoch 116/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.4124 - accuracy: 0.9383 - val_loss: 0.4887 - val_accuracy: 0.9524\n",
            "Epoch 117/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.4098 - accuracy: 0.9383 - val_loss: 0.4880 - val_accuracy: 0.9524\n",
            "Epoch 118/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.4074 - accuracy: 0.9383 - val_loss: 0.4874 - val_accuracy: 0.9524\n",
            "Epoch 119/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.4049 - accuracy: 0.9383 - val_loss: 0.4867 - val_accuracy: 0.9524\n",
            "Epoch 120/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.4025 - accuracy: 0.9383 - val_loss: 0.4860 - val_accuracy: 0.9524\n",
            "Epoch 121/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.4002 - accuracy: 0.9383 - val_loss: 0.4853 - val_accuracy: 0.9524\n",
            "Epoch 122/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.3979 - accuracy: 0.9506 - val_loss: 0.4846 - val_accuracy: 0.9524\n",
            "Epoch 123/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.3956 - accuracy: 0.9506 - val_loss: 0.4838 - val_accuracy: 0.9524\n",
            "Epoch 124/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.3934 - accuracy: 0.9506 - val_loss: 0.4830 - val_accuracy: 0.9524\n",
            "Epoch 125/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.3913 - accuracy: 0.9506 - val_loss: 0.4823 - val_accuracy: 0.9524\n",
            "Epoch 126/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.3891 - accuracy: 0.9506 - val_loss: 0.4816 - val_accuracy: 0.9524\n",
            "Epoch 127/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.3871 - accuracy: 0.9506 - val_loss: 0.4810 - val_accuracy: 0.9524\n",
            "Epoch 128/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.3850 - accuracy: 0.9506 - val_loss: 0.4805 - val_accuracy: 0.9524\n",
            "Epoch 129/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.3830 - accuracy: 0.9506 - val_loss: 0.4800 - val_accuracy: 0.9524\n",
            "Epoch 130/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.3810 - accuracy: 0.9506 - val_loss: 0.4797 - val_accuracy: 0.9524\n",
            "Epoch 131/1000\n",
            "81/81 [==============================] - 0s 78us/step - loss: 0.3791 - accuracy: 0.9506 - val_loss: 0.4794 - val_accuracy: 0.9524\n",
            "Epoch 132/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.3772 - accuracy: 0.9506 - val_loss: 0.4792 - val_accuracy: 0.9524\n",
            "Epoch 133/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.3753 - accuracy: 0.9506 - val_loss: 0.4790 - val_accuracy: 0.9524\n",
            "Epoch 134/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.3735 - accuracy: 0.9506 - val_loss: 0.4787 - val_accuracy: 0.9524\n",
            "Epoch 135/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.3717 - accuracy: 0.9630 - val_loss: 0.4784 - val_accuracy: 0.9524\n",
            "Epoch 136/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.3699 - accuracy: 0.9630 - val_loss: 0.4781 - val_accuracy: 0.9524\n",
            "Epoch 137/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.3682 - accuracy: 0.9630 - val_loss: 0.4777 - val_accuracy: 0.9524\n",
            "Epoch 138/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.3664 - accuracy: 0.9630 - val_loss: 0.4774 - val_accuracy: 0.9524\n",
            "Epoch 139/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.3648 - accuracy: 0.9630 - val_loss: 0.4771 - val_accuracy: 0.9524\n",
            "Epoch 140/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.3631 - accuracy: 0.9630 - val_loss: 0.4768 - val_accuracy: 0.9524\n",
            "Epoch 141/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.3614 - accuracy: 0.9630 - val_loss: 0.4766 - val_accuracy: 0.9524\n",
            "Epoch 142/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.3598 - accuracy: 0.9630 - val_loss: 0.4764 - val_accuracy: 0.9524\n",
            "Epoch 143/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.3582 - accuracy: 0.9630 - val_loss: 0.4763 - val_accuracy: 0.9524\n",
            "Epoch 144/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.3566 - accuracy: 0.9630 - val_loss: 0.4763 - val_accuracy: 0.9524\n",
            "Epoch 145/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.3551 - accuracy: 0.9630 - val_loss: 0.4763 - val_accuracy: 0.9524\n",
            "Epoch 146/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.3535 - accuracy: 0.9630 - val_loss: 0.4763 - val_accuracy: 0.9524\n",
            "Epoch 147/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.3520 - accuracy: 0.9630 - val_loss: 0.4764 - val_accuracy: 0.9524\n",
            "Epoch 148/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.3505 - accuracy: 0.9630 - val_loss: 0.4765 - val_accuracy: 0.9524\n",
            "Epoch 149/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.3490 - accuracy: 0.9630 - val_loss: 0.4766 - val_accuracy: 0.9524\n",
            "Epoch 150/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.3475 - accuracy: 0.9630 - val_loss: 0.4767 - val_accuracy: 0.9524\n",
            "Epoch 151/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.3461 - accuracy: 0.9630 - val_loss: 0.4767 - val_accuracy: 0.9524\n",
            "Epoch 152/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.3446 - accuracy: 0.9630 - val_loss: 0.4768 - val_accuracy: 0.9524\n",
            "Epoch 153/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.3432 - accuracy: 0.9630 - val_loss: 0.4769 - val_accuracy: 0.9524\n",
            "Epoch 154/1000\n",
            "81/81 [==============================] - 0s 92us/step - loss: 0.3418 - accuracy: 0.9630 - val_loss: 0.4769 - val_accuracy: 0.9524\n",
            "Epoch 155/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.3404 - accuracy: 0.9630 - val_loss: 0.4770 - val_accuracy: 0.9524\n",
            "Epoch 156/1000\n",
            "81/81 [==============================] - 0s 127us/step - loss: 0.3390 - accuracy: 0.9630 - val_loss: 0.4770 - val_accuracy: 0.9524\n",
            "Epoch 157/1000\n",
            "81/81 [==============================] - 0s 93us/step - loss: 0.3376 - accuracy: 0.9630 - val_loss: 0.4771 - val_accuracy: 0.9524\n",
            "Epoch 158/1000\n",
            "81/81 [==============================] - 0s 78us/step - loss: 0.3363 - accuracy: 0.9630 - val_loss: 0.4773 - val_accuracy: 0.9524\n",
            "Epoch 159/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.3349 - accuracy: 0.9630 - val_loss: 0.4775 - val_accuracy: 0.9524\n",
            "Epoch 160/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.3336 - accuracy: 0.9630 - val_loss: 0.4777 - val_accuracy: 0.9524\n",
            "Epoch 161/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.3323 - accuracy: 0.9630 - val_loss: 0.4780 - val_accuracy: 0.9524\n",
            "Epoch 162/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.3310 - accuracy: 0.9630 - val_loss: 0.4783 - val_accuracy: 0.9524\n",
            "Epoch 163/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.3297 - accuracy: 0.9630 - val_loss: 0.4787 - val_accuracy: 0.9524\n",
            "Epoch 164/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.3284 - accuracy: 0.9630 - val_loss: 0.4791 - val_accuracy: 0.9524\n",
            "Epoch 165/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.3272 - accuracy: 0.9630 - val_loss: 0.4794 - val_accuracy: 0.9524\n",
            "Epoch 166/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.3259 - accuracy: 0.9630 - val_loss: 0.4798 - val_accuracy: 0.9524\n",
            "Epoch 167/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.3247 - accuracy: 0.9630 - val_loss: 0.4801 - val_accuracy: 0.9524\n",
            "Epoch 168/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.3235 - accuracy: 0.9630 - val_loss: 0.4804 - val_accuracy: 0.9524\n",
            "Epoch 169/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.3222 - accuracy: 0.9630 - val_loss: 0.4806 - val_accuracy: 0.9524\n",
            "Epoch 170/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.3210 - accuracy: 0.9630 - val_loss: 0.4809 - val_accuracy: 0.9524\n",
            "Epoch 171/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.3198 - accuracy: 0.9630 - val_loss: 0.4812 - val_accuracy: 0.9524\n",
            "Epoch 172/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.3186 - accuracy: 0.9630 - val_loss: 0.4815 - val_accuracy: 0.9524\n",
            "Epoch 173/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.3175 - accuracy: 0.9630 - val_loss: 0.4818 - val_accuracy: 0.9524\n",
            "Epoch 174/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.3163 - accuracy: 0.9630 - val_loss: 0.4821 - val_accuracy: 0.9524\n",
            "Epoch 175/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.3151 - accuracy: 0.9630 - val_loss: 0.4825 - val_accuracy: 0.9524\n",
            "Epoch 176/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.3140 - accuracy: 0.9630 - val_loss: 0.4829 - val_accuracy: 0.9524\n",
            "Epoch 177/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.3128 - accuracy: 0.9630 - val_loss: 0.4833 - val_accuracy: 0.9524\n",
            "Epoch 178/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.3117 - accuracy: 0.9630 - val_loss: 0.4837 - val_accuracy: 0.9524\n",
            "Epoch 179/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.3106 - accuracy: 0.9630 - val_loss: 0.4842 - val_accuracy: 0.9524\n",
            "Epoch 180/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.3094 - accuracy: 0.9630 - val_loss: 0.4847 - val_accuracy: 0.9524\n",
            "Epoch 181/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.3083 - accuracy: 0.9630 - val_loss: 0.4852 - val_accuracy: 0.9524\n",
            "Epoch 182/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.3072 - accuracy: 0.9630 - val_loss: 0.4857 - val_accuracy: 0.9524\n",
            "Epoch 183/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.3061 - accuracy: 0.9630 - val_loss: 0.4862 - val_accuracy: 0.9524\n",
            "Epoch 184/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.3050 - accuracy: 0.9630 - val_loss: 0.4867 - val_accuracy: 0.9524\n",
            "Epoch 185/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.3039 - accuracy: 0.9630 - val_loss: 0.4872 - val_accuracy: 0.9524\n",
            "Epoch 186/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.3029 - accuracy: 0.9630 - val_loss: 0.4877 - val_accuracy: 0.9524\n",
            "Epoch 187/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.3018 - accuracy: 0.9630 - val_loss: 0.4882 - val_accuracy: 0.9524\n",
            "Epoch 188/1000\n",
            "81/81 [==============================] - 0s 46us/step - loss: 0.3007 - accuracy: 0.9630 - val_loss: 0.4887 - val_accuracy: 0.9524\n",
            "Epoch 189/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.2997 - accuracy: 0.9630 - val_loss: 0.4892 - val_accuracy: 0.9524\n",
            "Epoch 190/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2986 - accuracy: 0.9630 - val_loss: 0.4897 - val_accuracy: 0.9524\n",
            "Epoch 191/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.2976 - accuracy: 0.9630 - val_loss: 0.4902 - val_accuracy: 0.9524\n",
            "Epoch 192/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2966 - accuracy: 0.9630 - val_loss: 0.4908 - val_accuracy: 0.9524\n",
            "Epoch 193/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2955 - accuracy: 0.9630 - val_loss: 0.4913 - val_accuracy: 0.9524\n",
            "Epoch 194/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.2945 - accuracy: 0.9753 - val_loss: 0.4919 - val_accuracy: 0.9524\n",
            "Epoch 195/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2935 - accuracy: 0.9753 - val_loss: 0.4925 - val_accuracy: 0.9524\n",
            "Epoch 196/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2925 - accuracy: 0.9753 - val_loss: 0.4931 - val_accuracy: 0.9524\n",
            "Epoch 197/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.2915 - accuracy: 0.9753 - val_loss: 0.4937 - val_accuracy: 0.9524\n",
            "Epoch 198/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.2905 - accuracy: 0.9753 - val_loss: 0.4943 - val_accuracy: 0.9524\n",
            "Epoch 199/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2895 - accuracy: 0.9753 - val_loss: 0.4949 - val_accuracy: 0.9524\n",
            "Epoch 200/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2885 - accuracy: 0.9753 - val_loss: 0.4955 - val_accuracy: 0.9524\n",
            "Epoch 201/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2875 - accuracy: 0.9753 - val_loss: 0.4961 - val_accuracy: 0.9524\n",
            "Epoch 202/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2865 - accuracy: 0.9753 - val_loss: 0.4967 - val_accuracy: 0.9524\n",
            "Epoch 203/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2856 - accuracy: 0.9753 - val_loss: 0.4973 - val_accuracy: 0.9524\n",
            "Epoch 204/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.2846 - accuracy: 0.9753 - val_loss: 0.4980 - val_accuracy: 0.9524\n",
            "Epoch 205/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.2837 - accuracy: 0.9753 - val_loss: 0.4986 - val_accuracy: 0.9524\n",
            "Epoch 206/1000\n",
            "81/81 [==============================] - 0s 85us/step - loss: 0.2827 - accuracy: 0.9753 - val_loss: 0.4992 - val_accuracy: 0.9524\n",
            "Epoch 207/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.2818 - accuracy: 0.9753 - val_loss: 0.4998 - val_accuracy: 0.9524\n",
            "Epoch 208/1000\n",
            "81/81 [==============================] - 0s 93us/step - loss: 0.2808 - accuracy: 0.9753 - val_loss: 0.5005 - val_accuracy: 0.9524\n",
            "Epoch 209/1000\n",
            "81/81 [==============================] - 0s 96us/step - loss: 0.2799 - accuracy: 0.9753 - val_loss: 0.5011 - val_accuracy: 0.9524\n",
            "Epoch 210/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.2789 - accuracy: 0.9753 - val_loss: 0.5018 - val_accuracy: 0.9524\n",
            "Epoch 211/1000\n",
            "81/81 [==============================] - 0s 78us/step - loss: 0.2780 - accuracy: 0.9753 - val_loss: 0.5024 - val_accuracy: 0.9524\n",
            "Epoch 212/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.2771 - accuracy: 0.9753 - val_loss: 0.5031 - val_accuracy: 0.9524\n",
            "Epoch 213/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2762 - accuracy: 0.9753 - val_loss: 0.5038 - val_accuracy: 0.9524\n",
            "Epoch 214/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2753 - accuracy: 0.9753 - val_loss: 0.5044 - val_accuracy: 0.9524\n",
            "Epoch 215/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.2744 - accuracy: 0.9753 - val_loss: 0.5051 - val_accuracy: 0.9524\n",
            "Epoch 216/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.2735 - accuracy: 0.9753 - val_loss: 0.5058 - val_accuracy: 0.9524\n",
            "Epoch 217/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.2726 - accuracy: 0.9753 - val_loss: 0.5065 - val_accuracy: 0.9524\n",
            "Epoch 218/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.2717 - accuracy: 0.9753 - val_loss: 0.5072 - val_accuracy: 0.9524\n",
            "Epoch 219/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2708 - accuracy: 0.9753 - val_loss: 0.5079 - val_accuracy: 0.9524\n",
            "Epoch 220/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2700 - accuracy: 0.9753 - val_loss: 0.5086 - val_accuracy: 0.9524\n",
            "Epoch 221/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.2691 - accuracy: 0.9753 - val_loss: 0.5092 - val_accuracy: 0.9524\n",
            "Epoch 222/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2682 - accuracy: 0.9753 - val_loss: 0.5099 - val_accuracy: 0.9524\n",
            "Epoch 223/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2674 - accuracy: 0.9753 - val_loss: 0.5106 - val_accuracy: 0.9524\n",
            "Epoch 224/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2665 - accuracy: 0.9753 - val_loss: 0.5113 - val_accuracy: 0.9524\n",
            "Epoch 225/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2656 - accuracy: 0.9753 - val_loss: 0.5120 - val_accuracy: 0.9524\n",
            "Epoch 226/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2648 - accuracy: 0.9753 - val_loss: 0.5127 - val_accuracy: 0.9524\n",
            "Epoch 227/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2640 - accuracy: 0.9753 - val_loss: 0.5134 - val_accuracy: 0.9524\n",
            "Epoch 228/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.2631 - accuracy: 0.9753 - val_loss: 0.5141 - val_accuracy: 0.9524\n",
            "Epoch 229/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2623 - accuracy: 0.9753 - val_loss: 0.5149 - val_accuracy: 0.9524\n",
            "Epoch 230/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2615 - accuracy: 0.9753 - val_loss: 0.5156 - val_accuracy: 0.9524\n",
            "Epoch 231/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2606 - accuracy: 0.9753 - val_loss: 0.5163 - val_accuracy: 0.9524\n",
            "Epoch 232/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.2598 - accuracy: 0.9753 - val_loss: 0.5170 - val_accuracy: 0.9524\n",
            "Epoch 233/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.2590 - accuracy: 0.9753 - val_loss: 0.5177 - val_accuracy: 0.9524\n",
            "Epoch 234/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2582 - accuracy: 0.9753 - val_loss: 0.5184 - val_accuracy: 0.9524\n",
            "Epoch 235/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.2574 - accuracy: 0.9753 - val_loss: 0.5192 - val_accuracy: 0.9524\n",
            "Epoch 236/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.2566 - accuracy: 0.9753 - val_loss: 0.5199 - val_accuracy: 0.9524\n",
            "Epoch 237/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.2558 - accuracy: 0.9753 - val_loss: 0.5206 - val_accuracy: 0.9524\n",
            "Epoch 238/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.2550 - accuracy: 0.9753 - val_loss: 0.5213 - val_accuracy: 0.9524\n",
            "Epoch 239/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2542 - accuracy: 0.9753 - val_loss: 0.5221 - val_accuracy: 0.9524\n",
            "Epoch 240/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2534 - accuracy: 0.9753 - val_loss: 0.5228 - val_accuracy: 0.9524\n",
            "Epoch 241/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.2526 - accuracy: 0.9753 - val_loss: 0.5235 - val_accuracy: 0.9524\n",
            "Epoch 242/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2519 - accuracy: 0.9753 - val_loss: 0.5242 - val_accuracy: 0.9524\n",
            "Epoch 243/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.2511 - accuracy: 0.9753 - val_loss: 0.5250 - val_accuracy: 0.9524\n",
            "Epoch 244/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.2503 - accuracy: 0.9753 - val_loss: 0.5257 - val_accuracy: 0.9524\n",
            "Epoch 245/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2496 - accuracy: 0.9753 - val_loss: 0.5264 - val_accuracy: 0.9524\n",
            "Epoch 246/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 0.2488 - accuracy: 0.9753 - val_loss: 0.5271 - val_accuracy: 0.9524\n",
            "Epoch 247/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.2481 - accuracy: 0.9753 - val_loss: 0.5279 - val_accuracy: 0.9524\n",
            "Epoch 248/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.2473 - accuracy: 0.9753 - val_loss: 0.5286 - val_accuracy: 0.9524\n",
            "Epoch 249/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 0.2466 - accuracy: 0.9753 - val_loss: 0.5293 - val_accuracy: 0.9524\n",
            "Epoch 250/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2458 - accuracy: 0.9753 - val_loss: 0.5300 - val_accuracy: 0.9524\n",
            "Epoch 251/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.2451 - accuracy: 0.9753 - val_loss: 0.5308 - val_accuracy: 0.9524\n",
            "Epoch 252/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2444 - accuracy: 0.9753 - val_loss: 0.5315 - val_accuracy: 0.9524\n",
            "Epoch 253/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.2436 - accuracy: 0.9753 - val_loss: 0.5322 - val_accuracy: 0.9524\n",
            "Epoch 254/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 0.2429 - accuracy: 0.9753 - val_loss: 0.5329 - val_accuracy: 0.9524\n",
            "Epoch 255/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.2422 - accuracy: 0.9753 - val_loss: 0.5337 - val_accuracy: 0.9524\n",
            "Epoch 256/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.2415 - accuracy: 0.9753 - val_loss: 0.5344 - val_accuracy: 0.9524\n",
            "Epoch 257/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.2408 - accuracy: 0.9753 - val_loss: 0.5351 - val_accuracy: 0.9524\n",
            "Epoch 258/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2401 - accuracy: 0.9753 - val_loss: 0.5358 - val_accuracy: 0.9524\n",
            "Epoch 259/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2394 - accuracy: 0.9753 - val_loss: 0.5366 - val_accuracy: 0.9524\n",
            "Epoch 260/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2387 - accuracy: 0.9753 - val_loss: 0.5373 - val_accuracy: 0.9524\n",
            "Epoch 261/1000\n",
            "81/81 [==============================] - 0s 84us/step - loss: 0.2380 - accuracy: 0.9753 - val_loss: 0.5380 - val_accuracy: 0.9524\n",
            "Epoch 262/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2373 - accuracy: 0.9753 - val_loss: 0.5387 - val_accuracy: 0.9524\n",
            "Epoch 263/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.2366 - accuracy: 0.9753 - val_loss: 0.5394 - val_accuracy: 0.9524\n",
            "Epoch 264/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2359 - accuracy: 0.9753 - val_loss: 0.5402 - val_accuracy: 0.9524\n",
            "Epoch 265/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.2352 - accuracy: 0.9753 - val_loss: 0.5409 - val_accuracy: 0.9524\n",
            "Epoch 266/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2346 - accuracy: 0.9753 - val_loss: 0.5416 - val_accuracy: 0.9524\n",
            "Epoch 267/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2339 - accuracy: 0.9753 - val_loss: 0.5423 - val_accuracy: 0.9524\n",
            "Epoch 268/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.2332 - accuracy: 0.9753 - val_loss: 0.5430 - val_accuracy: 0.9524\n",
            "Epoch 269/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.2326 - accuracy: 0.9753 - val_loss: 0.5437 - val_accuracy: 0.9524\n",
            "Epoch 270/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.2319 - accuracy: 0.9753 - val_loss: 0.5444 - val_accuracy: 0.9524\n",
            "Epoch 271/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.2312 - accuracy: 0.9753 - val_loss: 0.5451 - val_accuracy: 0.9524\n",
            "Epoch 272/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2306 - accuracy: 0.9753 - val_loss: 0.5458 - val_accuracy: 0.9524\n",
            "Epoch 273/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2299 - accuracy: 0.9753 - val_loss: 0.5465 - val_accuracy: 0.9524\n",
            "Epoch 274/1000\n",
            "81/81 [==============================] - 0s 89us/step - loss: 0.2293 - accuracy: 0.9753 - val_loss: 0.5472 - val_accuracy: 0.9524\n",
            "Epoch 275/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.2287 - accuracy: 0.9753 - val_loss: 0.5479 - val_accuracy: 0.9524\n",
            "Epoch 276/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2280 - accuracy: 0.9753 - val_loss: 0.5486 - val_accuracy: 0.9524\n",
            "Epoch 277/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.2274 - accuracy: 0.9753 - val_loss: 0.5492 - val_accuracy: 0.9524\n",
            "Epoch 278/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.2268 - accuracy: 0.9753 - val_loss: 0.5499 - val_accuracy: 0.9524\n",
            "Epoch 279/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2261 - accuracy: 0.9753 - val_loss: 0.5506 - val_accuracy: 0.9524\n",
            "Epoch 280/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.2255 - accuracy: 0.9753 - val_loss: 0.5513 - val_accuracy: 0.9524\n",
            "Epoch 281/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.2249 - accuracy: 0.9753 - val_loss: 0.5520 - val_accuracy: 0.9524\n",
            "Epoch 282/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.2243 - accuracy: 0.9753 - val_loss: 0.5527 - val_accuracy: 0.9524\n",
            "Epoch 283/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.2237 - accuracy: 0.9753 - val_loss: 0.5534 - val_accuracy: 0.9524\n",
            "Epoch 284/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2231 - accuracy: 0.9753 - val_loss: 0.5541 - val_accuracy: 0.9524\n",
            "Epoch 285/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2225 - accuracy: 0.9753 - val_loss: 0.5548 - val_accuracy: 0.9524\n",
            "Epoch 286/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2219 - accuracy: 0.9753 - val_loss: 0.5555 - val_accuracy: 0.9524\n",
            "Epoch 287/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2213 - accuracy: 0.9753 - val_loss: 0.5561 - val_accuracy: 0.9524\n",
            "Epoch 288/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.2207 - accuracy: 0.9753 - val_loss: 0.5568 - val_accuracy: 0.9524\n",
            "Epoch 289/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2201 - accuracy: 0.9753 - val_loss: 0.5575 - val_accuracy: 0.9524\n",
            "Epoch 290/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.2195 - accuracy: 0.9753 - val_loss: 0.5581 - val_accuracy: 0.9524\n",
            "Epoch 291/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.2189 - accuracy: 0.9753 - val_loss: 0.5588 - val_accuracy: 0.9524\n",
            "Epoch 292/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2183 - accuracy: 0.9753 - val_loss: 0.5594 - val_accuracy: 0.9524\n",
            "Epoch 293/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.2178 - accuracy: 0.9753 - val_loss: 0.5601 - val_accuracy: 0.9524\n",
            "Epoch 294/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.2172 - accuracy: 0.9753 - val_loss: 0.5607 - val_accuracy: 0.9524\n",
            "Epoch 295/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2166 - accuracy: 0.9753 - val_loss: 0.5614 - val_accuracy: 0.9524\n",
            "Epoch 296/1000\n",
            "81/81 [==============================] - 0s 88us/step - loss: 0.2161 - accuracy: 0.9753 - val_loss: 0.5621 - val_accuracy: 0.9524\n",
            "Epoch 297/1000\n",
            "81/81 [==============================] - 0s 81us/step - loss: 0.2155 - accuracy: 0.9753 - val_loss: 0.5627 - val_accuracy: 0.9524\n",
            "Epoch 298/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.2149 - accuracy: 0.9753 - val_loss: 0.5634 - val_accuracy: 0.9524\n",
            "Epoch 299/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2144 - accuracy: 0.9753 - val_loss: 0.5640 - val_accuracy: 0.9524\n",
            "Epoch 300/1000\n",
            "81/81 [==============================] - 0s 89us/step - loss: 0.2138 - accuracy: 0.9753 - val_loss: 0.5647 - val_accuracy: 0.9524\n",
            "Epoch 301/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2133 - accuracy: 0.9753 - val_loss: 0.5653 - val_accuracy: 0.9524\n",
            "Epoch 302/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.2127 - accuracy: 0.9753 - val_loss: 0.5660 - val_accuracy: 0.9524\n",
            "Epoch 303/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.2122 - accuracy: 0.9877 - val_loss: 0.5666 - val_accuracy: 0.9524\n",
            "Epoch 304/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2116 - accuracy: 0.9877 - val_loss: 0.5672 - val_accuracy: 0.9524\n",
            "Epoch 305/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.2111 - accuracy: 0.9877 - val_loss: 0.5679 - val_accuracy: 0.9524\n",
            "Epoch 306/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.2106 - accuracy: 0.9877 - val_loss: 0.5685 - val_accuracy: 0.9524\n",
            "Epoch 307/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.2100 - accuracy: 0.9877 - val_loss: 0.5691 - val_accuracy: 0.9524\n",
            "Epoch 308/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.2095 - accuracy: 0.9877 - val_loss: 0.5697 - val_accuracy: 0.9524\n",
            "Epoch 309/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2090 - accuracy: 0.9877 - val_loss: 0.5704 - val_accuracy: 0.9524\n",
            "Epoch 310/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.2085 - accuracy: 0.9877 - val_loss: 0.5710 - val_accuracy: 0.9524\n",
            "Epoch 311/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.2080 - accuracy: 0.9877 - val_loss: 0.5716 - val_accuracy: 0.9524\n",
            "Epoch 312/1000\n",
            "81/81 [==============================] - 0s 146us/step - loss: 0.2074 - accuracy: 0.9877 - val_loss: 0.5722 - val_accuracy: 0.9524\n",
            "Epoch 313/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.2069 - accuracy: 0.9877 - val_loss: 0.5728 - val_accuracy: 0.9524\n",
            "Epoch 314/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.2064 - accuracy: 0.9877 - val_loss: 0.5734 - val_accuracy: 0.9524\n",
            "Epoch 315/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.2059 - accuracy: 0.9877 - val_loss: 0.5741 - val_accuracy: 0.9524\n",
            "Epoch 316/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.2054 - accuracy: 1.0000 - val_loss: 0.5747 - val_accuracy: 0.9524\n",
            "Epoch 317/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.2049 - accuracy: 1.0000 - val_loss: 0.5753 - val_accuracy: 0.9524\n",
            "Epoch 318/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.2044 - accuracy: 1.0000 - val_loss: 0.5759 - val_accuracy: 0.9524\n",
            "Epoch 319/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.2039 - accuracy: 1.0000 - val_loss: 0.5765 - val_accuracy: 0.9524\n",
            "Epoch 320/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2034 - accuracy: 1.0000 - val_loss: 0.5771 - val_accuracy: 0.9524\n",
            "Epoch 321/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.2029 - accuracy: 1.0000 - val_loss: 0.5777 - val_accuracy: 0.9524\n",
            "Epoch 322/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.2024 - accuracy: 1.0000 - val_loss: 0.5783 - val_accuracy: 0.9524\n",
            "Epoch 323/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.2019 - accuracy: 1.0000 - val_loss: 0.5789 - val_accuracy: 0.9524\n",
            "Epoch 324/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.2015 - accuracy: 1.0000 - val_loss: 0.5794 - val_accuracy: 0.9524\n",
            "Epoch 325/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.2010 - accuracy: 1.0000 - val_loss: 0.5800 - val_accuracy: 0.9524\n",
            "Epoch 326/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.2005 - accuracy: 1.0000 - val_loss: 0.5806 - val_accuracy: 0.9524\n",
            "Epoch 327/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.2000 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 0.9524\n",
            "Epoch 328/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1996 - accuracy: 1.0000 - val_loss: 0.5818 - val_accuracy: 0.9524\n",
            "Epoch 329/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.1991 - accuracy: 1.0000 - val_loss: 0.5823 - val_accuracy: 0.9524\n",
            "Epoch 330/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1986 - accuracy: 1.0000 - val_loss: 0.5829 - val_accuracy: 0.9524\n",
            "Epoch 331/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1982 - accuracy: 1.0000 - val_loss: 0.5835 - val_accuracy: 0.9524\n",
            "Epoch 332/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1977 - accuracy: 1.0000 - val_loss: 0.5841 - val_accuracy: 0.9524\n",
            "Epoch 333/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.1972 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.9524\n",
            "Epoch 334/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1968 - accuracy: 1.0000 - val_loss: 0.5852 - val_accuracy: 0.9524\n",
            "Epoch 335/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1963 - accuracy: 1.0000 - val_loss: 0.5857 - val_accuracy: 0.9524\n",
            "Epoch 336/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1959 - accuracy: 1.0000 - val_loss: 0.5863 - val_accuracy: 0.9524\n",
            "Epoch 337/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.1954 - accuracy: 1.0000 - val_loss: 0.5869 - val_accuracy: 0.9524\n",
            "Epoch 338/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.1950 - accuracy: 1.0000 - val_loss: 0.5874 - val_accuracy: 0.9524\n",
            "Epoch 339/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1945 - accuracy: 1.0000 - val_loss: 0.5880 - val_accuracy: 0.9524\n",
            "Epoch 340/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1941 - accuracy: 1.0000 - val_loss: 0.5885 - val_accuracy: 0.9524\n",
            "Epoch 341/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1936 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.9524\n",
            "Epoch 342/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1932 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 0.9524\n",
            "Epoch 343/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1928 - accuracy: 1.0000 - val_loss: 0.5902 - val_accuracy: 0.9524\n",
            "Epoch 344/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1923 - accuracy: 1.0000 - val_loss: 0.5907 - val_accuracy: 0.9524\n",
            "Epoch 345/1000\n",
            "81/81 [==============================] - 0s 40us/step - loss: 0.1919 - accuracy: 1.0000 - val_loss: 0.5912 - val_accuracy: 0.9524\n",
            "Epoch 346/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.1915 - accuracy: 1.0000 - val_loss: 0.5918 - val_accuracy: 0.9524\n",
            "Epoch 347/1000\n",
            "81/81 [==============================] - 0s 172us/step - loss: 0.1911 - accuracy: 1.0000 - val_loss: 0.5923 - val_accuracy: 0.9524\n",
            "Epoch 348/1000\n",
            "81/81 [==============================] - 0s 99us/step - loss: 0.1906 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 0.9524\n",
            "Epoch 349/1000\n",
            "81/81 [==============================] - 0s 111us/step - loss: 0.1902 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.9524\n",
            "Epoch 350/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1898 - accuracy: 1.0000 - val_loss: 0.5939 - val_accuracy: 0.9524\n",
            "Epoch 351/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1894 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 0.9524\n",
            "Epoch 352/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1890 - accuracy: 1.0000 - val_loss: 0.5949 - val_accuracy: 0.9524\n",
            "Epoch 353/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.1885 - accuracy: 1.0000 - val_loss: 0.5955 - val_accuracy: 0.9524\n",
            "Epoch 354/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1881 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 0.9524\n",
            "Epoch 355/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1877 - accuracy: 1.0000 - val_loss: 0.5965 - val_accuracy: 0.9524\n",
            "Epoch 356/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1873 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 0.9524\n",
            "Epoch 357/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1869 - accuracy: 1.0000 - val_loss: 0.5975 - val_accuracy: 0.9524\n",
            "Epoch 358/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1865 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.9524\n",
            "Epoch 359/1000\n",
            "81/81 [==============================] - 0s 81us/step - loss: 0.1861 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.9524\n",
            "Epoch 360/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.1857 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 0.9524\n",
            "Epoch 361/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1853 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 0.9524\n",
            "Epoch 362/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.1849 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 0.9524\n",
            "Epoch 363/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1845 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 0.9524\n",
            "Epoch 364/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1841 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 0.9524\n",
            "Epoch 365/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1838 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 0.9524\n",
            "Epoch 366/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1834 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 0.9524\n",
            "Epoch 367/1000\n",
            "81/81 [==============================] - 0s 82us/step - loss: 0.1830 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 0.9524\n",
            "Epoch 368/1000\n",
            "81/81 [==============================] - 0s 78us/step - loss: 0.1826 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.9524\n",
            "Epoch 369/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1822 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 0.9524\n",
            "Epoch 370/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1818 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 0.9524\n",
            "Epoch 371/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1815 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 0.9524\n",
            "Epoch 372/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1811 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 0.9524\n",
            "Epoch 373/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1807 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 0.9524\n",
            "Epoch 374/1000\n",
            "81/81 [==============================] - 0s 43us/step - loss: 0.1803 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 0.9524\n",
            "Epoch 375/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1800 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 0.9524\n",
            "Epoch 376/1000\n",
            "81/81 [==============================] - 0s 108us/step - loss: 0.1796 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 0.9524\n",
            "Epoch 377/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1792 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 0.9524\n",
            "Epoch 378/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1789 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 0.9524\n",
            "Epoch 379/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1785 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 0.9524\n",
            "Epoch 380/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1781 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.9524\n",
            "Epoch 381/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1778 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 0.9524\n",
            "Epoch 382/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1774 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 0.9524\n",
            "Epoch 383/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1771 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 0.9524\n",
            "Epoch 384/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1767 - accuracy: 1.0000 - val_loss: 0.6105 - val_accuracy: 0.9524\n",
            "Epoch 385/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.1764 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.9524\n",
            "Epoch 386/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.1760 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 0.9524\n",
            "Epoch 387/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1757 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 0.9524\n",
            "Epoch 388/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1753 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 0.9524\n",
            "Epoch 389/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1750 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 0.9524\n",
            "Epoch 390/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1746 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 0.9524\n",
            "Epoch 391/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.1743 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 0.9524\n",
            "Epoch 392/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1739 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 0.9524\n",
            "Epoch 393/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1736 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.9524\n",
            "Epoch 394/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1733 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 0.9524\n",
            "Epoch 395/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1729 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 0.9524\n",
            "Epoch 396/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1726 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.9524\n",
            "Epoch 397/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1723 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 0.9524\n",
            "Epoch 398/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1719 - accuracy: 1.0000 - val_loss: 0.6167 - val_accuracy: 0.9524\n",
            "Epoch 399/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1716 - accuracy: 1.0000 - val_loss: 0.6171 - val_accuracy: 0.9524\n",
            "Epoch 400/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1713 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 0.9524\n",
            "Epoch 401/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1709 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 0.9524\n",
            "Epoch 402/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1706 - accuracy: 1.0000 - val_loss: 0.6184 - val_accuracy: 0.9524\n",
            "Epoch 403/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1703 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.9524\n",
            "Epoch 404/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1700 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.9524\n",
            "Epoch 405/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1697 - accuracy: 1.0000 - val_loss: 0.6196 - val_accuracy: 0.9524\n",
            "Epoch 406/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1693 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 0.9524\n",
            "Epoch 407/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1690 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.9524\n",
            "Epoch 408/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1687 - accuracy: 1.0000 - val_loss: 0.6208 - val_accuracy: 0.9524\n",
            "Epoch 409/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1684 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.9524\n",
            "Epoch 410/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1681 - accuracy: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.9524\n",
            "Epoch 411/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1677 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 0.9524\n",
            "Epoch 412/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1674 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 0.9524\n",
            "Epoch 413/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1671 - accuracy: 1.0000 - val_loss: 0.6228 - val_accuracy: 0.9524\n",
            "Epoch 414/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1668 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 0.9524\n",
            "Epoch 415/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1665 - accuracy: 1.0000 - val_loss: 0.6236 - val_accuracy: 0.9524\n",
            "Epoch 416/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1662 - accuracy: 1.0000 - val_loss: 0.6240 - val_accuracy: 0.9524\n",
            "Epoch 417/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1659 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 0.9524\n",
            "Epoch 418/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1656 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 0.9524\n",
            "Epoch 419/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1653 - accuracy: 1.0000 - val_loss: 0.6251 - val_accuracy: 0.9524\n",
            "Epoch 420/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1650 - accuracy: 1.0000 - val_loss: 0.6255 - val_accuracy: 0.9524\n",
            "Epoch 421/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1647 - accuracy: 1.0000 - val_loss: 0.6259 - val_accuracy: 0.9524\n",
            "Epoch 422/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1644 - accuracy: 1.0000 - val_loss: 0.6263 - val_accuracy: 0.9524\n",
            "Epoch 423/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1641 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 0.9524\n",
            "Epoch 424/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1638 - accuracy: 1.0000 - val_loss: 0.6270 - val_accuracy: 0.9524\n",
            "Epoch 425/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1635 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 0.9524\n",
            "Epoch 426/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1632 - accuracy: 1.0000 - val_loss: 0.6278 - val_accuracy: 0.9524\n",
            "Epoch 427/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1629 - accuracy: 1.0000 - val_loss: 0.6281 - val_accuracy: 0.9524\n",
            "Epoch 428/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1626 - accuracy: 1.0000 - val_loss: 0.6285 - val_accuracy: 0.9524\n",
            "Epoch 429/1000\n",
            "81/81 [==============================] - 0s 46us/step - loss: 0.1623 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.9524\n",
            "Epoch 430/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1620 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 0.9524\n",
            "Epoch 431/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1617 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 0.9524\n",
            "Epoch 432/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1615 - accuracy: 1.0000 - val_loss: 0.6299 - val_accuracy: 0.9524\n",
            "Epoch 433/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 0.9524\n",
            "Epoch 434/1000\n",
            "81/81 [==============================] - 0s 97us/step - loss: 0.1609 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 0.9524\n",
            "Epoch 435/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1606 - accuracy: 1.0000 - val_loss: 0.6310 - val_accuracy: 0.9524\n",
            "Epoch 436/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1603 - accuracy: 1.0000 - val_loss: 0.6313 - val_accuracy: 0.9524\n",
            "Epoch 437/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1600 - accuracy: 1.0000 - val_loss: 0.6317 - val_accuracy: 0.9524\n",
            "Epoch 438/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1598 - accuracy: 1.0000 - val_loss: 0.6320 - val_accuracy: 0.9524\n",
            "Epoch 439/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1595 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 0.9524\n",
            "Epoch 440/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1592 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 0.9524\n",
            "Epoch 441/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1589 - accuracy: 1.0000 - val_loss: 0.6331 - val_accuracy: 0.9524\n",
            "Epoch 442/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1587 - accuracy: 1.0000 - val_loss: 0.6334 - val_accuracy: 0.9524\n",
            "Epoch 443/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1584 - accuracy: 1.0000 - val_loss: 0.6338 - val_accuracy: 0.9524\n",
            "Epoch 444/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1581 - accuracy: 1.0000 - val_loss: 0.6341 - val_accuracy: 0.9524\n",
            "Epoch 445/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1578 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.9524\n",
            "Epoch 446/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1576 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.9524\n",
            "Epoch 447/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1573 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 0.9524\n",
            "Epoch 448/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1570 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.9524\n",
            "Epoch 449/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1567 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.9524\n",
            "Epoch 450/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1565 - accuracy: 1.0000 - val_loss: 0.6361 - val_accuracy: 0.9524\n",
            "Epoch 451/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1562 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 0.9524\n",
            "Epoch 452/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1559 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.9524\n",
            "Epoch 453/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.1557 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 0.9524\n",
            "Epoch 454/1000\n",
            "81/81 [==============================] - 0s 84us/step - loss: 0.1554 - accuracy: 1.0000 - val_loss: 0.6373 - val_accuracy: 0.9524\n",
            "Epoch 455/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1552 - accuracy: 1.0000 - val_loss: 0.6377 - val_accuracy: 0.9524\n",
            "Epoch 456/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1549 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.9524\n",
            "Epoch 457/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1546 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 0.9524\n",
            "Epoch 458/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1544 - accuracy: 1.0000 - val_loss: 0.6386 - val_accuracy: 0.9524\n",
            "Epoch 459/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.1541 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.9524\n",
            "Epoch 460/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1538 - accuracy: 1.0000 - val_loss: 0.6392 - val_accuracy: 0.9524\n",
            "Epoch 461/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1536 - accuracy: 1.0000 - val_loss: 0.6395 - val_accuracy: 0.9524\n",
            "Epoch 462/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.1533 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.9524\n",
            "Epoch 463/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1531 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.9524\n",
            "Epoch 464/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1528 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.9524\n",
            "Epoch 465/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1526 - accuracy: 1.0000 - val_loss: 0.6407 - val_accuracy: 0.9524\n",
            "Epoch 466/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.1523 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.9524\n",
            "Epoch 467/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.1521 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 0.9524\n",
            "Epoch 468/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1518 - accuracy: 1.0000 - val_loss: 0.6416 - val_accuracy: 0.9524\n",
            "Epoch 469/1000\n",
            "81/81 [==============================] - 0s 81us/step - loss: 0.1516 - accuracy: 1.0000 - val_loss: 0.6419 - val_accuracy: 0.9524\n",
            "Epoch 470/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 0.1513 - accuracy: 1.0000 - val_loss: 0.6422 - val_accuracy: 0.9524\n",
            "Epoch 471/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1511 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.9524\n",
            "Epoch 472/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1508 - accuracy: 1.0000 - val_loss: 0.6428 - val_accuracy: 0.9524\n",
            "Epoch 473/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1506 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.9524\n",
            "Epoch 474/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1503 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 0.9524\n",
            "Epoch 475/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1501 - accuracy: 1.0000 - val_loss: 0.6436 - val_accuracy: 0.9524\n",
            "Epoch 476/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1498 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.9524\n",
            "Epoch 477/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1496 - accuracy: 1.0000 - val_loss: 0.6442 - val_accuracy: 0.9524\n",
            "Epoch 478/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.1493 - accuracy: 1.0000 - val_loss: 0.6444 - val_accuracy: 0.9524\n",
            "Epoch 479/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1491 - accuracy: 1.0000 - val_loss: 0.6447 - val_accuracy: 0.9524\n",
            "Epoch 480/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1489 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.9524\n",
            "Epoch 481/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1486 - accuracy: 1.0000 - val_loss: 0.6453 - val_accuracy: 0.9524\n",
            "Epoch 482/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1484 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.9524\n",
            "Epoch 483/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.1481 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 0.9524\n",
            "Epoch 484/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1479 - accuracy: 1.0000 - val_loss: 0.6461 - val_accuracy: 0.9524\n",
            "Epoch 485/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1477 - accuracy: 1.0000 - val_loss: 0.6463 - val_accuracy: 0.9524\n",
            "Epoch 486/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1474 - accuracy: 1.0000 - val_loss: 0.6466 - val_accuracy: 0.9524\n",
            "Epoch 487/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1472 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 0.9524\n",
            "Epoch 488/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1470 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 0.9524\n",
            "Epoch 489/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1467 - accuracy: 1.0000 - val_loss: 0.6474 - val_accuracy: 0.9524\n",
            "Epoch 490/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1465 - accuracy: 1.0000 - val_loss: 0.6476 - val_accuracy: 0.9524\n",
            "Epoch 491/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1463 - accuracy: 1.0000 - val_loss: 0.6479 - val_accuracy: 0.9524\n",
            "Epoch 492/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1460 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.9524\n",
            "Epoch 493/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1458 - accuracy: 1.0000 - val_loss: 0.6484 - val_accuracy: 0.9524\n",
            "Epoch 494/1000\n",
            "81/81 [==============================] - 0s 86us/step - loss: 0.1456 - accuracy: 1.0000 - val_loss: 0.6487 - val_accuracy: 0.9524\n",
            "Epoch 495/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1453 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.9524\n",
            "Epoch 496/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1451 - accuracy: 1.0000 - val_loss: 0.6492 - val_accuracy: 0.9524\n",
            "Epoch 497/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1449 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 0.9524\n",
            "Epoch 498/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1446 - accuracy: 1.0000 - val_loss: 0.6497 - val_accuracy: 0.9524\n",
            "Epoch 499/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1444 - accuracy: 1.0000 - val_loss: 0.6499 - val_accuracy: 0.9524\n",
            "Epoch 500/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1442 - accuracy: 1.0000 - val_loss: 0.6501 - val_accuracy: 0.9524\n",
            "Epoch 501/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.1440 - accuracy: 1.0000 - val_loss: 0.6504 - val_accuracy: 0.9524\n",
            "Epoch 502/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1437 - accuracy: 1.0000 - val_loss: 0.6507 - val_accuracy: 0.9524\n",
            "Epoch 503/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1435 - accuracy: 1.0000 - val_loss: 0.6509 - val_accuracy: 0.9524\n",
            "Epoch 504/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.1433 - accuracy: 1.0000 - val_loss: 0.6512 - val_accuracy: 0.9524\n",
            "Epoch 505/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1431 - accuracy: 1.0000 - val_loss: 0.6514 - val_accuracy: 0.9524\n",
            "Epoch 506/1000\n",
            "81/81 [==============================] - 0s 86us/step - loss: 0.1429 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 0.9524\n",
            "Epoch 507/1000\n",
            "81/81 [==============================] - 0s 124us/step - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.9524\n",
            "Epoch 508/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1424 - accuracy: 1.0000 - val_loss: 0.6522 - val_accuracy: 0.9524\n",
            "Epoch 509/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1422 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.9524\n",
            "Epoch 510/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1420 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 0.9524\n",
            "Epoch 511/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1418 - accuracy: 1.0000 - val_loss: 0.6529 - val_accuracy: 0.9524\n",
            "Epoch 512/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1415 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 0.9524\n",
            "Epoch 513/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1413 - accuracy: 1.0000 - val_loss: 0.6533 - val_accuracy: 0.9524\n",
            "Epoch 514/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1411 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 0.9524\n",
            "Epoch 515/1000\n",
            "81/81 [==============================] - 0s 85us/step - loss: 0.1409 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 0.9524\n",
            "Epoch 516/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1407 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 0.9524\n",
            "Epoch 517/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.1405 - accuracy: 1.0000 - val_loss: 0.6542 - val_accuracy: 0.9524\n",
            "Epoch 518/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1402 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 0.9524\n",
            "Epoch 519/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1400 - accuracy: 1.0000 - val_loss: 0.6546 - val_accuracy: 0.9524\n",
            "Epoch 520/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1398 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 0.9524\n",
            "Epoch 521/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1396 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 0.9524\n",
            "Epoch 522/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.1394 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 0.9524\n",
            "Epoch 523/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1392 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.9524\n",
            "Epoch 524/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1390 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 0.9524\n",
            "Epoch 525/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.1388 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 0.9524\n",
            "Epoch 526/1000\n",
            "81/81 [==============================] - 0s 87us/step - loss: 0.1385 - accuracy: 1.0000 - val_loss: 0.6562 - val_accuracy: 0.9524\n",
            "Epoch 527/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1383 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.9524\n",
            "Epoch 528/1000\n",
            "81/81 [==============================] - 0s 112us/step - loss: 0.1381 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 0.9524\n",
            "Epoch 529/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1379 - accuracy: 1.0000 - val_loss: 0.6568 - val_accuracy: 0.9524\n",
            "Epoch 530/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1377 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 0.9524\n",
            "Epoch 531/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1375 - accuracy: 1.0000 - val_loss: 0.6572 - val_accuracy: 0.9524\n",
            "Epoch 532/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1373 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.9524\n",
            "Epoch 533/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1371 - accuracy: 1.0000 - val_loss: 0.6576 - val_accuracy: 0.9524\n",
            "Epoch 534/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1369 - accuracy: 1.0000 - val_loss: 0.6578 - val_accuracy: 0.9524\n",
            "Epoch 535/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1367 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 0.9524\n",
            "Epoch 536/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1365 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.9524\n",
            "Epoch 537/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.1363 - accuracy: 1.0000 - val_loss: 0.6583 - val_accuracy: 0.9524\n",
            "Epoch 538/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1361 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 0.9524\n",
            "Epoch 539/1000\n",
            "81/81 [==============================] - 0s 82us/step - loss: 0.1359 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.9524\n",
            "Epoch 540/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1357 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 0.9524\n",
            "Epoch 541/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.1355 - accuracy: 1.0000 - val_loss: 0.6594 - val_accuracy: 0.9524\n",
            "Epoch 542/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1353 - accuracy: 1.0000 - val_loss: 0.6597 - val_accuracy: 0.9524\n",
            "Epoch 543/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.1351 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 0.9524\n",
            "Epoch 544/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1349 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.9524\n",
            "Epoch 545/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1347 - accuracy: 1.0000 - val_loss: 0.6605 - val_accuracy: 0.9524\n",
            "Epoch 546/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1345 - accuracy: 1.0000 - val_loss: 0.6607 - val_accuracy: 0.9524\n",
            "Epoch 547/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1343 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 0.9524\n",
            "Epoch 548/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1341 - accuracy: 1.0000 - val_loss: 0.6612 - val_accuracy: 0.9524\n",
            "Epoch 549/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1339 - accuracy: 1.0000 - val_loss: 0.6615 - val_accuracy: 0.9524\n",
            "Epoch 550/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1337 - accuracy: 1.0000 - val_loss: 0.6618 - val_accuracy: 0.9524\n",
            "Epoch 551/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1335 - accuracy: 1.0000 - val_loss: 0.6620 - val_accuracy: 0.9524\n",
            "Epoch 552/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1333 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.9524\n",
            "Epoch 553/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1331 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 0.9524\n",
            "Epoch 554/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1329 - accuracy: 1.0000 - val_loss: 0.6625 - val_accuracy: 0.9524\n",
            "Epoch 555/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1327 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.9524\n",
            "Epoch 556/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1325 - accuracy: 1.0000 - val_loss: 0.6628 - val_accuracy: 0.9524\n",
            "Epoch 557/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1323 - accuracy: 1.0000 - val_loss: 0.6630 - val_accuracy: 0.9524\n",
            "Epoch 558/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1321 - accuracy: 1.0000 - val_loss: 0.6632 - val_accuracy: 0.9524\n",
            "Epoch 559/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1319 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.9524\n",
            "Epoch 560/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1318 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 0.9524\n",
            "Epoch 561/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1316 - accuracy: 1.0000 - val_loss: 0.6638 - val_accuracy: 0.9524\n",
            "Epoch 562/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1314 - accuracy: 1.0000 - val_loss: 0.6640 - val_accuracy: 0.9524\n",
            "Epoch 563/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1312 - accuracy: 1.0000 - val_loss: 0.6642 - val_accuracy: 0.9524\n",
            "Epoch 564/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1310 - accuracy: 1.0000 - val_loss: 0.6644 - val_accuracy: 0.9524\n",
            "Epoch 565/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1308 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 0.9524\n",
            "Epoch 566/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1306 - accuracy: 1.0000 - val_loss: 0.6648 - val_accuracy: 0.9524\n",
            "Epoch 567/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1304 - accuracy: 1.0000 - val_loss: 0.6650 - val_accuracy: 0.9524\n",
            "Epoch 568/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1303 - accuracy: 1.0000 - val_loss: 0.6651 - val_accuracy: 0.9524\n",
            "Epoch 569/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1301 - accuracy: 1.0000 - val_loss: 0.6653 - val_accuracy: 0.9524\n",
            "Epoch 570/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1299 - accuracy: 1.0000 - val_loss: 0.6654 - val_accuracy: 0.9524\n",
            "Epoch 571/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1297 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 0.9524\n",
            "Epoch 572/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1295 - accuracy: 1.0000 - val_loss: 0.6656 - val_accuracy: 0.9524\n",
            "Epoch 573/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 0.6657 - val_accuracy: 0.9524\n",
            "Epoch 574/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1292 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 0.9524\n",
            "Epoch 575/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1290 - accuracy: 1.0000 - val_loss: 0.6661 - val_accuracy: 0.9524\n",
            "Epoch 576/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1288 - accuracy: 1.0000 - val_loss: 0.6663 - val_accuracy: 0.9524\n",
            "Epoch 577/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1286 - accuracy: 1.0000 - val_loss: 0.6665 - val_accuracy: 0.9524\n",
            "Epoch 578/1000\n",
            "81/81 [==============================] - 0s 91us/step - loss: 0.1284 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 0.9524\n",
            "Epoch 579/1000\n",
            "81/81 [==============================] - 0s 82us/step - loss: 0.1283 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 0.9524\n",
            "Epoch 580/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1281 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.9524\n",
            "Epoch 581/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.1279 - accuracy: 1.0000 - val_loss: 0.6671 - val_accuracy: 0.9524\n",
            "Epoch 582/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1277 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 0.9524\n",
            "Epoch 583/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1275 - accuracy: 1.0000 - val_loss: 0.6674 - val_accuracy: 0.9524\n",
            "Epoch 584/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1274 - accuracy: 1.0000 - val_loss: 0.6676 - val_accuracy: 0.9524\n",
            "Epoch 585/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.1272 - accuracy: 1.0000 - val_loss: 0.6678 - val_accuracy: 0.9524\n",
            "Epoch 586/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1270 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 0.9524\n",
            "Epoch 587/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1268 - accuracy: 1.0000 - val_loss: 0.6681 - val_accuracy: 0.9524\n",
            "Epoch 588/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1267 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.9524\n",
            "Epoch 589/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1265 - accuracy: 1.0000 - val_loss: 0.6683 - val_accuracy: 0.9524\n",
            "Epoch 590/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1263 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 0.9524\n",
            "Epoch 591/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.1261 - accuracy: 1.0000 - val_loss: 0.6686 - val_accuracy: 0.9524\n",
            "Epoch 592/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1260 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 0.9524\n",
            "Epoch 593/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1258 - accuracy: 1.0000 - val_loss: 0.6690 - val_accuracy: 0.9524\n",
            "Epoch 594/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1256 - accuracy: 1.0000 - val_loss: 0.6691 - val_accuracy: 0.9524\n",
            "Epoch 595/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1254 - accuracy: 1.0000 - val_loss: 0.6693 - val_accuracy: 0.9524\n",
            "Epoch 596/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 0.9524\n",
            "Epoch 597/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1251 - accuracy: 1.0000 - val_loss: 0.6695 - val_accuracy: 0.9524\n",
            "Epoch 598/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1249 - accuracy: 1.0000 - val_loss: 0.6697 - val_accuracy: 0.9524\n",
            "Epoch 599/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1247 - accuracy: 1.0000 - val_loss: 0.6698 - val_accuracy: 0.9524\n",
            "Epoch 600/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1246 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.9524\n",
            "Epoch 601/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1244 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 0.9524\n",
            "Epoch 602/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1242 - accuracy: 1.0000 - val_loss: 0.6703 - val_accuracy: 0.9524\n",
            "Epoch 603/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1241 - accuracy: 1.0000 - val_loss: 0.6704 - val_accuracy: 0.9524\n",
            "Epoch 604/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.1239 - accuracy: 1.0000 - val_loss: 0.6705 - val_accuracy: 0.9524\n",
            "Epoch 605/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1237 - accuracy: 1.0000 - val_loss: 0.6706 - val_accuracy: 0.9524\n",
            "Epoch 606/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1236 - accuracy: 1.0000 - val_loss: 0.6708 - val_accuracy: 0.9524\n",
            "Epoch 607/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1234 - accuracy: 1.0000 - val_loss: 0.6709 - val_accuracy: 0.9524\n",
            "Epoch 608/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1232 - accuracy: 1.0000 - val_loss: 0.6710 - val_accuracy: 0.9524\n",
            "Epoch 609/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1231 - accuracy: 1.0000 - val_loss: 0.6712 - val_accuracy: 0.9524\n",
            "Epoch 610/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1229 - accuracy: 1.0000 - val_loss: 0.6714 - val_accuracy: 0.9524\n",
            "Epoch 611/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1227 - accuracy: 1.0000 - val_loss: 0.6715 - val_accuracy: 0.9524\n",
            "Epoch 612/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1226 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 0.9524\n",
            "Epoch 613/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1224 - accuracy: 1.0000 - val_loss: 0.6717 - val_accuracy: 0.9524\n",
            "Epoch 614/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1222 - accuracy: 1.0000 - val_loss: 0.6719 - val_accuracy: 0.9524\n",
            "Epoch 615/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1221 - accuracy: 1.0000 - val_loss: 0.6720 - val_accuracy: 0.9524\n",
            "Epoch 616/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1219 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.9524\n",
            "Epoch 617/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1217 - accuracy: 1.0000 - val_loss: 0.6723 - val_accuracy: 0.9524\n",
            "Epoch 618/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1216 - accuracy: 1.0000 - val_loss: 0.6724 - val_accuracy: 0.9524\n",
            "Epoch 619/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1214 - accuracy: 1.0000 - val_loss: 0.6725 - val_accuracy: 0.9524\n",
            "Epoch 620/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1212 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.9524\n",
            "Epoch 621/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1211 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 0.9524\n",
            "Epoch 622/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.1209 - accuracy: 1.0000 - val_loss: 0.6729 - val_accuracy: 0.9524\n",
            "Epoch 623/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1208 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.9524\n",
            "Epoch 624/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1206 - accuracy: 1.0000 - val_loss: 0.6731 - val_accuracy: 0.9524\n",
            "Epoch 625/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1204 - accuracy: 1.0000 - val_loss: 0.6732 - val_accuracy: 0.9524\n",
            "Epoch 626/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1203 - accuracy: 1.0000 - val_loss: 0.6734 - val_accuracy: 0.9524\n",
            "Epoch 627/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1201 - accuracy: 1.0000 - val_loss: 0.6735 - val_accuracy: 0.9524\n",
            "Epoch 628/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1200 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.9524\n",
            "Epoch 629/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 0.9524\n",
            "Epoch 630/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1196 - accuracy: 1.0000 - val_loss: 0.6738 - val_accuracy: 0.9524\n",
            "Epoch 631/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1195 - accuracy: 1.0000 - val_loss: 0.6739 - val_accuracy: 0.9524\n",
            "Epoch 632/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1193 - accuracy: 1.0000 - val_loss: 0.6740 - val_accuracy: 0.9524\n",
            "Epoch 633/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1192 - accuracy: 1.0000 - val_loss: 0.6741 - val_accuracy: 0.9524\n",
            "Epoch 634/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1190 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.9524\n",
            "Epoch 635/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1189 - accuracy: 1.0000 - val_loss: 0.6744 - val_accuracy: 0.9524\n",
            "Epoch 636/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1187 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.9524\n",
            "Epoch 637/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1185 - accuracy: 1.0000 - val_loss: 0.6746 - val_accuracy: 0.9524\n",
            "Epoch 638/1000\n",
            "81/81 [==============================] - 0s 140us/step - loss: 0.1184 - accuracy: 1.0000 - val_loss: 0.6747 - val_accuracy: 0.9524\n",
            "Epoch 639/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1182 - accuracy: 1.0000 - val_loss: 0.6748 - val_accuracy: 0.9524\n",
            "Epoch 640/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1181 - accuracy: 1.0000 - val_loss: 0.6749 - val_accuracy: 0.9524\n",
            "Epoch 641/1000\n",
            "81/81 [==============================] - 0s 115us/step - loss: 0.1179 - accuracy: 1.0000 - val_loss: 0.6750 - val_accuracy: 0.9524\n",
            "Epoch 642/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1178 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.9524\n",
            "Epoch 643/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1176 - accuracy: 1.0000 - val_loss: 0.6752 - val_accuracy: 0.9524\n",
            "Epoch 644/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1175 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 0.9524\n",
            "Epoch 645/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1173 - accuracy: 1.0000 - val_loss: 0.6754 - val_accuracy: 0.9524\n",
            "Epoch 646/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1172 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.9524\n",
            "Epoch 647/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1170 - accuracy: 1.0000 - val_loss: 0.6756 - val_accuracy: 0.9524\n",
            "Epoch 648/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1169 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.9524\n",
            "Epoch 649/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.6758 - val_accuracy: 0.9524\n",
            "Epoch 650/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1166 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.9524\n",
            "Epoch 651/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1164 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 0.9524\n",
            "Epoch 652/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1162 - accuracy: 1.0000 - val_loss: 0.6762 - val_accuracy: 0.9524\n",
            "Epoch 653/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1161 - accuracy: 1.0000 - val_loss: 0.6763 - val_accuracy: 0.9524\n",
            "Epoch 654/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 0.6764 - val_accuracy: 0.9524\n",
            "Epoch 655/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1158 - accuracy: 1.0000 - val_loss: 0.6764 - val_accuracy: 0.9524\n",
            "Epoch 656/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1157 - accuracy: 1.0000 - val_loss: 0.6765 - val_accuracy: 0.9524\n",
            "Epoch 657/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1155 - accuracy: 1.0000 - val_loss: 0.6766 - val_accuracy: 0.9524\n",
            "Epoch 658/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1154 - accuracy: 1.0000 - val_loss: 0.6767 - val_accuracy: 0.9524\n",
            "Epoch 659/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.1152 - accuracy: 1.0000 - val_loss: 0.6768 - val_accuracy: 0.9524\n",
            "Epoch 660/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1151 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.9524\n",
            "Epoch 661/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1149 - accuracy: 1.0000 - val_loss: 0.6770 - val_accuracy: 0.9524\n",
            "Epoch 662/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1148 - accuracy: 1.0000 - val_loss: 0.6771 - val_accuracy: 0.9524\n",
            "Epoch 663/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1146 - accuracy: 1.0000 - val_loss: 0.6772 - val_accuracy: 0.9524\n",
            "Epoch 664/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1145 - accuracy: 1.0000 - val_loss: 0.6773 - val_accuracy: 0.9524\n",
            "Epoch 665/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1143 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.9524\n",
            "Epoch 666/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1142 - accuracy: 1.0000 - val_loss: 0.6775 - val_accuracy: 0.9524\n",
            "Epoch 667/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1140 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.9524\n",
            "Epoch 668/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1139 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.9524\n",
            "Epoch 669/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.6777 - val_accuracy: 0.9524\n",
            "Epoch 670/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1136 - accuracy: 1.0000 - val_loss: 0.6779 - val_accuracy: 0.9524\n",
            "Epoch 671/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1135 - accuracy: 1.0000 - val_loss: 0.6780 - val_accuracy: 0.9524\n",
            "Epoch 672/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.1133 - accuracy: 1.0000 - val_loss: 0.6781 - val_accuracy: 0.9524\n",
            "Epoch 673/1000\n",
            "81/81 [==============================] - 0s 43us/step - loss: 0.1132 - accuracy: 1.0000 - val_loss: 0.6781 - val_accuracy: 0.9524\n",
            "Epoch 674/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.9524\n",
            "Epoch 675/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1129 - accuracy: 1.0000 - val_loss: 0.6783 - val_accuracy: 0.9524\n",
            "Epoch 676/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1127 - accuracy: 1.0000 - val_loss: 0.6783 - val_accuracy: 0.9524\n",
            "Epoch 677/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1126 - accuracy: 1.0000 - val_loss: 0.6784 - val_accuracy: 0.9524\n",
            "Epoch 678/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1125 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.9524\n",
            "Epoch 679/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1123 - accuracy: 1.0000 - val_loss: 0.6786 - val_accuracy: 0.9524\n",
            "Epoch 680/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1122 - accuracy: 1.0000 - val_loss: 0.6787 - val_accuracy: 0.9524\n",
            "Epoch 681/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1120 - accuracy: 1.0000 - val_loss: 0.6788 - val_accuracy: 0.9524\n",
            "Epoch 682/1000\n",
            "81/81 [==============================] - 0s 88us/step - loss: 0.1119 - accuracy: 1.0000 - val_loss: 0.6789 - val_accuracy: 0.9524\n",
            "Epoch 683/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.9524\n",
            "Epoch 684/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1116 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 0.9524\n",
            "Epoch 685/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1115 - accuracy: 1.0000 - val_loss: 0.6791 - val_accuracy: 0.9524\n",
            "Epoch 686/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1113 - accuracy: 1.0000 - val_loss: 0.6792 - val_accuracy: 0.9524\n",
            "Epoch 687/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.1112 - accuracy: 1.0000 - val_loss: 0.6793 - val_accuracy: 0.9524\n",
            "Epoch 688/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1111 - accuracy: 1.0000 - val_loss: 0.6794 - val_accuracy: 0.9524\n",
            "Epoch 689/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1109 - accuracy: 1.0000 - val_loss: 0.6795 - val_accuracy: 0.9524\n",
            "Epoch 690/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.1108 - accuracy: 1.0000 - val_loss: 0.6796 - val_accuracy: 0.9524\n",
            "Epoch 691/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1106 - accuracy: 1.0000 - val_loss: 0.6796 - val_accuracy: 0.9524\n",
            "Epoch 692/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1105 - accuracy: 1.0000 - val_loss: 0.6797 - val_accuracy: 0.9524\n",
            "Epoch 693/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1104 - accuracy: 1.0000 - val_loss: 0.6797 - val_accuracy: 0.9524\n",
            "Epoch 694/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.1102 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.9524\n",
            "Epoch 695/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1101 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.9524\n",
            "Epoch 696/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 0.6800 - val_accuracy: 0.9524\n",
            "Epoch 697/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1098 - accuracy: 1.0000 - val_loss: 0.6801 - val_accuracy: 0.9524\n",
            "Epoch 698/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1097 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 0.9524\n",
            "Epoch 699/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1096 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 0.9524\n",
            "Epoch 700/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1094 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.9524\n",
            "Epoch 701/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1093 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.9524\n",
            "Epoch 702/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1091 - accuracy: 1.0000 - val_loss: 0.6804 - val_accuracy: 0.9524\n",
            "Epoch 703/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1090 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.9524\n",
            "Epoch 704/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1089 - accuracy: 1.0000 - val_loss: 0.6806 - val_accuracy: 0.9524\n",
            "Epoch 705/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.1087 - accuracy: 1.0000 - val_loss: 0.6807 - val_accuracy: 0.9524\n",
            "Epoch 706/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1086 - accuracy: 1.0000 - val_loss: 0.6808 - val_accuracy: 0.9524\n",
            "Epoch 707/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1085 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.9524\n",
            "Epoch 708/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1083 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.9524\n",
            "Epoch 709/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.9524\n",
            "Epoch 710/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1081 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.9524\n",
            "Epoch 711/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1080 - accuracy: 1.0000 - val_loss: 0.6810 - val_accuracy: 0.9524\n",
            "Epoch 712/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1078 - accuracy: 1.0000 - val_loss: 0.6811 - val_accuracy: 0.9524\n",
            "Epoch 713/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1077 - accuracy: 1.0000 - val_loss: 0.6812 - val_accuracy: 0.9524\n",
            "Epoch 714/1000\n",
            "81/81 [==============================] - 0s 46us/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 0.6813 - val_accuracy: 0.9524\n",
            "Epoch 715/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.1074 - accuracy: 1.0000 - val_loss: 0.6814 - val_accuracy: 0.9524\n",
            "Epoch 716/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1073 - accuracy: 1.0000 - val_loss: 0.6814 - val_accuracy: 0.9524\n",
            "Epoch 717/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1072 - accuracy: 1.0000 - val_loss: 0.6815 - val_accuracy: 0.9524\n",
            "Epoch 718/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 0.6815 - val_accuracy: 0.9524\n",
            "Epoch 719/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1069 - accuracy: 1.0000 - val_loss: 0.6816 - val_accuracy: 0.9524\n",
            "Epoch 720/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.1068 - accuracy: 1.0000 - val_loss: 0.6817 - val_accuracy: 0.9524\n",
            "Epoch 721/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.1067 - accuracy: 1.0000 - val_loss: 0.6818 - val_accuracy: 0.9524\n",
            "Epoch 722/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.1065 - accuracy: 1.0000 - val_loss: 0.6818 - val_accuracy: 0.9524\n",
            "Epoch 723/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.1064 - accuracy: 1.0000 - val_loss: 0.6819 - val_accuracy: 0.9524\n",
            "Epoch 724/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1063 - accuracy: 1.0000 - val_loss: 0.6819 - val_accuracy: 0.9524\n",
            "Epoch 725/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.1061 - accuracy: 1.0000 - val_loss: 0.6820 - val_accuracy: 0.9524\n",
            "Epoch 726/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1060 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.9524\n",
            "Epoch 727/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1059 - accuracy: 1.0000 - val_loss: 0.6822 - val_accuracy: 0.9524\n",
            "Epoch 728/1000\n",
            "81/81 [==============================] - 0s 95us/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 0.6823 - val_accuracy: 0.9524\n",
            "Epoch 729/1000\n",
            "81/81 [==============================] - 0s 88us/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.6824 - val_accuracy: 0.9524\n",
            "Epoch 730/1000\n",
            "81/81 [==============================] - 0s 114us/step - loss: 0.1055 - accuracy: 1.0000 - val_loss: 0.6824 - val_accuracy: 0.9524\n",
            "Epoch 731/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1054 - accuracy: 1.0000 - val_loss: 0.6825 - val_accuracy: 0.9524\n",
            "Epoch 732/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.1052 - accuracy: 1.0000 - val_loss: 0.6826 - val_accuracy: 0.9524\n",
            "Epoch 733/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1051 - accuracy: 1.0000 - val_loss: 0.6826 - val_accuracy: 0.9524\n",
            "Epoch 734/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1050 - accuracy: 1.0000 - val_loss: 0.6827 - val_accuracy: 0.9524\n",
            "Epoch 735/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1049 - accuracy: 1.0000 - val_loss: 0.6827 - val_accuracy: 0.9524\n",
            "Epoch 736/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1047 - accuracy: 1.0000 - val_loss: 0.6827 - val_accuracy: 0.9524\n",
            "Epoch 737/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1046 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.9524\n",
            "Epoch 738/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1045 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.9524\n",
            "Epoch 739/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1044 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.9524\n",
            "Epoch 740/1000\n",
            "81/81 [==============================] - 0s 106us/step - loss: 0.1042 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.9524\n",
            "Epoch 741/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1041 - accuracy: 1.0000 - val_loss: 0.6829 - val_accuracy: 0.9524\n",
            "Epoch 742/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1040 - accuracy: 1.0000 - val_loss: 0.6831 - val_accuracy: 0.9524\n",
            "Epoch 743/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1039 - accuracy: 1.0000 - val_loss: 0.6832 - val_accuracy: 0.9524\n",
            "Epoch 744/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1038 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.9524\n",
            "Epoch 745/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1036 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.9524\n",
            "Epoch 746/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1035 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.9524\n",
            "Epoch 747/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1034 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.9524\n",
            "Epoch 748/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1033 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.9524\n",
            "Epoch 749/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.1031 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.9524\n",
            "Epoch 750/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1030 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.9524\n",
            "Epoch 751/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.1029 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 0.9524\n",
            "Epoch 752/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1028 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 0.9524\n",
            "Epoch 753/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.9524\n",
            "Epoch 754/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1025 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.9524\n",
            "Epoch 755/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.1024 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.9524\n",
            "Epoch 756/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.9524\n",
            "Epoch 757/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1022 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 0.9524\n",
            "Epoch 758/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1021 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.9524\n",
            "Epoch 759/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 0.6840 - val_accuracy: 0.9524\n",
            "Epoch 760/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1018 - accuracy: 1.0000 - val_loss: 0.6840 - val_accuracy: 0.9524\n",
            "Epoch 761/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1017 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.9524\n",
            "Epoch 762/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.1016 - accuracy: 1.0000 - val_loss: 0.6842 - val_accuracy: 0.9524\n",
            "Epoch 763/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1015 - accuracy: 1.0000 - val_loss: 0.6842 - val_accuracy: 0.9524\n",
            "Epoch 764/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 0.6842 - val_accuracy: 0.9524\n",
            "Epoch 765/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.1012 - accuracy: 1.0000 - val_loss: 0.6843 - val_accuracy: 0.9524\n",
            "Epoch 766/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1011 - accuracy: 1.0000 - val_loss: 0.6843 - val_accuracy: 0.9524\n",
            "Epoch 767/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.6844 - val_accuracy: 0.9524\n",
            "Epoch 768/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.1009 - accuracy: 1.0000 - val_loss: 0.6844 - val_accuracy: 0.9524\n",
            "Epoch 769/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.1008 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.9524\n",
            "Epoch 770/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.1006 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.9524\n",
            "Epoch 771/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.1005 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.9524\n",
            "Epoch 772/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.1004 - accuracy: 1.0000 - val_loss: 0.6846 - val_accuracy: 0.9524\n",
            "Epoch 773/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.1003 - accuracy: 1.0000 - val_loss: 0.6846 - val_accuracy: 0.9524\n",
            "Epoch 774/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.1002 - accuracy: 1.0000 - val_loss: 0.6847 - val_accuracy: 0.9524\n",
            "Epoch 775/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.1001 - accuracy: 1.0000 - val_loss: 0.6847 - val_accuracy: 0.9524\n",
            "Epoch 776/1000\n",
            "81/81 [==============================] - 0s 125us/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 0.6848 - val_accuracy: 0.9524\n",
            "Epoch 777/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.0998 - accuracy: 1.0000 - val_loss: 0.6848 - val_accuracy: 0.9524\n",
            "Epoch 778/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.0997 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 0.9524\n",
            "Epoch 779/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0996 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 0.9524\n",
            "Epoch 780/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.0995 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.9524\n",
            "Epoch 781/1000\n",
            "81/81 [==============================] - 0s 89us/step - loss: 0.0994 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.9524\n",
            "Epoch 782/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0993 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.9524\n",
            "Epoch 783/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.9524\n",
            "Epoch 784/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0990 - accuracy: 1.0000 - val_loss: 0.6852 - val_accuracy: 0.9524\n",
            "Epoch 785/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0989 - accuracy: 1.0000 - val_loss: 0.6852 - val_accuracy: 0.9524\n",
            "Epoch 786/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.0988 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.9524\n",
            "Epoch 787/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.9524\n",
            "Epoch 788/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.9524\n",
            "Epoch 789/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0985 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.9524\n",
            "Epoch 790/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0984 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.9524\n",
            "Epoch 791/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0983 - accuracy: 1.0000 - val_loss: 0.6854 - val_accuracy: 0.9524\n",
            "Epoch 792/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0981 - accuracy: 1.0000 - val_loss: 0.6854 - val_accuracy: 0.9524\n",
            "Epoch 793/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0980 - accuracy: 1.0000 - val_loss: 0.6855 - val_accuracy: 0.9524\n",
            "Epoch 794/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0979 - accuracy: 1.0000 - val_loss: 0.6855 - val_accuracy: 0.9524\n",
            "Epoch 795/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0978 - accuracy: 1.0000 - val_loss: 0.6856 - val_accuracy: 0.9524\n",
            "Epoch 796/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0977 - accuracy: 1.0000 - val_loss: 0.6856 - val_accuracy: 0.9524\n",
            "Epoch 797/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0976 - accuracy: 1.0000 - val_loss: 0.6856 - val_accuracy: 0.9524\n",
            "Epoch 798/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.0975 - accuracy: 1.0000 - val_loss: 0.6857 - val_accuracy: 0.9524\n",
            "Epoch 799/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 0.6857 - val_accuracy: 0.9524\n",
            "Epoch 800/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0973 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.9524\n",
            "Epoch 801/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.0972 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.9524\n",
            "Epoch 802/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0970 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.9524\n",
            "Epoch 803/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0969 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.9524\n",
            "Epoch 804/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0968 - accuracy: 1.0000 - val_loss: 0.6859 - val_accuracy: 0.9524\n",
            "Epoch 805/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0967 - accuracy: 1.0000 - val_loss: 0.6860 - val_accuracy: 0.9524\n",
            "Epoch 806/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0966 - accuracy: 1.0000 - val_loss: 0.6860 - val_accuracy: 0.9524\n",
            "Epoch 807/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.0965 - accuracy: 1.0000 - val_loss: 0.6860 - val_accuracy: 0.9524\n",
            "Epoch 808/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.6860 - val_accuracy: 0.9524\n",
            "Epoch 809/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 0.9524\n",
            "Epoch 810/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0962 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 0.9524\n",
            "Epoch 811/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0961 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 0.9524\n",
            "Epoch 812/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0960 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 0.9524\n",
            "Epoch 813/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.9524\n",
            "Epoch 814/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0958 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.9524\n",
            "Epoch 815/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0956 - accuracy: 1.0000 - val_loss: 0.6863 - val_accuracy: 0.9524\n",
            "Epoch 816/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0955 - accuracy: 1.0000 - val_loss: 0.6863 - val_accuracy: 0.9524\n",
            "Epoch 817/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0954 - accuracy: 1.0000 - val_loss: 0.6863 - val_accuracy: 0.9524\n",
            "Epoch 818/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0953 - accuracy: 1.0000 - val_loss: 0.6863 - val_accuracy: 0.9524\n",
            "Epoch 819/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0952 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.9524\n",
            "Epoch 820/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0951 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.9524\n",
            "Epoch 821/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.9524\n",
            "Epoch 822/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0949 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.9524\n",
            "Epoch 823/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.9524\n",
            "Epoch 824/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.0947 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.9524\n",
            "Epoch 825/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0946 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.9524\n",
            "Epoch 826/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0945 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.9524\n",
            "Epoch 827/1000\n",
            "81/81 [==============================] - 0s 44us/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.9524\n",
            "Epoch 828/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.0943 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.9524\n",
            "Epoch 829/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.0942 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.9524\n",
            "Epoch 830/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0941 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.9524\n",
            "Epoch 831/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.0940 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.9524\n",
            "Epoch 832/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0939 - accuracy: 1.0000 - val_loss: 0.6868 - val_accuracy: 0.9524\n",
            "Epoch 833/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0938 - accuracy: 1.0000 - val_loss: 0.6868 - val_accuracy: 0.9524\n",
            "Epoch 834/1000\n",
            "81/81 [==============================] - 0s 44us/step - loss: 0.0937 - accuracy: 1.0000 - val_loss: 0.6868 - val_accuracy: 0.9524\n",
            "Epoch 835/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.0936 - accuracy: 1.0000 - val_loss: 0.6868 - val_accuracy: 0.9524\n",
            "Epoch 836/1000\n",
            "81/81 [==============================] - 0s 46us/step - loss: 0.0935 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.9524\n",
            "Epoch 837/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.0934 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.9524\n",
            "Epoch 838/1000\n",
            "81/81 [==============================] - 0s 44us/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.9524\n",
            "Epoch 839/1000\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.0932 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.9524\n",
            "Epoch 840/1000\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.0931 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.9524\n",
            "Epoch 841/1000\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.9524\n",
            "Epoch 842/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.0929 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.9524\n",
            "Epoch 843/1000\n",
            "81/81 [==============================] - 0s 135us/step - loss: 0.0928 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 0.9524\n",
            "Epoch 844/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0927 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.9524\n",
            "Epoch 845/1000\n",
            "81/81 [==============================] - 0s 82us/step - loss: 0.0926 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.9524\n",
            "Epoch 846/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.0925 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.9524\n",
            "Epoch 847/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 848/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0923 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 849/1000\n",
            "81/81 [==============================] - 0s 85us/step - loss: 0.0922 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.9524\n",
            "Epoch 850/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0921 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.9524\n",
            "Epoch 851/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 852/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 853/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0918 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 854/1000\n",
            "81/81 [==============================] - 0s 44us/step - loss: 0.0917 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 855/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.9524\n",
            "Epoch 856/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 0.9524\n",
            "Epoch 857/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0914 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 0.9524\n",
            "Epoch 858/1000\n",
            "81/81 [==============================] - 0s 65us/step - loss: 0.0913 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 0.9524\n",
            "Epoch 859/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.0912 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 0.9524\n",
            "Epoch 860/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.9524\n",
            "Epoch 861/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0910 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.9524\n",
            "Epoch 862/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.0909 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.9524\n",
            "Epoch 863/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0908 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 864/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0907 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 865/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0906 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.9524\n",
            "Epoch 866/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.9524\n",
            "Epoch 867/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 868/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 869/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0902 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 870/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0901 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 871/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 0.6875 - val_accuracy: 0.9524\n",
            "Epoch 872/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0899 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.9524\n",
            "Epoch 873/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.0898 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.9524\n",
            "Epoch 874/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0897 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.9524\n",
            "Epoch 875/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.9524\n",
            "Epoch 876/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0895 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.9524\n",
            "Epoch 877/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0895 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 0.9524\n",
            "Epoch 878/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 879/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 880/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0892 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 881/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0891 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 882/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0890 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 883/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.0889 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 884/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0888 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 885/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.0887 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 886/1000\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.0886 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 0.9524\n",
            "Epoch 887/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 888/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.0884 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 889/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 890/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.0882 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 891/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0882 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 892/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0881 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.9524\n",
            "Epoch 893/1000\n",
            "81/81 [==============================] - 0s 74us/step - loss: 0.0880 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 894/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.0879 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 895/1000\n",
            "81/81 [==============================] - 0s 54us/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 896/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0877 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 897/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 898/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0875 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 899/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 900/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0873 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 0.9524\n",
            "Epoch 901/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.0872 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 902/1000\n",
            "81/81 [==============================] - 0s 117us/step - loss: 0.0872 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 903/1000\n",
            "81/81 [==============================] - 0s 82us/step - loss: 0.0871 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 904/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.0870 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 905/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.0869 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 906/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0868 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 907/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.0867 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 908/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0866 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 909/1000\n",
            "81/81 [==============================] - 0s 97us/step - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 910/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.0864 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 911/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0864 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 912/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.0863 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.9524\n",
            "Epoch 913/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.0862 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 914/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 915/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.0860 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 916/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 917/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.0858 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 918/1000\n",
            "81/81 [==============================] - 0s 111us/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 919/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 920/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.0856 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 921/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0855 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 922/1000\n",
            "81/81 [==============================] - 0s 85us/step - loss: 0.0854 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 923/1000\n",
            "81/81 [==============================] - 0s 80us/step - loss: 0.0853 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 924/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.0852 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 925/1000\n",
            "81/81 [==============================] - 0s 49us/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 926/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 927/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0850 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 928/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0849 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 929/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0848 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.9524\n",
            "Epoch 930/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0847 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 931/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0846 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 932/1000\n",
            "81/81 [==============================] - 0s 68us/step - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 933/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 934/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0844 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 935/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 936/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0842 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 937/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0841 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 938/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0840 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 939/1000\n",
            "81/81 [==============================] - 0s 72us/step - loss: 0.0840 - accuracy: 1.0000 - val_loss: 0.6882 - val_accuracy: 0.9524\n",
            "Epoch 940/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0839 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 941/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0838 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 942/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.0837 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 943/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.0836 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 944/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 945/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 946/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 947/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0833 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 948/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 949/1000\n",
            "81/81 [==============================] - 0s 46us/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 950/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 951/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 952/1000\n",
            "81/81 [==============================] - 0s 58us/step - loss: 0.0829 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 953/1000\n",
            "81/81 [==============================] - 0s 83us/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 954/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0827 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 955/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.0826 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 956/1000\n",
            "81/81 [==============================] - 0s 77us/step - loss: 0.0826 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 957/1000\n",
            "81/81 [==============================] - 0s 84us/step - loss: 0.0825 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 958/1000\n",
            "81/81 [==============================] - 0s 89us/step - loss: 0.0824 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 959/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.0823 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 960/1000\n",
            "81/81 [==============================] - 0s 108us/step - loss: 0.0822 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 961/1000\n",
            "81/81 [==============================] - 0s 79us/step - loss: 0.0822 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 962/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0821 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 963/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 964/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0819 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 965/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 966/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 967/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0817 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 968/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0816 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 969/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0815 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 970/1000\n",
            "81/81 [==============================] - 0s 111us/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 971/1000\n",
            "81/81 [==============================] - 0s 75us/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 972/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0813 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 973/1000\n",
            "81/81 [==============================] - 0s 66us/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 974/1000\n",
            "81/81 [==============================] - 0s 59us/step - loss: 0.0811 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 975/1000\n",
            "81/81 [==============================] - 0s 76us/step - loss: 0.0811 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 976/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0810 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 977/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0809 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 978/1000\n",
            "81/81 [==============================] - 0s 73us/step - loss: 0.0808 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 979/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0807 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 980/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0807 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 981/1000\n",
            "81/81 [==============================] - 0s 71us/step - loss: 0.0806 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 982/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0805 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 983/1000\n",
            "81/81 [==============================] - 0s 69us/step - loss: 0.0804 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 984/1000\n",
            "81/81 [==============================] - 0s 48us/step - loss: 0.0804 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 985/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.0803 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 986/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9524\n",
            "Epoch 987/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0801 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9524\n",
            "Epoch 988/1000\n",
            "81/81 [==============================] - 0s 53us/step - loss: 0.0801 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9524\n",
            "Epoch 989/1000\n",
            "81/81 [==============================] - 0s 56us/step - loss: 0.0800 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 990/1000\n",
            "81/81 [==============================] - 0s 67us/step - loss: 0.0799 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 991/1000\n",
            "81/81 [==============================] - 0s 61us/step - loss: 0.0798 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.9524\n",
            "Epoch 992/1000\n",
            "81/81 [==============================] - 0s 64us/step - loss: 0.0797 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 993/1000\n",
            "81/81 [==============================] - 0s 70us/step - loss: 0.0797 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 994/1000\n",
            "81/81 [==============================] - 0s 50us/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9524\n",
            "Epoch 995/1000\n",
            "81/81 [==============================] - 0s 57us/step - loss: 0.0795 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9524\n",
            "Epoch 996/1000\n",
            "81/81 [==============================] - 0s 55us/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9524\n",
            "Epoch 997/1000\n",
            "81/81 [==============================] - 0s 60us/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 998/1000\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.0793 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 999/1000\n",
            "81/81 [==============================] - 0s 51us/step - loss: 0.0792 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n",
            "Epoch 1000/1000\n",
            "81/81 [==============================] - 0s 62us/step - loss: 0.0791 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.9524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Iw_OwqGB7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bcc3cb7-03ae-4ece-8ac1-98b28fabf0be"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b603ddf4-d520-4c68-8c5c-a4758b729684",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1f4f0be438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d5e5d6dd-610a-4721-99a1-fdcdc9548e3c",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1f50af0588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJs39fzCHWuPWCouwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "387421a6-eff7-4b0f-90b4-e36dca073e39",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, y_train, epochs= 300, batch_size=81, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, y_test)\n",
        "  "
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "81/81 [==============================] - 0s 670us/step - loss: 1.0943 - accuracy: 0.3333\n",
            "Epoch 2/300\n",
            "81/81 [==============================] - 0s 43us/step - loss: 1.0912 - accuracy: 0.3333\n",
            "Epoch 3/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 1.0854 - accuracy: 0.3333\n",
            "Epoch 4/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 1.0772 - accuracy: 0.3457\n",
            "Epoch 5/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 1.0670 - accuracy: 0.3457\n",
            "Epoch 6/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 1.0550 - accuracy: 0.3457\n",
            "Epoch 7/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 1.0417 - accuracy: 0.3457\n",
            "Epoch 8/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 1.0272 - accuracy: 0.3457\n",
            "Epoch 9/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 1.0120 - accuracy: 0.3457\n",
            "Epoch 10/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.9962 - accuracy: 0.3457\n",
            "Epoch 11/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.9802 - accuracy: 0.3827\n",
            "Epoch 12/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.9639 - accuracy: 0.4074\n",
            "Epoch 13/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.9476 - accuracy: 0.4321\n",
            "Epoch 14/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.9314 - accuracy: 0.4568\n",
            "Epoch 15/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.9154 - accuracy: 0.4568\n",
            "Epoch 16/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.8998 - accuracy: 0.4444\n",
            "Epoch 17/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.8845 - accuracy: 0.4568\n",
            "Epoch 18/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.8696 - accuracy: 0.4938\n",
            "Epoch 19/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.8552 - accuracy: 0.5185\n",
            "Epoch 20/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.8414 - accuracy: 0.5556\n",
            "Epoch 21/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 0.8280 - accuracy: 0.5679\n",
            "Epoch 22/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.8153 - accuracy: 0.5926\n",
            "Epoch 23/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.8031 - accuracy: 0.5679\n",
            "Epoch 24/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.7914 - accuracy: 0.5679\n",
            "Epoch 25/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7804 - accuracy: 0.5926\n",
            "Epoch 26/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7698 - accuracy: 0.6049\n",
            "Epoch 27/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.7597 - accuracy: 0.6173\n",
            "Epoch 28/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.7500 - accuracy: 0.6420\n",
            "Epoch 29/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7408 - accuracy: 0.6790\n",
            "Epoch 30/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7319 - accuracy: 0.6790\n",
            "Epoch 31/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7234 - accuracy: 0.6914\n",
            "Epoch 32/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7152 - accuracy: 0.7037\n",
            "Epoch 33/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7075 - accuracy: 0.7160\n",
            "Epoch 34/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.7000 - accuracy: 0.7160\n",
            "Epoch 35/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.6929 - accuracy: 0.7160\n",
            "Epoch 36/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.6860 - accuracy: 0.7284\n",
            "Epoch 37/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.6794 - accuracy: 0.7407\n",
            "Epoch 38/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.6730 - accuracy: 0.7407\n",
            "Epoch 39/300\n",
            "81/81 [==============================] - 0s 40us/step - loss: 0.6668 - accuracy: 0.7407\n",
            "Epoch 40/300\n",
            "81/81 [==============================] - 0s 44us/step - loss: 0.6608 - accuracy: 0.7654\n",
            "Epoch 41/300\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.6550 - accuracy: 0.7654\n",
            "Epoch 42/300\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.6493 - accuracy: 0.7654\n",
            "Epoch 43/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.6438 - accuracy: 0.7778\n",
            "Epoch 44/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.6384 - accuracy: 0.7778\n",
            "Epoch 45/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.6332 - accuracy: 0.8025\n",
            "Epoch 46/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.6281 - accuracy: 0.8148\n",
            "Epoch 47/300\n",
            "81/81 [==============================] - 0s 40us/step - loss: 0.6230 - accuracy: 0.8148\n",
            "Epoch 48/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.6181 - accuracy: 0.8148\n",
            "Epoch 49/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.6132 - accuracy: 0.8148\n",
            "Epoch 50/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.6085 - accuracy: 0.8148\n",
            "Epoch 51/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.6039 - accuracy: 0.8148\n",
            "Epoch 52/300\n",
            "81/81 [==============================] - 0s 45us/step - loss: 0.5994 - accuracy: 0.8272\n",
            "Epoch 53/300\n",
            "81/81 [==============================] - 0s 32us/step - loss: 0.5950 - accuracy: 0.8272\n",
            "Epoch 54/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 0.5906 - accuracy: 0.8272\n",
            "Epoch 55/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 0.5864 - accuracy: 0.8272\n",
            "Epoch 56/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.5822 - accuracy: 0.8272\n",
            "Epoch 57/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.5781 - accuracy: 0.8395\n",
            "Epoch 58/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5741 - accuracy: 0.8395\n",
            "Epoch 59/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.5702 - accuracy: 0.8519\n",
            "Epoch 60/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.5663 - accuracy: 0.8642\n",
            "Epoch 61/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.5626 - accuracy: 0.8642\n",
            "Epoch 62/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.5588 - accuracy: 0.8765\n",
            "Epoch 63/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.5552 - accuracy: 0.8765\n",
            "Epoch 64/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.5516 - accuracy: 0.8765\n",
            "Epoch 65/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.5480 - accuracy: 0.8765\n",
            "Epoch 66/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5445 - accuracy: 0.8765\n",
            "Epoch 67/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5410 - accuracy: 0.8765\n",
            "Epoch 68/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5376 - accuracy: 0.8765\n",
            "Epoch 69/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5343 - accuracy: 0.8765\n",
            "Epoch 70/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.5310 - accuracy: 0.8765\n",
            "Epoch 71/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5277 - accuracy: 0.8765\n",
            "Epoch 72/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.5245 - accuracy: 0.8765\n",
            "Epoch 73/300\n",
            "81/81 [==============================] - 0s 34us/step - loss: 0.5213 - accuracy: 0.8765\n",
            "Epoch 74/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 0.5182 - accuracy: 0.8765\n",
            "Epoch 75/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.5151 - accuracy: 0.8765\n",
            "Epoch 76/300\n",
            "81/81 [==============================] - 0s 47us/step - loss: 0.5120 - accuracy: 0.8765\n",
            "Epoch 77/300\n",
            "81/81 [==============================] - 0s 43us/step - loss: 0.5090 - accuracy: 0.8765\n",
            "Epoch 78/300\n",
            "81/81 [==============================] - 0s 38us/step - loss: 0.5060 - accuracy: 0.8765\n",
            "Epoch 79/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.5031 - accuracy: 0.8765\n",
            "Epoch 80/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.5002 - accuracy: 0.8765\n",
            "Epoch 81/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.4973 - accuracy: 0.8765\n",
            "Epoch 82/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.4945 - accuracy: 0.8765\n",
            "Epoch 83/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.4917 - accuracy: 0.8765\n",
            "Epoch 84/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.4890 - accuracy: 0.8765\n",
            "Epoch 85/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.4862 - accuracy: 0.8765\n",
            "Epoch 86/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.4835 - accuracy: 0.8765\n",
            "Epoch 87/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.4808 - accuracy: 0.8765\n",
            "Epoch 88/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.4781 - accuracy: 0.8765\n",
            "Epoch 89/300\n",
            "81/81 [==============================] - 0s 30us/step - loss: 0.4755 - accuracy: 0.8765\n",
            "Epoch 90/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4728 - accuracy: 0.8765\n",
            "Epoch 91/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4702 - accuracy: 0.8765\n",
            "Epoch 92/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.4677 - accuracy: 0.8765\n",
            "Epoch 93/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.4651 - accuracy: 0.8765\n",
            "Epoch 94/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4626 - accuracy: 0.8765\n",
            "Epoch 95/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4601 - accuracy: 0.8765\n",
            "Epoch 96/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.4577 - accuracy: 0.8765\n",
            "Epoch 97/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.4552 - accuracy: 0.8889\n",
            "Epoch 98/300\n",
            "81/81 [==============================] - 0s 31us/step - loss: 0.4528 - accuracy: 0.8889\n",
            "Epoch 99/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.4504 - accuracy: 0.8889\n",
            "Epoch 100/300\n",
            "81/81 [==============================] - 0s 34us/step - loss: 0.4480 - accuracy: 0.8889\n",
            "Epoch 101/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.4456 - accuracy: 0.8889\n",
            "Epoch 102/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4432 - accuracy: 0.8889\n",
            "Epoch 103/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.4409 - accuracy: 0.8889\n",
            "Epoch 104/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.4386 - accuracy: 0.8889\n",
            "Epoch 105/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.4363 - accuracy: 0.8889\n",
            "Epoch 106/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.4340 - accuracy: 0.8889\n",
            "Epoch 107/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.4318 - accuracy: 0.8889\n",
            "Epoch 108/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.4295 - accuracy: 0.9012\n",
            "Epoch 109/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.4273 - accuracy: 0.9012\n",
            "Epoch 110/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4251 - accuracy: 0.9012\n",
            "Epoch 111/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.4230 - accuracy: 0.9012\n",
            "Epoch 112/300\n",
            "81/81 [==============================] - 0s 32us/step - loss: 0.4208 - accuracy: 0.9012\n",
            "Epoch 113/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.4187 - accuracy: 0.9012\n",
            "Epoch 114/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.4166 - accuracy: 0.9012\n",
            "Epoch 115/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.4145 - accuracy: 0.9012\n",
            "Epoch 116/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.4124 - accuracy: 0.9012\n",
            "Epoch 117/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.4103 - accuracy: 0.9012\n",
            "Epoch 118/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.4082 - accuracy: 0.9012\n",
            "Epoch 119/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.4062 - accuracy: 0.9136\n",
            "Epoch 120/300\n",
            "81/81 [==============================] - 0s 37us/step - loss: 0.4042 - accuracy: 0.9136\n",
            "Epoch 121/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.4023 - accuracy: 0.9136\n",
            "Epoch 122/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.4004 - accuracy: 0.9136\n",
            "Epoch 123/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3985 - accuracy: 0.9136\n",
            "Epoch 124/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.3967 - accuracy: 0.9136\n",
            "Epoch 125/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3948 - accuracy: 0.9136\n",
            "Epoch 126/300\n",
            "81/81 [==============================] - 0s 34us/step - loss: 0.3930 - accuracy: 0.9259\n",
            "Epoch 127/300\n",
            "81/81 [==============================] - 0s 32us/step - loss: 0.3912 - accuracy: 0.9383\n",
            "Epoch 128/300\n",
            "81/81 [==============================] - 0s 33us/step - loss: 0.3895 - accuracy: 0.9383\n",
            "Epoch 129/300\n",
            "81/81 [==============================] - 0s 31us/step - loss: 0.3877 - accuracy: 0.9383\n",
            "Epoch 130/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3859 - accuracy: 0.9383\n",
            "Epoch 131/300\n",
            "81/81 [==============================] - 0s 37us/step - loss: 0.3842 - accuracy: 0.9383\n",
            "Epoch 132/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.3825 - accuracy: 0.9383\n",
            "Epoch 133/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.3808 - accuracy: 0.9383\n",
            "Epoch 134/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.3791 - accuracy: 0.9383\n",
            "Epoch 135/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.3775 - accuracy: 0.9383\n",
            "Epoch 136/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3759 - accuracy: 0.9383\n",
            "Epoch 137/300\n",
            "81/81 [==============================] - 0s 34us/step - loss: 0.3743 - accuracy: 0.9383\n",
            "Epoch 138/300\n",
            "81/81 [==============================] - 0s 32us/step - loss: 0.3727 - accuracy: 0.9383\n",
            "Epoch 139/300\n",
            "81/81 [==============================] - 0s 31us/step - loss: 0.3712 - accuracy: 0.9383\n",
            "Epoch 140/300\n",
            "81/81 [==============================] - 0s 32us/step - loss: 0.3696 - accuracy: 0.9506\n",
            "Epoch 141/300\n",
            "81/81 [==============================] - 0s 32us/step - loss: 0.3680 - accuracy: 0.9506\n",
            "Epoch 142/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.3665 - accuracy: 0.9506\n",
            "Epoch 143/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.3650 - accuracy: 0.9506\n",
            "Epoch 144/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.3634 - accuracy: 0.9506\n",
            "Epoch 145/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.3619 - accuracy: 0.9506\n",
            "Epoch 146/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.3605 - accuracy: 0.9506\n",
            "Epoch 147/300\n",
            "81/81 [==============================] - 0s 30us/step - loss: 0.3590 - accuracy: 0.9506\n",
            "Epoch 148/300\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.3576 - accuracy: 0.9506\n",
            "Epoch 149/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.3561 - accuracy: 0.9506\n",
            "Epoch 150/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.3547 - accuracy: 0.9506\n",
            "Epoch 151/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3533 - accuracy: 0.9506\n",
            "Epoch 152/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3519 - accuracy: 0.9506\n",
            "Epoch 153/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3504 - accuracy: 0.9506\n",
            "Epoch 154/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.3491 - accuracy: 0.9506\n",
            "Epoch 155/300\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.3477 - accuracy: 0.9506\n",
            "Epoch 156/300\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.3463 - accuracy: 0.9506\n",
            "Epoch 157/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.3449 - accuracy: 0.9506\n",
            "Epoch 158/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.3436 - accuracy: 0.9506\n",
            "Epoch 159/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.3422 - accuracy: 0.9506\n",
            "Epoch 160/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3409 - accuracy: 0.9506\n",
            "Epoch 161/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3396 - accuracy: 0.9506\n",
            "Epoch 162/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.3383 - accuracy: 0.9506\n",
            "Epoch 163/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.3370 - accuracy: 0.9506\n",
            "Epoch 164/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.3357 - accuracy: 0.9506\n",
            "Epoch 165/300\n",
            "81/81 [==============================] - 0s 63us/step - loss: 0.3344 - accuracy: 0.9506\n",
            "Epoch 166/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.3331 - accuracy: 0.9506\n",
            "Epoch 167/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.3319 - accuracy: 0.9506\n",
            "Epoch 168/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3307 - accuracy: 0.9506\n",
            "Epoch 169/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.3294 - accuracy: 0.9506\n",
            "Epoch 170/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3282 - accuracy: 0.9506\n",
            "Epoch 171/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.3270 - accuracy: 0.9506\n",
            "Epoch 172/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3258 - accuracy: 0.9506\n",
            "Epoch 173/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3246 - accuracy: 0.9630\n",
            "Epoch 174/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.3234 - accuracy: 0.9630\n",
            "Epoch 175/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3222 - accuracy: 0.9630\n",
            "Epoch 176/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3210 - accuracy: 0.9630\n",
            "Epoch 177/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3198 - accuracy: 0.9630\n",
            "Epoch 178/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.3187 - accuracy: 0.9630\n",
            "Epoch 179/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.3175 - accuracy: 0.9630\n",
            "Epoch 180/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.3164 - accuracy: 0.9753\n",
            "Epoch 181/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3153 - accuracy: 0.9753\n",
            "Epoch 182/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.3142 - accuracy: 0.9753\n",
            "Epoch 183/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.3131 - accuracy: 0.9753\n",
            "Epoch 184/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3120 - accuracy: 0.9753\n",
            "Epoch 185/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3109 - accuracy: 0.9753\n",
            "Epoch 186/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.3098 - accuracy: 0.9753\n",
            "Epoch 187/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3087 - accuracy: 0.9753\n",
            "Epoch 188/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.3076 - accuracy: 0.9753\n",
            "Epoch 189/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.3066 - accuracy: 0.9877\n",
            "Epoch 190/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.3055 - accuracy: 0.9877\n",
            "Epoch 191/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.3045 - accuracy: 0.9877\n",
            "Epoch 192/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.3034 - accuracy: 0.9877\n",
            "Epoch 193/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3024 - accuracy: 0.9877\n",
            "Epoch 194/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.3014 - accuracy: 0.9877\n",
            "Epoch 195/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.3004 - accuracy: 0.9877\n",
            "Epoch 196/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2993 - accuracy: 0.9877\n",
            "Epoch 197/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2983 - accuracy: 0.9877\n",
            "Epoch 198/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2973 - accuracy: 0.9877\n",
            "Epoch 199/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2964 - accuracy: 0.9877\n",
            "Epoch 200/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2954 - accuracy: 0.9877\n",
            "Epoch 201/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.2944 - accuracy: 0.9877\n",
            "Epoch 202/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2934 - accuracy: 0.9877\n",
            "Epoch 203/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2925 - accuracy: 0.9877\n",
            "Epoch 204/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2915 - accuracy: 0.9877\n",
            "Epoch 205/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2905 - accuracy: 0.9877\n",
            "Epoch 206/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2896 - accuracy: 0.9877\n",
            "Epoch 207/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2886 - accuracy: 0.9877\n",
            "Epoch 208/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.2877 - accuracy: 0.9877\n",
            "Epoch 209/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2868 - accuracy: 0.9877\n",
            "Epoch 210/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2858 - accuracy: 0.9877\n",
            "Epoch 211/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2849 - accuracy: 0.9877\n",
            "Epoch 212/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2840 - accuracy: 0.9877\n",
            "Epoch 213/300\n",
            "81/81 [==============================] - 0s 30us/step - loss: 0.2831 - accuracy: 0.9877\n",
            "Epoch 214/300\n",
            "81/81 [==============================] - 0s 31us/step - loss: 0.2822 - accuracy: 0.9877\n",
            "Epoch 215/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2813 - accuracy: 0.9877\n",
            "Epoch 216/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2804 - accuracy: 0.9877\n",
            "Epoch 217/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2795 - accuracy: 0.9877\n",
            "Epoch 218/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2786 - accuracy: 0.9877\n",
            "Epoch 219/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2777 - accuracy: 0.9877\n",
            "Epoch 220/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2769 - accuracy: 0.9877\n",
            "Epoch 221/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2760 - accuracy: 0.9877\n",
            "Epoch 222/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2752 - accuracy: 0.9877\n",
            "Epoch 223/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2743 - accuracy: 0.9877\n",
            "Epoch 224/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2735 - accuracy: 0.9877\n",
            "Epoch 225/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2726 - accuracy: 0.9877\n",
            "Epoch 226/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2718 - accuracy: 0.9877\n",
            "Epoch 227/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2709 - accuracy: 0.9877\n",
            "Epoch 228/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2701 - accuracy: 0.9877\n",
            "Epoch 229/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2693 - accuracy: 0.9877\n",
            "Epoch 230/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2685 - accuracy: 0.9877\n",
            "Epoch 231/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2676 - accuracy: 0.9877\n",
            "Epoch 232/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2668 - accuracy: 0.9877\n",
            "Epoch 233/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.2660 - accuracy: 0.9877\n",
            "Epoch 234/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2652 - accuracy: 0.9877\n",
            "Epoch 235/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2644 - accuracy: 0.9877\n",
            "Epoch 236/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 0.2636 - accuracy: 0.9877\n",
            "Epoch 237/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2628 - accuracy: 0.9877\n",
            "Epoch 238/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.2620 - accuracy: 0.9877\n",
            "Epoch 239/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2612 - accuracy: 0.9877\n",
            "Epoch 240/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2604 - accuracy: 0.9877\n",
            "Epoch 241/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2596 - accuracy: 0.9877\n",
            "Epoch 242/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2588 - accuracy: 0.9877\n",
            "Epoch 243/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.2581 - accuracy: 0.9877\n",
            "Epoch 244/300\n",
            "81/81 [==============================] - 0s 33us/step - loss: 0.2573 - accuracy: 0.9877\n",
            "Epoch 245/300\n",
            "81/81 [==============================] - 0s 31us/step - loss: 0.2565 - accuracy: 0.9877\n",
            "Epoch 246/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2558 - accuracy: 0.9877\n",
            "Epoch 247/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2550 - accuracy: 0.9877\n",
            "Epoch 248/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.2543 - accuracy: 0.9877\n",
            "Epoch 249/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.2535 - accuracy: 0.9877\n",
            "Epoch 250/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2528 - accuracy: 0.9877\n",
            "Epoch 251/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2520 - accuracy: 0.9877\n",
            "Epoch 252/300\n",
            "81/81 [==============================] - 0s 30us/step - loss: 0.2513 - accuracy: 0.9877\n",
            "Epoch 253/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.2506 - accuracy: 0.9877\n",
            "Epoch 254/300\n",
            "81/81 [==============================] - 0s 23us/step - loss: 0.2499 - accuracy: 0.9877\n",
            "Epoch 255/300\n",
            "81/81 [==============================] - 0s 17us/step - loss: 0.2492 - accuracy: 0.9877\n",
            "Epoch 256/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.2485 - accuracy: 0.9877\n",
            "Epoch 257/300\n",
            "81/81 [==============================] - 0s 16us/step - loss: 0.2478 - accuracy: 0.9877\n",
            "Epoch 258/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.2471 - accuracy: 0.9877\n",
            "Epoch 259/300\n",
            "81/81 [==============================] - 0s 52us/step - loss: 0.2464 - accuracy: 0.9877\n",
            "Epoch 260/300\n",
            "81/81 [==============================] - 0s 35us/step - loss: 0.2457 - accuracy: 0.9877\n",
            "Epoch 261/300\n",
            "81/81 [==============================] - 0s 41us/step - loss: 0.2450 - accuracy: 0.9877\n",
            "Epoch 262/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2444 - accuracy: 0.9877\n",
            "Epoch 263/300\n",
            "81/81 [==============================] - 0s 40us/step - loss: 0.2437 - accuracy: 0.9877\n",
            "Epoch 264/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2430 - accuracy: 0.9877\n",
            "Epoch 265/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.2424 - accuracy: 0.9877\n",
            "Epoch 266/300\n",
            "81/81 [==============================] - 0s 22us/step - loss: 0.2417 - accuracy: 0.9877\n",
            "Epoch 267/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2411 - accuracy: 0.9877\n",
            "Epoch 268/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2405 - accuracy: 0.9877\n",
            "Epoch 269/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.2398 - accuracy: 0.9877\n",
            "Epoch 270/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2392 - accuracy: 0.9877\n",
            "Epoch 271/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2385 - accuracy: 0.9877\n",
            "Epoch 272/300\n",
            "81/81 [==============================] - 0s 31us/step - loss: 0.2378 - accuracy: 0.9877\n",
            "Epoch 273/300\n",
            "81/81 [==============================] - 0s 36us/step - loss: 0.2372 - accuracy: 0.9877\n",
            "Epoch 274/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.2365 - accuracy: 0.9877\n",
            "Epoch 275/300\n",
            "81/81 [==============================] - 0s 29us/step - loss: 0.2358 - accuracy: 0.9877\n",
            "Epoch 276/300\n",
            "81/81 [==============================] - 0s 35us/step - loss: 0.2351 - accuracy: 0.9877\n",
            "Epoch 277/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.2345 - accuracy: 0.9877\n",
            "Epoch 278/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2338 - accuracy: 0.9877\n",
            "Epoch 279/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.2331 - accuracy: 0.9877\n",
            "Epoch 280/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.2324 - accuracy: 0.9877\n",
            "Epoch 281/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2317 - accuracy: 0.9877\n",
            "Epoch 282/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.2311 - accuracy: 0.9877\n",
            "Epoch 283/300\n",
            "81/81 [==============================] - 0s 27us/step - loss: 0.2304 - accuracy: 0.9877\n",
            "Epoch 284/300\n",
            "81/81 [==============================] - 0s 28us/step - loss: 0.2297 - accuracy: 0.9877\n",
            "Epoch 285/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2291 - accuracy: 0.9877\n",
            "Epoch 286/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.2284 - accuracy: 0.9877\n",
            "Epoch 287/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2278 - accuracy: 0.9877\n",
            "Epoch 288/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2271 - accuracy: 0.9877\n",
            "Epoch 289/300\n",
            "81/81 [==============================] - 0s 25us/step - loss: 0.2265 - accuracy: 0.9877\n",
            "Epoch 290/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2258 - accuracy: 0.9877\n",
            "Epoch 291/300\n",
            "81/81 [==============================] - 0s 24us/step - loss: 0.2252 - accuracy: 0.9877\n",
            "Epoch 292/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2246 - accuracy: 0.9877\n",
            "Epoch 293/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2240 - accuracy: 0.9877\n",
            "Epoch 294/300\n",
            "81/81 [==============================] - 0s 20us/step - loss: 0.2234 - accuracy: 0.9877\n",
            "Epoch 295/300\n",
            "81/81 [==============================] - 0s 18us/step - loss: 0.2228 - accuracy: 0.9877\n",
            "Epoch 296/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2222 - accuracy: 0.9877\n",
            "Epoch 297/300\n",
            "81/81 [==============================] - 0s 21us/step - loss: 0.2216 - accuracy: 0.9877\n",
            "Epoch 298/300\n",
            "81/81 [==============================] - 0s 26us/step - loss: 0.2210 - accuracy: 0.9877\n",
            "Epoch 299/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2204 - accuracy: 0.9877\n",
            "Epoch 300/300\n",
            "81/81 [==============================] - 0s 19us/step - loss: 0.2198 - accuracy: 0.9877\n",
            "44/44 [==============================] - 0s 826us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "97e03746-ff87-460c-d980-f2a36c1da92a",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "29f05eb8-94c0-4045-de32-90db9086b875",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8409090638160706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}